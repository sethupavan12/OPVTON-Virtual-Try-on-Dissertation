Requirement already satisfied: piqa in ./myenv/lib/python3.9/site-packages (1.2.2)
Requirement already satisfied: torch>=1.8.0 in ./myenv/lib/python3.9/site-packages (from piqa) (2.0.0)
Requirement already satisfied: torchvision>=0.9.0 in ./myenv/lib/python3.9/site-packages (from piqa) (0.15.1)
Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (11.7.4.91)
Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (11.4.0.1)
Requirement already satisfied: sympy in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (1.11.1)
Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (2.14.3)
Requirement already satisfied: networkx in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (3.0)
Requirement already satisfied: filelock in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (3.10.7)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (11.7.99)
Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (11.7.101)
Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (10.9.0.58)
Requirement already satisfied: triton==2.0.0 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (2.0.0)
Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (8.5.0.96)
Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (10.2.10.91)
Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (11.10.3.66)
Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (11.7.91)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (11.7.99)
Requirement already satisfied: jinja2 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (3.1.2)
Requirement already satisfied: typing-extensions in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->piqa) (4.5.0)
Requirement already satisfied: wheel in ./myenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.0->piqa) (0.40.0)
Requirement already satisfied: setuptools in ./myenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.0->piqa) (49.2.1)
Requirement already satisfied: cmake in ./myenv/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.8.0->piqa) (3.26.1)
Requirement already satisfied: lit in ./myenv/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.8.0->piqa) (16.0.0)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./myenv/lib/python3.9/site-packages (from torchvision>=0.9.0->piqa) (9.4.0)
Requirement already satisfied: requests in ./myenv/lib/python3.9/site-packages (from torchvision>=0.9.0->piqa) (2.28.2)
Requirement already satisfied: numpy in ./myenv/lib/python3.9/site-packages (from torchvision>=0.9.0->piqa) (1.24.2)
Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.9/site-packages (from jinja2->torch>=1.8.0->piqa) (2.1.2)
Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.9/site-packages (from requests->torchvision>=0.9.0->piqa) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.9/site-packages (from requests->torchvision>=0.9.0->piqa) (3.4)
Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.9/site-packages (from requests->torchvision>=0.9.0->piqa) (2022.12.7)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./myenv/lib/python3.9/site-packages (from requests->torchvision>=0.9.0->piqa) (1.26.15)
Requirement already satisfied: mpmath>=0.19 in ./myenv/lib/python3.9/site-packages (from sympy->torch>=1.8.0->piqa) (1.3.0)
Requirement already satisfied: optuna in ./myenv/lib/python3.9/site-packages (3.1.0)
Requirement already satisfied: PyYAML in ./myenv/lib/python3.9/site-packages (from optuna) (6.0)
Requirement already satisfied: alembic>=1.5.0 in ./myenv/lib/python3.9/site-packages (from optuna) (1.10.2)
Requirement already satisfied: tqdm in ./myenv/lib/python3.9/site-packages (from optuna) (4.65.0)
Requirement already satisfied: colorlog in ./myenv/lib/python3.9/site-packages (from optuna) (6.7.0)
Requirement already satisfied: cmaes>=0.9.1 in ./myenv/lib/python3.9/site-packages (from optuna) (0.9.1)
Requirement already satisfied: sqlalchemy>=1.3.0 in ./myenv/lib/python3.9/site-packages (from optuna) (2.0.8)
Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.9/site-packages (from optuna) (23.0)
Requirement already satisfied: numpy in ./myenv/lib/python3.9/site-packages (from optuna) (1.24.2)
Requirement already satisfied: Mako in ./myenv/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (1.2.4)
Requirement already satisfied: typing-extensions>=4 in ./myenv/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.5.0)
Requirement already satisfied: greenlet!=0.4.17 in ./myenv/lib/python3.9/site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)
Requirement already satisfied: MarkupSafe>=0.9.2 in ./myenv/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)
Collecting pytorch-msssim
  Downloading pytorch_msssim-0.2.1-py3-none-any.whl (7.2 kB)
Requirement already satisfied: torch in ./myenv/lib/python3.9/site-packages (from pytorch-msssim) (2.0.0)
Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (8.5.0.96)
Requirement already satisfied: triton==2.0.0 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (2.0.0)
Requirement already satisfied: networkx in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (3.0)
Requirement already satisfied: sympy in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (1.11.1)
Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (11.7.101)
Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (11.10.3.66)
Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (11.4.0.1)
Requirement already satisfied: typing-extensions in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (4.5.0)
Requirement already satisfied: filelock in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (3.10.7)
Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (11.7.4.91)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (11.7.99)
Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (10.9.0.58)
Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (2.14.3)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (11.7.99)
Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (10.2.10.91)
Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (11.7.91)
Requirement already satisfied: jinja2 in ./myenv/lib/python3.9/site-packages (from torch->pytorch-msssim) (3.1.2)
Requirement already satisfied: wheel in ./myenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->pytorch-msssim) (0.40.0)
Requirement already satisfied: setuptools in ./myenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->pytorch-msssim) (49.2.1)
Requirement already satisfied: cmake in ./myenv/lib/python3.9/site-packages (from triton==2.0.0->torch->pytorch-msssim) (3.26.1)
Requirement already satisfied: lit in ./myenv/lib/python3.9/site-packages (from triton==2.0.0->torch->pytorch-msssim) (16.0.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.9/site-packages (from jinja2->torch->pytorch-msssim) (2.1.2)
Requirement already satisfied: mpmath>=0.19 in ./myenv/lib/python3.9/site-packages (from sympy->torch->pytorch-msssim) (1.3.0)
Installing collected packages: pytorch-msssim
Successfully installed pytorch-msssim-0.2.1
Namespace(name='GMM_with_SSIM_loss_new_channels_higher_weight', workers=20, batch_size=32, dataroot='/scratch/c.c1984628/my_diss/bpgm/data', datamode='train', stage='GMM', data_list='/scratch/c.c1984628/my_diss/bpgm/data/train_pairs.txt', dataset='viton', fine_width=192, fine_height=256, radius=5, grid_size=5, lr=0.0001, tensorboard_dir='tensorboard', checkpoint_dir='/scratch/c.c1984628/my_diss/checkpoints/new_ssim_loss_new_channels_higher_ssim_weight', checkpoint='', display_count=20, save_count=5000, keep_step=100000, decay_step=100000, shuffle=False, train_size=0.7, val_size=0.3, img_size=256)
Start to train stage: GMM, named: GMM_with_SSIM_loss_new_channels_higher_weight!
initialization method [normal]
initialization method [normal]
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
Best mask loss weight: 0.10027115448092762
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /scratch/c.c1984628/my_diss/myenv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth
step:       20, time: 2.820, loss: 0.066261
step:       40, time: 2.512, loss: 0.070484
step:       60, time: 2.609, loss: 0.085710
step:       80, time: 2.514, loss: 0.060132
step:      100, time: 2.508, loss: 0.077529
step:      120, time: 2.557, loss: 0.074542
step:      140, time: 2.478, loss: 0.065681
step:      160, time: 2.627, loss: 0.085554
step:      180, time: 2.576, loss: 0.067005
step:      200, time: 2.477, loss: 0.061078
step:      220, time: 2.413, loss: 0.062659
step:      240, time: 2.245, loss: 0.058002
step:      260, time: 2.227, loss: 0.073272
step:      280, time: 2.478, loss: 0.080177
step:      300, time: 2.407, loss: 0.062942
step:      320, time: 2.364, loss: 0.075465
step:      340, time: 2.358, loss: 0.060200
step:      360, time: 2.319, loss: 0.058703
step:      380, time: 2.420, loss: 0.061686
step:      400, time: 2.321, loss: 0.057165
step:      420, time: 2.284, loss: 0.054879
step:      440, time: 2.313, loss: 0.071341
step:      460, time: 2.336, loss: 0.056089
step:      480, time: 2.343, loss: 0.060528
step:      500, time: 2.431, loss: 0.066356
step:      520, time: 2.369, loss: 0.065986
step:      540, time: 2.294, loss: 0.065184
step:      560, time: 2.187, loss: 0.060678
step:      580, time: 2.494, loss: 0.063909
step:      600, time: 2.423, loss: 0.061950
step:      620, time: 2.493, loss: 0.076374
step:      640, time: 2.327, loss: 0.058244
step:      660, time: 2.359, loss: 0.060999
step:      680, time: 2.384, loss: 0.064023
step:      700, time: 2.415, loss: 0.059039
step:      720, time: 2.355, loss: 0.058674
step:      740, time: 2.348, loss: 0.050727
step:      760, time: 2.428, loss: 0.062456
step:      780, time: 2.327, loss: 0.060670
step:      800, time: 2.324, loss: 0.060318
step:      820, time: 2.403, loss: 0.066826
step:      840, time: 2.285, loss: 0.056194
step:      860, time: 2.284, loss: 0.063221
step:      880, time: 2.256, loss: 0.063189
step:      900, time: 2.408, loss: 0.061300
step:      920, time: 2.422, loss: 0.071615
step:      940, time: 2.381, loss: 0.068599
step:      960, time: 2.373, loss: 0.061221
step:      980, time: 2.380, loss: 0.070323
step:     1000, time: 2.397, loss: 0.066280
step:     1020, time: 2.454, loss: 0.066226
step:     1040, time: 2.316, loss: 0.061379
step:     1060, time: 2.383, loss: 0.070714
step:     1080, time: 2.360, loss: 0.067315
step:     1100, time: 2.318, loss: 0.055928
step:     1120, time: 2.315, loss: 0.052724
step:     1140, time: 2.381, loss: 0.058464
step:     1160, time: 2.371, loss: 0.060890
step:     1180, time: 2.213, loss: 0.057557
step:     1200, time: 2.444, loss: 0.071536
step:     1220, time: 2.405, loss: 0.064026
step:     1240, time: 2.373, loss: 0.059351
step:     1260, time: 2.323, loss: 0.059411
step:     1280, time: 2.362, loss: 0.058905
step:     1300, time: 2.371, loss: 0.057961
step:     1320, time: 2.380, loss: 0.056168
step:     1340, time: 2.358, loss: 0.054329
step:     1360, time: 2.432, loss: 0.062618
step:     1380, time: 2.355, loss: 0.058523
step:     1400, time: 2.324, loss: 0.063568
step:     1420, time: 2.346, loss: 0.051000
step:     1440, time: 2.342, loss: 0.059669
step:     1460, time: 2.433, loss: 0.066713
step:     1480, time: 2.284, loss: 0.058960
step:     1500, time: 2.262, loss: 0.057624
step:     1520, time: 2.466, loss: 0.062220
step:     1540, time: 2.410, loss: 0.059677
step:     1560, time: 2.423, loss: 0.069156
step:     1580, time: 2.397, loss: 0.055614
step:     1600, time: 2.310, loss: 0.054421
step:     1620, time: 2.384, loss: 0.056349
step:     1640, time: 2.343, loss: 0.059597
step:     1660, time: 2.313, loss: 0.054056
step:     1680, time: 2.293, loss: 0.053796
step:     1700, time: 2.387, loss: 0.059894
step:     1720, time: 2.396, loss: 0.060303
step:     1740, time: 2.418, loss: 0.057824
step:     1760, time: 2.353, loss: 0.054270
step:     1780, time: 2.416, loss: 0.060146
step:     1800, time: 2.220, loss: 0.065502
step:     1820, time: 2.264, loss: 0.067361
step:     1840, time: 2.559, loss: 0.071820
step:     1860, time: 2.355, loss: 0.055750
step:     1880, time: 2.337, loss: 0.067973
step:     1900, time: 2.343, loss: 0.056097
step:     1920, time: 2.344, loss: 0.056794
step:     1940, time: 2.445, loss: 0.060773
step:     1960, time: 2.318, loss: 0.053640
step:     1980, time: 2.404, loss: 0.057805
step:     2000, time: 2.350, loss: 0.046687
step:     2020, time: 2.315, loss: 0.046036
step:     2040, time: 2.315, loss: 0.048892
step:     2060, time: 2.354, loss: 0.060596
step:     2080, time: 2.441, loss: 0.068045
step:     2100, time: 2.375, loss: 0.063366
step:     2120, time: 2.261, loss: 0.055695
step:     2140, time: 2.355, loss: 0.057352
step:     2160, time: 2.308, loss: 0.057300
step:     2180, time: 2.417, loss: 0.049248
step:     2200, time: 2.406, loss: 0.055880
step:     2220, time: 2.456, loss: 0.059861
step:     2240, time: 2.465, loss: 0.066103
step:     2260, time: 2.379, loss: 0.061132
step:     2280, time: 2.338, loss: 0.048049
step:     2300, time: 2.403, loss: 0.055632
step:     2320, time: 2.320, loss: 0.058909
step:     2340, time: 2.314, loss: 0.051828
step:     2360, time: 2.317, loss: 0.047294
step:     2380, time: 2.341, loss: 0.050196
step:     2400, time: 2.449, loss: 0.055832
step:     2420, time: 2.339, loss: 0.058364
step:     2440, time: 2.308, loss: 0.064475
step:     2460, time: 2.382, loss: 0.049860
step:     2480, time: 2.353, loss: 0.053252
step:     2500, time: 2.389, loss: 0.048815
step:     2520, time: 2.318, loss: 0.053077
step:     2540, time: 2.394, loss: 0.060949
step:     2560, time: 2.373, loss: 0.056875
step:     2580, time: 2.432, loss: 0.059385
step:     2600, time: 2.322, loss: 0.054033
step:     2620, time: 2.315, loss: 0.050345
step:     2640, time: 2.277, loss: 0.046990
step:     2660, time: 2.445, loss: 0.066509
step:     2680, time: 2.393, loss: 0.064827
step:     2700, time: 2.338, loss: 0.053381
step:     2720, time: 2.327, loss: 0.051218
step:     2740, time: 2.234, loss: 0.058782
step:     2760, time: 2.546, loss: 0.063984
step:     2780, time: 2.449, loss: 0.064323
step:     2800, time: 2.354, loss: 0.050761
step:     2820, time: 2.368, loss: 0.060317
step:     2840, time: 2.323, loss: 0.046178
step:     2860, time: 2.329, loss: 0.061929
step:     2880, time: 2.383, loss: 0.050276
step:     2900, time: 2.453, loss: 0.063749
step:     2920, time: 2.354, loss: 0.052522
step:     2940, time: 2.379, loss: 0.059193
step:     2960, time: 2.347, loss: 0.050938
step:     2980, time: 2.318, loss: 0.049642
step:     3000, time: 2.307, loss: 0.047529
step:     3020, time: 2.416, loss: 0.058142
step:     3040, time: 2.352, loss: 0.052700
step:     3060, time: 2.257, loss: 0.056971
step:     3080, time: 2.365, loss: 0.059250
step:     3100, time: 2.436, loss: 0.059626
step:     3120, time: 2.283, loss: 0.052718
step:     3140, time: 2.275, loss: 0.046268
step:     3160, time: 2.420, loss: 0.061757
step:     3180, time: 2.325, loss: 0.052584
step:     3200, time: 2.469, loss: 0.063040
step:     3220, time: 2.403, loss: 0.053476
step:     3240, time: 2.407, loss: 0.062344
step:     3260, time: 2.329, loss: 0.061380
step:     3280, time: 2.319, loss: 0.050895
step:     3300, time: 2.344, loss: 0.052377
step:     3320, time: 2.337, loss: 0.049723
step:     3340, time: 2.341, loss: 0.055115
step:     3360, time: 2.300, loss: 0.052549
step:     3380, time: 2.237, loss: 0.048847
step:     3400, time: 2.392, loss: 0.055679
step:     3420, time: 2.309, loss: 0.050288
step:     3440, time: 2.369, loss: 0.058788
step:     3460, time: 2.330, loss: 0.049026
step:     3480, time: 2.341, loss: 0.051489
step:     3500, time: 2.358, loss: 0.059086
step:     3520, time: 2.373, loss: 0.061750
step:     3540, time: 2.327, loss: 0.053935
step:     3560, time: 2.376, loss: 0.048054
step:     3580, time: 2.365, loss: 0.054217
step:     3600, time: 2.376, loss: 0.053230
step:     3620, time: 2.422, loss: 0.063676
step:     3640, time: 2.393, loss: 0.060586
step:     3660, time: 2.345, loss: 0.057672
step:     3680, time: 2.210, loss: 0.053213
step:     3700, time: 2.404, loss: 0.054811
step:     3720, time: 2.359, loss: 0.053441
step:     3740, time: 2.446, loss: 0.055357
step:     3760, time: 2.290, loss: 0.053878
step:     3780, time: 2.396, loss: 0.057588
step:     3800, time: 2.356, loss: 0.066151
step:     3820, time: 2.391, loss: 0.055107
step:     3840, time: 2.365, loss: 0.055805
step:     3860, time: 2.378, loss: 0.055676
step:     3880, time: 2.369, loss: 0.053353
step:     3900, time: 2.348, loss: 0.061541
step:     3920, time: 2.420, loss: 0.059626
step:     3940, time: 2.387, loss: 0.053643
step:     3960, time: 2.374, loss: 0.054005
step:     3980, time: 2.355, loss: 0.055391
step:     4000, time: 2.286, loss: 0.057294
step:     4020, time: 2.380, loss: 0.052048
step:     4040, time: 2.379, loss: 0.051697
step:     4060, time: 2.348, loss: 0.055728
step:     4080, time: 2.439, loss: 0.059974
step:     4100, time: 2.357, loss: 0.052755
step:     4120, time: 2.314, loss: 0.053287
step:     4140, time: 2.320, loss: 0.049060
step:     4160, time: 2.330, loss: 0.049693
step:     4180, time: 2.345, loss: 0.057729
step:     4200, time: 2.345, loss: 0.050405
step:     4220, time: 2.324, loss: 0.050628
step:     4240, time: 2.426, loss: 0.057119
step:     4260, time: 2.336, loss: 0.057905
step:     4280, time: 2.276, loss: 0.049040
step:     4300, time: 2.269, loss: 0.057718
step:     4320, time: 3.740, loss: 0.063125
step:     4340, time: 2.270, loss: 0.057822
step:     4360, time: 2.313, loss: 0.050180
step:     4380, time: 2.355, loss: 0.054509
step:     4400, time: 2.385, loss: 0.053689
step:     4420, time: 2.366, loss: 0.050788
step:     4440, time: 2.332, loss: 0.055033
step:     4460, time: 2.405, loss: 0.062793
step:     4480, time: 2.300, loss: 0.048690
step:     4500, time: 2.412, loss: 0.059943
step:     4520, time: 2.332, loss: 0.048535
step:     4540, time: 2.388, loss: 0.054288
step:     4560, time: 2.418, loss: 0.058488
step:     4580, time: 2.443, loss: 0.056739
step:     4600, time: 2.284, loss: 0.057211
step:     4620, time: 2.260, loss: 0.057668
step:     4640, time: 2.435, loss: 0.064834
step:     4660, time: 2.339, loss: 0.051270
step:     4680, time: 2.384, loss: 0.054322
step:     4700, time: 2.286, loss: 0.046541
step:     4720, time: 2.473, loss: 0.067339
step:     4740, time: 2.357, loss: 0.060450
step:     4760, time: 2.415, loss: 0.059754
step:     4780, time: 2.409, loss: 0.059745
step:     4800, time: 2.365, loss: 0.055388
step:     4820, time: 2.351, loss: 0.058720
step:     4840, time: 2.391, loss: 0.063819
step:     4860, time: 2.383, loss: 0.055474
step:     4880, time: 2.393, loss: 0.055834
step:     4900, time: 2.468, loss: 0.061736
step:     4920, time: 2.252, loss: 0.049958
step:     4940, time: 2.266, loss: 0.049816
step:     4960, time: 2.417, loss: 0.056669
step:     4980, time: 2.405, loss: 0.056977
step:     5000, time: 2.424, loss: 0.057493
step:     5020, time: 2.414, loss: 0.057395
step:     5040, time: 2.401, loss: 0.050219
step:     5060, time: 2.418, loss: 0.058046
step:     5080, time: 2.334, loss: 0.047737
step:     5100, time: 2.354, loss: 0.047525
step:     5120, time: 2.322, loss: 0.048048
step:     5140, time: 2.482, loss: 0.066906
step:     5160, time: 2.345, loss: 0.059222
step:     5180, time: 2.437, loss: 0.055930
step:     5200, time: 2.417, loss: 0.054428
step:     5220, time: 2.261, loss: 0.053724
step:     5240, time: 2.220, loss: 0.048206
step:     5260, time: 2.495, loss: 0.061530
step:     5280, time: 2.438, loss: 0.056049
step:     5300, time: 2.270, loss: 0.049428
step:     5320, time: 2.401, loss: 0.054888
step:     5340, time: 2.370, loss: 0.052695
step:     5360, time: 2.370, loss: 0.049132
step:     5380, time: 2.271, loss: 0.051969
step:     5400, time: 2.335, loss: 0.058039
step:     5420, time: 2.383, loss: 0.051090
step:     5440, time: 2.326, loss: 0.048042
step:     5460, time: 2.306, loss: 0.047177
step:     5480, time: 2.342, loss: 0.048003
step:     5500, time: 2.323, loss: 0.051347
step:     5520, time: 2.373, loss: 0.065110
step:     5540, time: 2.254, loss: 0.048488
step:     5560, time: 2.243, loss: 0.056458
step:     5580, time: 2.477, loss: 0.056487
step:     5600, time: 2.430, loss: 0.054644
step:     5620, time: 2.419, loss: 0.063473
step:     5640, time: 2.434, loss: 0.061099
step:     5660, time: 2.373, loss: 0.059378
step:     5680, time: 2.342, loss: 0.050140
step:     5700, time: 2.352, loss: 0.057679
step:     5720, time: 2.394, loss: 0.049551
step:     5740, time: 2.396, loss: 0.059489
step:     5760, time: 2.302, loss: 0.050852
step:     5780, time: 2.436, loss: 0.061841
step:     5800, time: 2.358, loss: 0.055491
step:     5820, time: 2.333, loss: 0.049210
step:     5840, time: 2.309, loss: 0.051393
step:     5860, time: 2.145, loss: 0.040264
step:     5880, time: 2.478, loss: 0.063724
step:     5900, time: 2.323, loss: 0.051543
step:     5920, time: 2.366, loss: 0.051481
step:     5940, time: 2.432, loss: 0.064149
step:     5960, time: 2.348, loss: 0.051728
step:     5980, time: 2.352, loss: 0.056932
step:     6000, time: 2.442, loss: 0.062873
step:     6020, time: 2.288, loss: 0.043377
step:     6040, time: 2.326, loss: 0.043406
step:     6060, time: 2.380, loss: 0.050737
step:     6080, time: 2.373, loss: 0.066333
step:     6100, time: 2.361, loss: 0.053302
step:     6120, time: 2.474, loss: 0.060158
step:     6140, time: 2.368, loss: 0.044997
step:     6160, time: 2.315, loss: 0.052285
step:     6180, time: 2.244, loss: 0.049661
step:     6200, time: 2.494, loss: 0.061540
step:     6220, time: 2.387, loss: 0.049174
step:     6240, time: 2.316, loss: 0.053707
step:     6260, time: 2.395, loss: 0.056644
step:     6280, time: 2.447, loss: 0.066133
step:     6300, time: 2.392, loss: 0.053617
step:     6320, time: 2.308, loss: 0.054610
step:     6340, time: 2.364, loss: 0.049179
step:     6360, time: 2.408, loss: 0.068884
step:     6380, time: 2.430, loss: 0.057079
step:     6400, time: 2.353, loss: 0.051621
step:     6420, time: 2.386, loss: 0.051797
step:     6440, time: 2.421, loss: 0.057283
step:     6460, time: 2.353, loss: 0.051330
step:     6480, time: 2.262, loss: 0.053416
step:     6500, time: 2.260, loss: 0.058312
step:     6520, time: 2.429, loss: 0.051358
step:     6540, time: 2.344, loss: 0.057738
step:     6560, time: 2.292, loss: 0.052041
step:     6580, time: 2.405, loss: 0.050916
step:     6600, time: 2.461, loss: 0.054275
step:     6620, time: 2.452, loss: 0.067706
step:     6640, time: 2.404, loss: 0.058789
step:     6660, time: 2.337, loss: 0.046455
step:     6680, time: 2.441, loss: 0.060625
step:     6700, time: 2.357, loss: 0.049630
step:     6720, time: 2.376, loss: 0.057969
step:     6740, time: 2.312, loss: 0.052264
step:     6760, time: 2.403, loss: 0.051744
step:     6780, time: 2.335, loss: 0.057670
step:     6800, time: 2.179, loss: 0.049466
step:     6820, time: 2.422, loss: 0.053319
step:     6840, time: 2.323, loss: 0.052358
step:     6860, time: 2.374, loss: 0.053481
step:     6880, time: 2.389, loss: 0.056203
step:     6900, time: 2.261, loss: 0.042536
step:     6920, time: 2.441, loss: 0.064771
step:     6940, time: 2.289, loss: 0.046816
step:     6960, time: 2.438, loss: 0.049669
step:     6980, time: 2.308, loss: 0.046770
step:     7000, time: 2.447, loss: 0.063198
step:     7020, time: 2.304, loss: 0.043838
step:     7040, time: 2.441, loss: 0.058814
step:     7060, time: 2.476, loss: 0.058140
step:     7080, time: 2.265, loss: 0.045095
step:     7100, time: 2.247, loss: 0.057455
step:     7120, time: 2.243, loss: 0.050548
step:     7140, time: 2.459, loss: 0.055239
step:     7160, time: 2.353, loss: 0.044020
step:     7180, time: 2.296, loss: 0.052120
step:     7200, time: 2.336, loss: 0.041834
step:     7220, time: 2.396, loss: 0.055472
step:     7240, time: 2.468, loss: 0.058867
step:     7260, time: 2.346, loss: 0.046085
step:     7280, time: 2.432, loss: 0.062651
step:     7300, time: 2.308, loss: 0.045866
step:     7320, time: 2.366, loss: 0.048389
step:     7340, time: 2.386, loss: 0.055602
step:     7360, time: 2.313, loss: 0.055117
step:     7380, time: 2.366, loss: 0.051968
step:     7400, time: 2.322, loss: 0.056777
step:     7420, time: 2.204, loss: 0.046366
step:     7440, time: 2.378, loss: 0.050674
step:     7460, time: 2.447, loss: 0.055460
step:     7480, time: 2.319, loss: 0.050474
step:     7500, time: 2.359, loss: 0.053287
step:     7520, time: 2.395, loss: 0.064107
step:     7540, time: 2.365, loss: 0.054935
step:     7560, time: 2.373, loss: 0.056847
step:     7580, time: 2.292, loss: 0.048171
step:     7600, time: 2.341, loss: 0.043675
step:     7620, time: 2.369, loss: 0.049369
step:     7640, time: 2.319, loss: 0.047424
step:     7660, time: 2.375, loss: 0.049531
step:     7680, time: 2.393, loss: 0.051848
step:     7700, time: 2.354, loss: 0.053257
step:     7720, time: 2.267, loss: 0.049197
step:     7740, time: 2.264, loss: 0.050329
step:     7760, time: 2.397, loss: 0.052595
step:     7780, time: 2.325, loss: 0.055858
step:     7800, time: 2.368, loss: 0.062338
step:     7820, time: 2.406, loss: 0.053409
step:     7840, time: 2.414, loss: 0.059517
step:     7860, time: 2.422, loss: 0.057549
step:     7880, time: 2.372, loss: 0.051955
step:     7900, time: 2.319, loss: 0.042221
step:     7920, time: 2.407, loss: 0.056098
step:     7940, time: 2.416, loss: 0.068398
step:     7960, time: 2.344, loss: 0.047457
step:     7980, time: 2.390, loss: 0.051943
step:     8000, time: 2.446, loss: 0.056543
step:     8020, time: 2.428, loss: 0.055505
step:     8040, time: 2.252, loss: 0.044136
step:     8060, time: 2.167, loss: 0.056671
step:     8080, time: 2.435, loss: 0.056141
step:     8100, time: 2.457, loss: 0.059190
step:     8120, time: 2.384, loss: 0.054135
step:     8140, time: 2.396, loss: 0.050207
step:     8160, time: 2.310, loss: 0.048661
step:     8180, time: 2.350, loss: 0.047489
step:     8200, time: 2.289, loss: 0.041941
step:     8220, time: 2.459, loss: 0.059691
step:     8240, time: 2.393, loss: 0.051465
step:     8260, time: 2.358, loss: 0.051717
step:     8280, time: 2.290, loss: 0.050652
step:     8300, time: 2.406, loss: 0.060784
step:     8320, time: 2.378, loss: 0.057251
step:     8340, time: 2.382, loss: 0.058570
step:     8360, time: 2.258, loss: 0.048585
step:     8380, time: 2.413, loss: 0.050247
step:     8400, time: 2.443, loss: 0.060991
step:     8420, time: 2.392, loss: 0.052827
step:     8440, time: 2.393, loss: 0.058612
step:     8460, time: 2.347, loss: 0.053665
step:     8480, time: 2.405, loss: 0.050086
step:     8500, time: 2.354, loss: 0.058274
step:     8520, time: 2.479, loss: 0.060309
step:     8540, time: 2.501, loss: 0.059774
step:     8560, time: 2.302, loss: 0.042808
step:     8580, time: 2.438, loss: 0.049202
step:     8600, time: 2.410, loss: 0.052192
step:     8620, time: 2.435, loss: 0.065073
step:     8640, time: 2.380, loss: 0.053188
step:     8660, time: 2.302, loss: 0.057076
step:     8680, time: 2.157, loss: 0.049139
step:     8700, time: 2.410, loss: 0.056017
step:     8720, time: 2.355, loss: 0.053642
step:     8740, time: 2.418, loss: 0.057619
step:     8760, time: 2.332, loss: 0.050891
step:     8780, time: 2.339, loss: 0.051007
step:     8800, time: 2.334, loss: 0.050387
step:     8820, time: 2.458, loss: 0.053184
step:     8840, time: 2.321, loss: 0.045038
step:     8860, time: 2.330, loss: 0.044777
step:     8880, time: 2.423, loss: 0.053159
step:     8900, time: 2.370, loss: 0.053836
step:     8920, time: 2.393, loss: 0.048130
step:     8940, time: 2.354, loss: 0.048868
step:     8960, time: 2.302, loss: 0.051778
step:     8980, time: 2.146, loss: 0.049485
step:     9000, time: 3.462, loss: 0.052421
step:     9020, time: 2.427, loss: 0.050810
step:     9040, time: 2.451, loss: 0.055044
step:     9060, time: 2.470, loss: 0.060917
step:     9080, time: 2.384, loss: 0.050972
step:     9100, time: 2.462, loss: 0.056666
step:     9120, time: 2.377, loss: 0.056808
step:     9140, time: 2.515, loss: 0.062326
step:     9160, time: 2.337, loss: 0.056710
step:     9180, time: 2.282, loss: 0.049100
step:     9200, time: 2.342, loss: 0.047917
step:     9220, time: 2.399, loss: 0.055740
step:     9240, time: 2.365, loss: 0.054011
step:     9260, time: 2.458, loss: 0.057784
step:     9280, time: 2.309, loss: 0.056353
step:     9300, time: 2.292, loss: 0.054402
step:     9320, time: 2.439, loss: 0.050948
step:     9340, time: 2.402, loss: 0.050532
step:     9360, time: 2.543, loss: 0.064027
step:     9380, time: 2.384, loss: 0.048496
step:     9400, time: 2.472, loss: 0.062405
step:     9420, time: 2.416, loss: 0.055296
step:     9440, time: 2.374, loss: 0.057677
step:     9460, time: 2.360, loss: 0.047324
step:     9480, time: 2.396, loss: 0.058378
step:     9500, time: 2.312, loss: 0.038165
step:     9520, time: 2.335, loss: 0.051051
step:     9540, time: 2.375, loss: 0.058178
step:     9560, time: 2.305, loss: 0.043502
step:     9580, time: 2.390, loss: 0.047567
step:     9600, time: 2.243, loss: 0.049202
step:     9620, time: 2.243, loss: 0.051087
step:     9640, time: 2.468, loss: 0.058615
step:     9660, time: 2.301, loss: 0.046771
step:     9680, time: 2.305, loss: 0.044792
step:     9700, time: 2.415, loss: 0.050280
step:     9720, time: 2.391, loss: 0.054493
step:     9740, time: 2.274, loss: 0.046348
step:     9760, time: 2.338, loss: 0.050827
step:     9780, time: 2.358, loss: 0.049118
step:     9800, time: 2.323, loss: 0.045249
step:     9820, time: 2.318, loss: 0.052221
step:     9840, time: 2.344, loss: 0.048399
step:     9860, time: 2.388, loss: 0.052484
step:     9880, time: 2.349, loss: 0.048898
step:     9900, time: 2.288, loss: 0.052307
step:     9920, time: 2.208, loss: 0.049013
step:     9940, time: 2.410, loss: 0.048348
step:     9960, time: 2.427, loss: 0.051953
step:     9980, time: 2.430, loss: 0.055958
step:    10000, time: 2.357, loss: 0.044756
step:    10020, time: 2.392, loss: 0.054058
step:    10040, time: 2.352, loss: 0.046024
step:    10060, time: 2.397, loss: 0.058155
step:    10080, time: 2.423, loss: 0.052492
step:    10100, time: 2.359, loss: 0.053805
step:    10120, time: 2.343, loss: 0.048094
step:    10140, time: 2.375, loss: 0.053763
step:    10160, time: 2.312, loss: 0.048707
step:    10180, time: 2.381, loss: 0.050253
step:    10200, time: 2.251, loss: 0.042076
step:    10220, time: 2.200, loss: 0.049907
step:    10240, time: 2.229, loss: 0.057461
step:    10260, time: 2.406, loss: 0.048188
step:    10280, time: 2.419, loss: 0.059291
step:    10300, time: 2.397, loss: 0.051927
step:    10320, time: 2.400, loss: 0.050527
step:    10340, time: 2.224, loss: 0.044870
step:    10360, time: 2.297, loss: 0.043642
step:    10380, time: 2.438, loss: 0.060876
step:    10400, time: 2.378, loss: 0.062170
step:    10420, time: 2.337, loss: 0.047523
step:    10440, time: 2.321, loss: 0.045052
step:    10460, time: 2.359, loss: 0.052898
step:    10480, time: 2.417, loss: 0.061891
step:    10500, time: 2.369, loss: 0.050800
step:    10520, time: 2.348, loss: 0.047067
step:    10540, time: 2.195, loss: 0.048290
step:    10560, time: 3.091, loss: 0.063435
step:    10580, time: 2.417, loss: 0.055118
step:    10600, time: 2.420, loss: 0.059522
step:    10620, time: 2.315, loss: 0.049154
step:    10640, time: 2.384, loss: 0.053011
step:    10660, time: 2.443, loss: 0.056109
step:    10680, time: 2.331, loss: 0.043907
step:    10700, time: 2.286, loss: 0.040605
step:    10720, time: 2.376, loss: 0.051186
step:    10740, time: 2.359, loss: 0.061554
step:    10760, time: 2.324, loss: 0.049047
step:    10780, time: 2.403, loss: 0.052138
step:    10800, time: 2.438, loss: 0.049808
step:    10820, time: 2.294, loss: 0.043235
step:    10840, time: 2.335, loss: 0.061551
step:    10860, time: 2.288, loss: 0.058154
step:    10880, time: 2.470, loss: 0.065031
step:    10900, time: 2.339, loss: 0.044906
step:    10920, time: 2.335, loss: 0.053225
step:    10940, time: 2.421, loss: 0.055028
step:    10960, time: 2.394, loss: 0.056493
step:    10980, time: 2.385, loss: 0.051131
step:    11000, time: 2.464, loss: 0.063450
step:    11020, time: 2.334, loss: 0.043163
step:    11040, time: 2.379, loss: 0.048813
step:    11060, time: 2.471, loss: 0.058754
step:    11080, time: 2.397, loss: 0.057452
step:    11100, time: 2.378, loss: 0.053647
step:    11120, time: 2.494, loss: 0.068357
step:    11140, time: 2.462, loss: 0.055362
step:    11160, time: 2.312, loss: 0.052200
step:    11180, time: 2.288, loss: 0.049435
step:    11200, time: 2.279, loss: 0.042838
step:    11220, time: 2.438, loss: 0.052232
step:    11240, time: 2.428, loss: 0.050125
step:    11260, time: 2.376, loss: 0.050229
step:    11280, time: 2.334, loss: 0.049898
step:    11300, time: 2.438, loss: 0.061236
step:    11320, time: 2.412, loss: 0.049959
step:    11340, time: 2.334, loss: 0.044410
step:    11360, time: 2.469, loss: 0.062628
step:    11380, time: 2.266, loss: 0.048521
step:    11400, time: 2.422, loss: 0.064681
step:    11420, time: 2.367, loss: 0.051046
step:    11440, time: 2.358, loss: 0.046750
step:    11460, time: 2.349, loss: 0.053581
step:    11480, time: 2.240, loss: 0.057233
step:    11500, time: 2.404, loss: 0.054453
step:    11520, time: 2.373, loss: 0.059974
step:    11540, time: 2.365, loss: 0.056457
step:    11560, time: 2.294, loss: 0.043041
step:    11580, time: 2.384, loss: 0.056179
step:    11600, time: 2.407, loss: 0.048516
step:    11620, time: 2.373, loss: 0.056431
step:    11640, time: 2.359, loss: 0.050025
step:    11660, time: 2.361, loss: 0.050437
step:    11680, time: 2.341, loss: 0.049553
step:    11700, time: 2.412, loss: 0.062623
step:    11720, time: 2.328, loss: 0.043530
step:    11740, time: 2.422, loss: 0.053263
step:    11760, time: 2.381, loss: 0.048542
step:    11780, time: 2.197, loss: 0.060709
step:    11800, time: 2.294, loss: 0.058790
step:    11820, time: 2.376, loss: 0.052241
step:    11840, time: 2.437, loss: 0.066203
step:    11860, time: 2.459, loss: 0.061805
step:    11880, time: 2.418, loss: 0.049942
step:    11900, time: 2.391, loss: 0.049159
step:    11920, time: 2.304, loss: 0.047093
step:    11940, time: 2.327, loss: 0.049605
step:    11960, time: 2.445, loss: 0.053792
step:    11980, time: 2.375, loss: 0.058419
step:    12000, time: 2.440, loss: 0.056833
step:    12020, time: 2.490, loss: 0.058670
step:    12040, time: 2.372, loss: 0.052364
step:    12060, time: 2.402, loss: 0.054989
step:    12080, time: 2.267, loss: 0.045049
step:    12100, time: 2.227, loss: 0.055103
step:    12120, time: 2.392, loss: 0.049916
step:    12140, time: 2.445, loss: 0.050022
step:    12160, time: 2.320, loss: 0.041668
step:    12180, time: 2.421, loss: 0.053110
step:    12200, time: 2.344, loss: 0.050699
step:    12220, time: 2.367, loss: 0.053347
step:    12240, time: 2.395, loss: 0.053606
step:    12260, time: 2.342, loss: 0.050809
step:    12280, time: 2.363, loss: 0.048007
step:    12300, time: 2.331, loss: 0.063442
step:    12320, time: 2.278, loss: 0.047545
step:    12340, time: 2.409, loss: 0.054092
step:    12360, time: 2.344, loss: 0.045922
step:    12380, time: 2.362, loss: 0.048535
step:    12400, time: 2.309, loss: 0.047835
step:    12420, time: 2.308, loss: 0.050046
step:    12440, time: 2.386, loss: 0.048576
step:    12460, time: 2.399, loss: 0.049006
step:    12480, time: 2.328, loss: 0.051607
step:    12500, time: 2.350, loss: 0.046627
step:    12520, time: 2.355, loss: 0.054820
step:    12540, time: 2.343, loss: 0.043362
step:    12560, time: 2.389, loss: 0.055877
step:    12580, time: 2.481, loss: 0.055981
step:    12600, time: 2.361, loss: 0.051805
step:    12620, time: 2.352, loss: 0.046265
step:    12640, time: 2.344, loss: 0.050519
step:    12660, time: 2.326, loss: 0.048396
step:    12680, time: 2.423, loss: 0.056198
step:    12700, time: 2.377, loss: 0.054218
step:    12720, time: 2.265, loss: 0.050221
step:    12740, time: 2.253, loss: 0.049442
step:    12760, time: 2.458, loss: 0.052072
step:    12780, time: 2.418, loss: 0.052682
step:    12800, time: 2.401, loss: 0.051591
step:    12820, time: 2.448, loss: 0.055184
step:    12840, time: 2.391, loss: 0.045161
step:    12860, time: 2.422, loss: 0.053710
step:    12880, time: 2.464, loss: 0.055067
step:    12900, time: 2.280, loss: 0.038357
step:    12920, time: 2.334, loss: 0.041108
step:    12940, time: 2.418, loss: 0.050221
step:    12960, time: 2.386, loss: 0.049483
step:    12980, time: 2.461, loss: 0.058570
step:    13000, time: 2.381, loss: 0.049834
step:    13020, time: 2.277, loss: 0.050739
step:    13040, time: 2.258, loss: 0.054387
step:    13060, time: 2.525, loss: 0.065360
step:    13080, time: 2.403, loss: 0.054414
step:    13100, time: 2.368, loss: 0.047060
step:    13120, time: 2.279, loss: 0.046457
step:    13140, time: 2.371, loss: 0.046451
step:    13160, time: 2.420, loss: 0.051622
step:    13180, time: 2.375, loss: 0.061469
step:    13200, time: 2.299, loss: 0.039922
step:    13220, time: 2.383, loss: 0.052066
step:    13240, time: 2.422, loss: 0.055214
step:    13260, time: 2.447, loss: 0.054996
step:    13280, time: 2.401, loss: 0.050662
step:    13300, time: 2.461, loss: 0.054464
step:    13320, time: 2.374, loss: 0.057169
step:    13340, time: 2.246, loss: 0.048869
step:    13360, time: 2.339, loss: 0.063885
step:    13380, time: 2.416, loss: 0.049525
step:    13400, time: 2.357, loss: 0.045969
step:    13420, time: 2.389, loss: 0.052513
step:    13440, time: 2.431, loss: 0.069032
step:    13460, time: 2.504, loss: 0.054343
step:    13480, time: 2.365, loss: 0.049180
step:    13500, time: 2.332, loss: 0.053252
step:    13520, time: 2.333, loss: 0.055267
step:    13540, time: 2.280, loss: 0.043721
step:    13560, time: 2.344, loss: 0.047157
step:    13580, time: 2.380, loss: 0.048222
step:    13600, time: 2.381, loss: 0.050433
step:    13620, time: 2.371, loss: 0.046317
step:    13640, time: 2.219, loss: 0.047038
step:    13660, time: 2.239, loss: 0.049060
step:    13680, time: 2.585, loss: 0.049051
step:    13700, time: 2.383, loss: 0.052741
step:    13720, time: 2.391, loss: 0.051463
step:    13740, time: 2.345, loss: 0.058911
step:    13760, time: 2.380, loss: 0.051452
step:    13780, time: 2.402, loss: 0.051471
step:    13800, time: 2.426, loss: 0.058946
step:    13820, time: 2.302, loss: 0.043684
step:    13840, time: 2.317, loss: 0.041562
step:    13860, time: 2.422, loss: 0.051604
step:    13880, time: 2.389, loss: 0.050453
step:    13900, time: 2.442, loss: 0.062516
step:    13920, time: 2.348, loss: 0.051306
step:    13940, time: 2.356, loss: 0.047958
step:    13960, time: 2.173, loss: 0.046036
step:    13980, time: 2.254, loss: 0.060358
step:    14000, time: 2.359, loss: 0.046879
step:    14020, time: 2.320, loss: 0.045861
step:    14040, time: 2.360, loss: 0.047867
step:    14060, time: 2.398, loss: 0.048682
step:    14080, time: 2.297, loss: 0.047991
step:    14100, time: 2.388, loss: 0.051748
step:    14120, time: 2.275, loss: 0.039060
step:    14140, time: 2.367, loss: 0.052382
step:    14160, time: 2.446, loss: 0.056827
step:    14180, time: 2.489, loss: 0.059888
step:    14200, time: 2.302, loss: 0.051199
step:    14220, time: 2.346, loss: 0.052869
step:    14240, time: 2.376, loss: 0.052094
step:    14260, time: 2.434, loss: 0.057527
step:    14280, time: 2.276, loss: 0.052480
step:    14300, time: 2.225, loss: 0.041421
step:    14320, time: 2.412, loss: 0.047307
step:    14340, time: 2.355, loss: 0.042233
step:    14360, time: 2.390, loss: 0.060024
step:    14380, time: 2.408, loss: 0.054275
step:    14400, time: 2.360, loss: 0.050367
step:    14420, time: 2.411, loss: 0.057208
step:    14440, time: 2.358, loss: 0.049738
step:    14460, time: 2.401, loss: 0.061795
step:    14480, time: 2.332, loss: 0.042736
step:    14500, time: 2.337, loss: 0.055687
step:    14520, time: 2.482, loss: 0.057034
step:    14540, time: 2.399, loss: 0.045137
step:    14560, time: 2.491, loss: 0.059454
step:    14580, time: 2.250, loss: 0.043348
step:    14600, time: 2.234, loss: 0.045821
step:    14620, time: 2.455, loss: 0.048845
step:    14640, time: 2.440, loss: 0.056398
step:    14660, time: 2.369, loss: 0.053974
step:    14680, time: 2.378, loss: 0.050229
step:    14700, time: 2.365, loss: 0.044889
step:    14720, time: 2.333, loss: 0.046084
step:    14740, time: 2.348, loss: 0.041427
step:    14760, time: 2.373, loss: 0.051405
step:    14780, time: 2.436, loss: 0.058264
step:    14800, time: 2.380, loss: 0.053602
step:    14820, time: 2.298, loss: 0.045255
step:    14840, time: 2.354, loss: 0.051654
step:    14860, time: 2.380, loss: 0.049116
step:    14880, time: 2.373, loss: 0.048169
step:    14900, time: 2.261, loss: 0.047590
step:    14920, time: 2.237, loss: 0.045649
step:    14940, time: 2.352, loss: 0.044345
step:    14960, time: 2.352, loss: 0.054964
step:    14980, time: 2.337, loss: 0.046037
step:    15000, time: 2.344, loss: 0.047725
step:    15020, time: 2.420, loss: 0.055796
step:    15040, time: 2.344, loss: 0.041630
step:    15060, time: 2.372, loss: 0.049686
step:    15080, time: 2.327, loss: 0.045540
step:    15100, time: 2.284, loss: 0.042593
step:    15120, time: 2.414, loss: 0.050342
step:    15140, time: 2.349, loss: 0.048722
step:    15160, time: 2.370, loss: 0.050651
step:    15180, time: 2.377, loss: 0.048025
step:    15200, time: 2.317, loss: 0.048889
step:    15220, time: 2.238, loss: 0.051876
step:    15240, time: 2.410, loss: 0.054091
step:    15260, time: 2.368, loss: 0.048034
step:    15280, time: 2.347, loss: 0.044678
step:    15300, time: 2.383, loss: 0.052092
step:    15320, time: 2.379, loss: 0.040967
step:    15340, time: 2.395, loss: 0.051288
step:    15360, time: 2.391, loss: 0.046124
step:    15380, time: 2.358, loss: 0.050650
step:    15400, time: 2.323, loss: 0.045245
step:    15420, time: 2.355, loss: 0.047512
step:    15440, time: 2.364, loss: 0.050697
step:    15460, time: 2.320, loss: 0.040807
step:    15480, time: 2.393, loss: 0.054204
step:    15500, time: 2.434, loss: 0.055451
step:    15520, time: 2.291, loss: 0.050232
step:    15540, time: 2.174, loss: 0.042776
step:    15560, time: 2.470, loss: 0.056484
step:    15580, time: 2.409, loss: 0.045687
step:    15600, time: 2.412, loss: 0.049380
step:    15620, time: 2.392, loss: 0.052365
step:    15640, time: 2.417, loss: 0.051695
step:    15660, time: 2.428, loss: 0.055826
step:    15680, time: 2.466, loss: 0.053842
step:    15700, time: 2.330, loss: 0.055801
step:    15720, time: 2.348, loss: 0.042008
step:    15740, time: 2.337, loss: 0.045983
step:    15760, time: 2.375, loss: 0.051464
step:    15780, time: 2.386, loss: 0.056682
step:    15800, time: 2.373, loss: 0.045880
step:    15820, time: 2.405, loss: 0.045673
step:    15840, time: 2.315, loss: 0.071149
step:    15860, time: 2.297, loss: 0.060087
step:    15880, time: 2.395, loss: 0.043124
step:    15900, time: 2.360, loss: 0.043910
step:    15920, time: 2.301, loss: 0.044798
step:    15940, time: 2.327, loss: 0.049549
step:    15960, time: 2.426, loss: 0.062220
step:    15980, time: 2.330, loss: 0.047671
step:    16000, time: 2.280, loss: 0.046707
step:    16020, time: 2.454, loss: 0.052564
step:    16040, time: 2.335, loss: 0.047922
step:    16060, time: 2.414, loss: 0.058056
step:    16080, time: 2.391, loss: 0.049580
step:    16100, time: 2.384, loss: 0.052357
step:    16120, time: 2.272, loss: 0.047209
step:    16140, time: 2.229, loss: 0.042144
step:    16160, time: 2.194, loss: 0.042418
step:    16180, time: 2.348, loss: 0.045340
step:    16200, time: 2.435, loss: 0.053654
step:    16220, time: 2.384, loss: 0.052065
step:    16240, time: 2.487, loss: 0.059981
step:    16260, time: 2.232, loss: 0.039850
step:    16280, time: 2.319, loss: 0.041645
step:    16300, time: 2.365, loss: 0.044806
step:    16320, time: 2.399, loss: 0.047848
step:    16340, time: 2.340, loss: 0.050946
step:    16360, time: 2.325, loss: 0.055328
step:    16380, time: 2.415, loss: 0.050166
step:    16400, time: 2.427, loss: 0.055529
step:    16420, time: 2.409, loss: 0.050399
step:    16440, time: 2.379, loss: 0.054143
step:    16460, time: 2.174, loss: 0.042661
step:    16480, time: 2.287, loss: 0.059325
step:    16500, time: 2.415, loss: 0.047162
step:    16520, time: 2.319, loss: 0.045299
step:    16540, time: 2.386, loss: 0.055281
step:    16560, time: 2.308, loss: 0.057895
step:    16580, time: 2.336, loss: 0.053394
step:    16600, time: 2.395, loss: 0.049687
step:    16620, time: 2.419, loss: 0.052013
step:    16640, time: 2.378, loss: 0.057399
step:    16660, time: 2.390, loss: 0.047940
step:    16680, time: 2.339, loss: 0.054326
step:    16700, time: 2.429, loss: 0.050752
step:    16720, time: 2.316, loss: 0.046413
step:    16740, time: 2.374, loss: 0.048846
step:    16760, time: 2.392, loss: 0.054263
step:    16780, time: 2.274, loss: 0.055125
step:    16800, time: 3.112, loss: 0.049418
step:    16820, time: 2.462, loss: 0.057717
step:    16840, time: 2.397, loss: 0.045417
step:    16860, time: 2.349, loss: 0.047833
step:    16880, time: 2.325, loss: 0.042365
step:    16900, time: 2.374, loss: 0.054700
step:    16920, time: 2.317, loss: 0.041014
step:    16940, time: 2.367, loss: 0.052025
step:    16960, time: 2.302, loss: 0.047752
step:    16980, time: 2.450, loss: 0.048496
step:    17000, time: 2.397, loss: 0.053454
step:    17020, time: 2.380, loss: 0.046162
step:    17040, time: 2.371, loss: 0.052549
step:    17060, time: 2.323, loss: 0.041264
step:    17080, time: 2.236, loss: 0.052278
step:    17100, time: 2.235, loss: 0.045370
step:    17120, time: 2.438, loss: 0.053329
step:    17140, time: 2.381, loss: 0.048615
step:    17160, time: 2.450, loss: 0.064509
step:    17180, time: 2.417, loss: 0.050565
step:    17200, time: 2.368, loss: 0.047903
step:    17220, time: 2.381, loss: 0.053865
step:    17240, time: 2.375, loss: 0.048396
step:    17260, time: 2.351, loss: 0.049301
step:    17280, time: 2.426, loss: 0.053425
step:    17300, time: 2.411, loss: 0.052214
step:    17320, time: 2.380, loss: 0.055225
step:    17340, time: 2.374, loss: 0.048609
step:    17360, time: 2.397, loss: 0.051200
step:    17380, time: 2.345, loss: 0.053925
step:    17400, time: 2.302, loss: 0.048443
step:    17420, time: 2.285, loss: 0.056720
step:    17440, time: 2.429, loss: 0.045945
step:    17460, time: 2.307, loss: 0.052020
step:    17480, time: 2.401, loss: 0.058693
step:    17500, time: 2.332, loss: 0.053257
step:    17520, time: 2.316, loss: 0.049158
step:    17540, time: 2.321, loss: 0.046502
step:    17560, time: 2.386, loss: 0.056987
step:    17580, time: 2.361, loss: 0.049983
step:    17600, time: 2.434, loss: 0.056150
step:    17620, time: 2.354, loss: 0.046082
step:    17640, time: 2.374, loss: 0.050515
step:    17660, time: 2.368, loss: 0.041904
step:    17680, time: 2.417, loss: 0.050637
step:    17700, time: 2.280, loss: 0.047349
step:    17720, time: 2.241, loss: 0.049121
step:    17740, time: 2.396, loss: 0.052156
step:    17760, time: 2.361, loss: 0.048828
step:    17780, time: 2.308, loss: 0.039815
step:    17800, time: 2.437, loss: 0.054501
step:    17820, time: 2.325, loss: 0.044943
step:    17840, time: 2.407, loss: 0.046185
step:    17860, time: 2.387, loss: 0.042739
step:    17880, time: 2.431, loss: 0.059845
step:    17900, time: 2.376, loss: 0.050808
step:    17920, time: 2.310, loss: 0.050374
step:    17940, time: 2.333, loss: 0.045724
step:    17960, time: 2.369, loss: 0.049983
step:    17980, time: 2.325, loss: 0.042326
step:    18000, time: 2.323, loss: 0.043797
step:    18020, time: 2.194, loss: 0.043139
step:    18040, time: 2.246, loss: 0.042809
step:    18060, time: 2.363, loss: 0.045953
step:    18080, time: 3.431, loss: 0.050801
step:    18100, time: 2.340, loss: 0.044765
step:    18120, time: 2.388, loss: 0.052924
step:    18140, time: 2.363, loss: 0.046380
step:    18160, time: 2.441, loss: 0.058950
step:    18180, time: 2.364, loss: 0.048637
step:    18200, time: 2.371, loss: 0.051287
step:    18220, time: 2.380, loss: 0.047002
step:    18240, time: 2.313, loss: 0.051184
step:    18260, time: 2.368, loss: 0.044344
step:    18280, time: 2.422, loss: 0.050229
step:    18300, time: 2.374, loss: 0.052701
step:    18320, time: 2.385, loss: 0.049629
step:    18340, time: 2.273, loss: 0.058406
step:    18360, time: 2.468, loss: 0.055390
step:    18380, time: 2.378, loss: 0.053362
step:    18400, time: 2.356, loss: 0.042655
step:    18420, time: 2.464, loss: 0.062730
step:    18440, time: 2.326, loss: 0.052989
step:    18460, time: 2.384, loss: 0.055947
step:    18480, time: 2.383, loss: 0.043932
step:    18500, time: 2.353, loss: 0.047434
step:    18520, time: 2.395, loss: 0.051657
step:    18540, time: 2.399, loss: 0.047316
step:    18560, time: 2.326, loss: 0.042947
step:    18580, time: 2.317, loss: 0.048620
step:    18600, time: 2.478, loss: 0.054554
step:    18620, time: 2.273, loss: 0.041626
step:    18640, time: 2.259, loss: 0.047234
step:    18660, time: 2.217, loss: 0.048980
step:    18680, time: 2.357, loss: 0.047983
step:    18700, time: 2.425, loss: 0.053443
step:    18720, time: 2.371, loss: 0.055372
step:    18740, time: 2.310, loss: 0.043197
step:    18760, time: 2.356, loss: 0.049769
step:    18780, time: 2.420, loss: 0.063513
step:    18800, time: 2.361, loss: 0.046681
step:    18820, time: 2.359, loss: 0.044935
step:    18840, time: 2.444, loss: 0.065182
step:    18860, time: 2.319, loss: 0.044993
step:    18880, time: 2.473, loss: 0.057220
step:    18900, time: 2.395, loss: 0.049966
step:    18920, time: 2.407, loss: 0.046690
step:    18940, time: 2.277, loss: 0.043637
step:    18960, time: 2.351, loss: 0.057781
step:    18980, time: 2.199, loss: 0.050119
step:    19000, time: 2.416, loss: 0.050835
step:    19020, time: 2.309, loss: 0.046189
step:    19040, time: 2.370, loss: 0.055841
step:    19060, time: 2.358, loss: 0.043398
step:    19080, time: 2.316, loss: 0.045737
step:    19100, time: 2.291, loss: 0.039921
step:    19120, time: 2.389, loss: 0.048143
step:    19140, time: 2.328, loss: 0.046340
step:    19160, time: 2.398, loss: 0.050202
step:    19180, time: 2.345, loss: 0.056325
step:    19200, time: 2.348, loss: 0.047191
step:    19220, time: 2.429, loss: 0.054646
step:    19240, time: 2.299, loss: 0.043083
step:    19260, time: 2.415, loss: 0.057880
step:    19280, time: 2.192, loss: 0.045286
step:    19300, time: 2.432, loss: 0.046272
step:    19320, time: 2.341, loss: 0.043261
step:    19340, time: 2.403, loss: 0.046921
step:    19360, time: 2.380, loss: 0.047013
step:    19380, time: 2.392, loss: 0.049294
step:    19400, time: 2.372, loss: 0.052053
step:    19420, time: 2.355, loss: 0.047141
step:    19440, time: 2.352, loss: 0.050055
step:    19460, time: 2.370, loss: 0.051150
step:    19480, time: 2.381, loss: 0.049310
step:    19500, time: 2.306, loss: 0.044813
step:    19520, time: 2.358, loss: 0.047256
step:    19540, time: 2.311, loss: 0.053176
step:    19560, time: 2.334, loss: 0.044263
step:    19580, time: 2.241, loss: 0.048018
step:    19600, time: 2.319, loss: 0.049544
step:    19620, time: 2.392, loss: 0.048286
step:    19640, time: 2.287, loss: 0.040625
step:    19660, time: 2.320, loss: 0.046448
step:    19680, time: 2.360, loss: 0.045101
step:    19700, time: 2.333, loss: 0.044217
step:    19720, time: 2.367, loss: 0.046199
step:    19740, time: 2.347, loss: 0.051901
step:    19760, time: 2.340, loss: 0.043489
step:    19780, time: 2.349, loss: 0.046505
step:    19800, time: 2.470, loss: 0.061562
step:    19820, time: 2.418, loss: 0.050730
step:    19840, time: 2.321, loss: 0.045063
step:    19860, time: 2.340, loss: 0.044596
step:    19880, time: 2.342, loss: 0.054586
step:    19900, time: 2.269, loss: 0.059684
step:    19920, time: 4.532, loss: 0.045749
step:    19940, time: 2.499, loss: 0.055299
step:    19960, time: 2.281, loss: 0.039709
step:    19980, time: 2.296, loss: 0.046491
step:    20000, time: 2.479, loss: 0.051016
step:    20020, time: 2.664, loss: 0.056027
step:    20040, time: 2.571, loss: 0.051871
step:    20060, time: 2.514, loss: 0.048556
step:    20080, time: 2.544, loss: 0.048061
step:    20100, time: 2.560, loss: 0.046439
step:    20120, time: 2.579, loss: 0.054578
step:    20140, time: 2.499, loss: 0.052716
step:    20160, time: 2.573, loss: 0.057847
step:    20180, time: 2.554, loss: 0.045039
step:    20200, time: 2.464, loss: 0.046948
step:    20220, time: 2.381, loss: 0.042360
step:    20240, time: 2.513, loss: 0.043235
step:    20260, time: 2.570, loss: 0.047050
step:    20280, time: 2.572, loss: 0.052785
step:    20300, time: 2.475, loss: 0.049820
step:    20320, time: 2.576, loss: 0.045600
step:    20340, time: 2.494, loss: 0.042982
step:    20360, time: 2.548, loss: 0.049310
step:    20380, time: 2.566, loss: 0.046167
step:    20400, time: 2.500, loss: 0.044745
step:    20420, time: 2.465, loss: 0.035278
step:    20440, time: 2.633, loss: 0.049537
step:    20460, time: 2.680, loss: 0.062546
step:    20480, time: 2.638, loss: 0.054722
step:    20500, time: 2.582, loss: 0.050658
step:    20520, time: 2.394, loss: 0.047471
step:    20540, time: 2.317, loss: 0.040678
step:    20560, time: 2.630, loss: 0.046462
step:    20580, time: 2.578, loss: 0.047847
step:    20600, time: 2.664, loss: 0.057623
step:    20620, time: 2.528, loss: 0.048798
step:    20640, time: 2.523, loss: 0.049980
step:    20660, time: 2.600, loss: 0.056020
step:    20680, time: 2.574, loss: 0.049638
step:    20700, time: 2.551, loss: 0.042185
step:    20720, time: 2.478, loss: 0.047908
step:    20740, time: 2.660, loss: 0.049682
step:    20760, time: 2.542, loss: 0.044336
step:    20780, time: 2.480, loss: 0.044338
step:    20800, time: 2.465, loss: 0.046136
step:    20820, time: 2.376, loss: 0.042133
step:    20840, time: 2.408, loss: 0.052528
step:    20860, time: 2.565, loss: 0.048140
step:    20880, time: 2.448, loss: 0.041271
step:    20900, time: 2.618, loss: 0.046793
step:    20920, time: 2.493, loss: 0.044558
step:    20940, time: 2.613, loss: 0.051701
step:    20960, time: 2.561, loss: 0.041620
step:    20980, time: 2.582, loss: 0.052944
step:    21000, time: 2.568, loss: 0.057268
step:    21020, time: 2.616, loss: 0.046527
step:    21040, time: 2.573, loss: 0.045193
step:    21060, time: 2.577, loss: 0.051950
step:    21080, time: 2.561, loss: 0.048726
step:    21100, time: 2.573, loss: 0.054766
step:    21120, time: 2.605, loss: 0.051692
step:    21140, time: 2.389, loss: 0.046189
step:    21160, time: 2.369, loss: 0.048918
step:    21180, time: 2.645, loss: 0.046073
step:    21200, time: 2.506, loss: 0.043149
step:    21220, time: 2.597, loss: 0.050334
step:    21240, time: 2.559, loss: 0.053900
step:    21260, time: 2.596, loss: 0.045773
step:    21280, time: 2.600, loss: 0.046145
step:    21300, time: 2.563, loss: 0.055137
step:    21320, time: 2.602, loss: 0.048193
step:    21340, time: 2.581, loss: 0.047873
step:    21360, time: 2.586, loss: 0.047564
step:    21380, time: 2.627, loss: 0.050030
step:    21400, time: 2.566, loss: 0.045885
step:    21420, time: 2.605, loss: 0.046934
step:    21440, time: 2.498, loss: 0.049241
step:    21460, time: 2.375, loss: 0.060234
step:    21480, time: 2.628, loss: 0.054412
step:    21500, time: 2.474, loss: 0.039260
step:    21520, time: 2.560, loss: 0.047889
step:    21540, time: 2.626, loss: 0.060032
step:    21560, time: 2.531, loss: 0.052368
step:    21580, time: 2.570, loss: 0.051645
step:    21600, time: 2.545, loss: 0.051114
step:    21620, time: 2.492, loss: 0.044871
step:    21640, time: 2.621, loss: 0.053273
step:    21660, time: 2.535, loss: 0.042482
step:    21680, time: 2.622, loss: 0.048311
step:    21700, time: 2.550, loss: 0.046436
step:    21720, time: 2.576, loss: 0.052335
step:    21740, time: 2.528, loss: 0.045558
step:    21760, time: 2.379, loss: 0.048506
step:    21780, time: 2.459, loss: 0.049878
step:    21800, time: 2.539, loss: 0.040530
step:    21820, time: 2.621, loss: 0.057321
step:    21840, time: 2.512, loss: 0.045889
step:    21860, time: 2.442, loss: 0.045578
step:    21880, time: 2.615, loss: 0.049859
step:    21900, time: 2.532, loss: 0.046257
step:    21920, time: 2.458, loss: 0.040912
step:    21940, time: 2.521, loss: 0.045143
step:    21960, time: 2.536, loss: 0.051718
step:    21980, time: 2.667, loss: 0.064521
step:    22000, time: 2.527, loss: 0.047692
step:    22020, time: 2.549, loss: 0.056400
step:    22040, time: 2.485, loss: 0.038468
step:    22060, time: 2.534, loss: 0.055882
step:    22080, time: 2.415, loss: 0.049338
step:    22100, time: 2.416, loss: 0.047833
step:    22120, time: 2.478, loss: 0.042373
step:    22140, time: 2.581, loss: 0.045544
step:    22160, time: 2.523, loss: 0.042776
step:    22180, time: 2.528, loss: 0.045808
step:    22200, time: 2.561, loss: 0.045608
step:    22220, time: 2.506, loss: 0.055925
step:    22240, time: 2.514, loss: 0.046421
step:    22260, time: 2.557, loss: 0.049559
step:    22280, time: 2.539, loss: 0.045979
step:    22300, time: 2.575, loss: 0.052558
step:    22320, time: 2.550, loss: 0.053021
step:    22340, time: 2.476, loss: 0.043323
step:    22360, time: 2.510, loss: 0.040626
step:    22380, time: 2.389, loss: 0.046456
step:    22400, time: 2.332, loss: 0.044005
step:    22420, time: 2.530, loss: 0.052418
step:    22440, time: 2.613, loss: 0.048645
step:    22460, time: 2.603, loss: 0.057617
step:    22480, time: 2.583, loss: 0.059296
step:    22500, time: 2.638, loss: 0.052358
step:    22520, time: 2.616, loss: 0.048752
step:    22540, time: 2.563, loss: 0.050121
step:    22560, time: 2.521, loss: 0.049597
step:    22580, time: 2.614, loss: 0.049380
step:    22600, time: 2.503, loss: 0.048190
step:    22620, time: 2.508, loss: 0.045125
step:    22640, time: 2.502, loss: 0.046176
step:    22660, time: 2.473, loss: 0.047333
step:    22680, time: 2.547, loss: 0.046246
step:    22700, time: 2.404, loss: 0.048283
step:    22720, time: 2.388, loss: 0.049394
step:    22740, time: 2.519, loss: 0.037929
step:    22760, time: 2.542, loss: 0.040711
step:    22780, time: 2.526, loss: 0.046840
step:    22800, time: 2.639, loss: 0.053845
step:    22820, time: 2.494, loss: 0.045286
step:    22840, time: 2.566, loss: 0.048016
step:    22860, time: 2.581, loss: 0.060442
step:    22880, time: 2.503, loss: 0.040842
step:    22900, time: 2.623, loss: 0.053797
step:    22920, time: 2.584, loss: 0.047470
step:    22940, time: 2.677, loss: 0.059026
step:    22960, time: 2.644, loss: 0.053779
step:    22980, time: 2.655, loss: 0.051767
step:    23000, time: 2.436, loss: 0.039796
step:    23020, time: 2.454, loss: 0.052254
step:    23040, time: 2.634, loss: 0.048004
step:    23060, time: 2.553, loss: 0.039997
step:    23080, time: 2.507, loss: 0.048075
step:    23100, time: 2.557, loss: 0.043251
step:    23120, time: 2.558, loss: 0.051309
step:    23140, time: 2.501, loss: 0.040979
step:    23160, time: 2.514, loss: 0.041589
step:    23180, time: 2.542, loss: 0.068119
step:    23200, time: 2.486, loss: 0.051771
step:    23220, time: 2.585, loss: 0.046718
step:    23240, time: 2.522, loss: 0.041681
step:    23260, time: 2.607, loss: 0.051632
step:    23280, time: 2.608, loss: 0.056905
step:    23300, time: 2.503, loss: 0.053502
step:    23320, time: 2.458, loss: 0.057433
step:    23340, time: 2.334, loss: 0.047401
step:    23360, time: 2.616, loss: 0.054298
step:    23380, time: 2.501, loss: 0.043424
step:    23400, time: 2.602, loss: 0.054502
step:    23420, time: 2.573, loss: 0.048498
step:    23440, time: 2.530, loss: 0.044971
step:    23460, time: 2.568, loss: 0.057018
step:    23480, time: 2.587, loss: 0.050723
step:    23500, time: 2.448, loss: 0.043320
step:    23520, time: 2.566, loss: 0.051993
step:    23540, time: 2.475, loss: 0.041768
step:    23560, time: 2.608, loss: 0.061167
step:    23580, time: 2.570, loss: 0.053566
step:    23600, time: 2.439, loss: 0.044753
step:    23620, time: 2.461, loss: 0.046805
step:    23640, time: 2.403, loss: 0.045857
step:    23660, time: 2.417, loss: 0.052076
step:    23680, time: 2.578, loss: 0.049547
step:    23700, time: 2.603, loss: 0.044103
step:    23720, time: 2.623, loss: 0.050581
step:    23740, time: 2.541, loss: 0.051584
step:    23760, time: 2.536, loss: 0.048783
step:    23780, time: 2.526, loss: 0.049020
step:    23800, time: 2.596, loss: 0.045932
step:    23820, time: 2.639, loss: 0.057986
step:    23840, time: 2.554, loss: 0.047311
step:    23860, time: 2.509, loss: 0.042074
step:    23880, time: 2.603, loss: 0.057019
step:    23900, time: 2.625, loss: 0.055759
step:    23920, time: 2.519, loss: 0.044896
step:    23940, time: 2.480, loss: 0.043907
step:    23960, time: 2.381, loss: 0.042896
step:    23980, time: 2.669, loss: 0.052882
step:    24000, time: 2.513, loss: 0.040167
step:    24020, time: 2.640, loss: 0.053555
step:    24040, time: 2.533, loss: 0.042197
step:    24060, time: 2.466, loss: 0.050984
step:    24080, time: 2.539, loss: 0.043860
step:    24100, time: 2.579, loss: 0.048415
step:    24120, time: 2.562, loss: 0.049167
step:    24140, time: 2.593, loss: 0.047197
step:    24160, time: 2.559, loss: 0.044673
step:    24180, time: 2.621, loss: 0.054799
step:    24200, time: 2.554, loss: 0.046274
step:    24220, time: 2.566, loss: 0.053694
step:    24240, time: 2.533, loss: 0.045440
step:    24260, time: 2.320, loss: 0.041455
step:    24280, time: 2.354, loss: 0.049757
step:    24300, time: 2.672, loss: 0.049790
step:    24320, time: 2.533, loss: 0.045714
step:    24340, time: 2.448, loss: 0.039906
step:    24360, time: 2.592, loss: 0.052563
step:    24380, time: 2.595, loss: 0.056862
step:    24400, time: 2.590, loss: 0.047410
step:    24420, time: 2.521, loss: 0.046441
step:    24440, time: 2.547, loss: 0.046354
step:    24460, time: 2.527, loss: 0.047635
step:    24480, time: 2.615, loss: 0.045243
step:    24500, time: 2.625, loss: 0.052572
step:    24520, time: 2.540, loss: 0.041652
step:    24540, time: 2.573, loss: 0.051918
step:    24560, time: 2.476, loss: 0.052336
step:    24580, time: 2.399, loss: 0.050498
step:    24600, time: 4.052, loss: 0.044382
step:    24620, time: 2.617, loss: 0.053657
step:    24640, time: 2.587, loss: 0.043789
step:    24660, time: 2.535, loss: 0.052717
step:    24680, time: 2.615, loss: 0.054663
step:    24700, time: 2.494, loss: 0.041456
step:    24720, time: 2.599, loss: 0.051625
step:    24740, time: 2.514, loss: 0.039108
step:    24760, time: 2.711, loss: 0.063270
step:    24780, time: 2.606, loss: 0.053441
step:    24800, time: 2.546, loss: 0.050554
step:    24820, time: 2.729, loss: 0.060689
step:    24840, time: 2.538, loss: 0.047597
step:    24860, time: 2.557, loss: 0.041844
step:    24880, time: 2.311, loss: 0.044462
step:    24900, time: 2.294, loss: 0.041404
step:    24920, time: 2.634, loss: 0.051933
step:    24940, time: 2.476, loss: 0.042056
step:    24960, time: 2.531, loss: 0.058750
step:    24980, time: 2.564, loss: 0.052127
step:    25000, time: 2.567, loss: 0.042474
step:    25020, time: 2.371, loss: 0.051256
step:    25040, time: 2.348, loss: 0.047868
step:    25060, time: 2.365, loss: 0.052203
step:    25080, time: 2.372, loss: 0.048101
step:    25100, time: 2.387, loss: 0.048988
step:    25120, time: 2.321, loss: 0.047902
step:    25140, time: 2.385, loss: 0.045658
step:    25160, time: 2.341, loss: 0.048356
step:    25180, time: 2.302, loss: 0.044920
step:    25200, time: 2.199, loss: 0.052245
step:    25220, time: 2.202, loss: 0.048048
step:    25240, time: 2.402, loss: 0.053194
step:    25260, time: 2.469, loss: 0.047577
step:    25280, time: 2.424, loss: 0.053255
step:    25300, time: 2.364, loss: 0.044793
step:    25320, time: 2.470, loss: 0.060289
step:    25340, time: 2.458, loss: 0.051326
step:    25360, time: 2.303, loss: 0.041759
step:    25380, time: 2.444, loss: 0.052519
step:    25400, time: 2.323, loss: 0.045730
step:    25420, time: 2.299, loss: 0.050424
step:    25440, time: 2.379, loss: 0.041638
step:    25460, time: 2.410, loss: 0.048100
step:    25480, time: 2.335, loss: 0.043073
step:    25500, time: 2.393, loss: 0.058642
step:    25520, time: 2.169, loss: 0.046317
step:    25540, time: 2.349, loss: 0.041692
step:    25560, time: 2.396, loss: 0.043119
step:    25580, time: 2.322, loss: 0.045728
step:    25600, time: 2.353, loss: 0.042645
step:    25620, time: 2.369, loss: 0.059524
step:    25640, time: 2.319, loss: 0.046874
step:    25660, time: 2.369, loss: 0.047241
step:    25680, time: 2.310, loss: 0.040802
step:    25700, time: 2.438, loss: 0.054277
step:    25720, time: 2.373, loss: 0.046244
step:    25740, time: 2.373, loss: 0.049582
step:    25760, time: 2.328, loss: 0.043367
step:    25780, time: 2.316, loss: 0.049376
step:    25800, time: 2.449, loss: 0.051885
step:    25820, time: 2.252, loss: 0.050702
step:    25840, time: 2.303, loss: 0.051734
step:    25860, time: 2.422, loss: 0.048888
step:    25880, time: 2.314, loss: 0.048283
step:    25900, time: 2.330, loss: 0.046197
step:    25920, time: 2.400, loss: 0.045886
step:    25940, time: 2.319, loss: 0.044426
step:    25960, time: 2.353, loss: 0.045737
step:    25980, time: 2.398, loss: 0.059024
step:    26000, time: 2.216, loss: 0.038604
step:    26020, time: 2.344, loss: 0.049301
step:    26040, time: 2.286, loss: 0.044205
step:    26060, time: 2.401, loss: 0.056961
step:    26080, time: 2.247, loss: 0.042701
step:    26100, time: 2.395, loss: 0.047848
step:    26120, time: 2.283, loss: 0.044611
step:    26140, time: 2.227, loss: 0.056725
step:    26160, time: 3.097, loss: 0.046231
step:    26180, time: 2.320, loss: 0.039201
step:    26200, time: 2.374, loss: 0.048437
step:    26220, time: 2.393, loss: 0.042390
step:    26240, time: 2.454, loss: 0.054998
step:    26260, time: 2.382, loss: 0.049453
step:    26280, time: 2.372, loss: 0.048256
step:    26300, time: 2.383, loss: 0.043965
step:    26320, time: 2.366, loss: 0.051885
step:    26340, time: 2.376, loss: 0.056160
step:    26360, time: 2.334, loss: 0.052432
step:    26380, time: 2.385, loss: 0.052346
step:    26400, time: 2.328, loss: 0.047127
step:    26420, time: 2.350, loss: 0.046036
step:    26440, time: 2.272, loss: 0.050058
step:    26460, time: 2.267, loss: 0.055056
step:    26480, time: 2.445, loss: 0.046776
step:    26500, time: 2.378, loss: 0.048660
step:    26520, time: 2.232, loss: 0.039459
step:    26540, time: 2.372, loss: 0.049725
step:    26560, time: 2.406, loss: 0.057874
step:    26580, time: 2.365, loss: 0.045104
step:    26600, time: 2.335, loss: 0.039770
step:    26620, time: 2.340, loss: 0.044713
step:    26640, time: 2.396, loss: 0.056683
step:    26660, time: 2.374, loss: 0.051189
step:    26680, time: 2.320, loss: 0.041185
step:    26700, time: 2.461, loss: 0.059715
step:    26720, time: 2.352, loss: 0.043634
step:    26740, time: 2.381, loss: 0.054678
step:    26760, time: 2.358, loss: 0.063514
step:    26780, time: 2.351, loss: 0.061740
step:    26800, time: 2.434, loss: 0.061249
step:    26820, time: 2.348, loss: 0.048034
step:    26840, time: 2.303, loss: 0.046104
step:    26860, time: 2.389, loss: 0.046453
step:    26880, time: 2.416, loss: 0.054188
step:    26900, time: 2.334, loss: 0.041062
step:    26920, time: 2.347, loss: 0.051821
step:    26940, time: 2.390, loss: 0.047730
step:    26960, time: 2.374, loss: 0.049693
step:    26980, time: 2.403, loss: 0.052478
step:    27000, time: 2.396, loss: 0.048661
step:    27020, time: 2.357, loss: 0.048801
step:    27040, time: 2.353, loss: 0.046940
step:    27060, time: 2.384, loss: 0.047936
step:    27080, time: 2.182, loss: 0.047931
step:    27100, time: 2.362, loss: 0.046743
step:    27120, time: 2.422, loss: 0.049094
step:    27140, time: 2.350, loss: 0.047945
step:    27160, time: 2.376, loss: 0.047889
step:    27180, time: 2.353, loss: 0.044657
step:    27200, time: 2.315, loss: 0.048694
step:    27220, time: 2.418, loss: 0.058309
step:    27240, time: 2.325, loss: 0.042561
step:    27260, time: 2.380, loss: 0.044557
step:    27280, time: 2.339, loss: 0.044195
step:    27300, time: 2.396, loss: 0.046841
step:    27320, time: 2.392, loss: 0.050215
step:    27340, time: 2.391, loss: 0.054076
step:    27360, time: 2.435, loss: 0.057361
step:    27380, time: 2.291, loss: 0.051846
step:    27400, time: 2.242, loss: 0.047668
step:    27420, time: 2.406, loss: 0.054016
step:    27440, time: 2.456, loss: 0.050065
step:    27460, time: 2.401, loss: 0.048924
step:    27480, time: 2.430, loss: 0.047785
step:    27500, time: 2.295, loss: 0.042676
step:    27520, time: 2.351, loss: 0.056312
step:    27540, time: 2.335, loss: 0.042700
step:    27560, time: 2.359, loss: 0.046913
step:    27580, time: 2.303, loss: 0.046108
step:    27600, time: 2.362, loss: 0.050598
step:    27620, time: 2.380, loss: 0.046831
step:    27640, time: 2.417, loss: 0.054996
step:    27660, time: 2.290, loss: 0.044653
step:    27680, time: 2.285, loss: 0.046729
step:    27700, time: 2.194, loss: 0.039812
step:    27720, time: 2.399, loss: 0.047160
step:    27740, time: 2.353, loss: 0.043161
step:    27760, time: 2.449, loss: 0.045470
step:    27780, time: 2.297, loss: 0.043179
step:    27800, time: 2.398, loss: 0.049038
step:    27820, time: 2.384, loss: 0.055176
step:    27840, time: 2.350, loss: 0.047258
step:    27860, time: 2.444, loss: 0.047655
step:    27880, time: 2.350, loss: 0.062158
step:    27900, time: 2.335, loss: 0.048474
step:    27920, time: 2.505, loss: 0.064641
step:    27940, time: 2.458, loss: 0.059661
step:    27960, time: 2.334, loss: 0.049425
step:    27980, time: 2.404, loss: 0.046513
step:    28000, time: 2.345, loss: 0.052840
step:    28020, time: 2.192, loss: 0.045914
step:    28040, time: 2.381, loss: 0.052644
step:    28060, time: 2.397, loss: 0.046811
step:    28080, time: 2.360, loss: 0.051049
step:    28100, time: 2.367, loss: 0.046082
step:    28120, time: 2.344, loss: 0.048332
step:    28140, time: 2.372, loss: 0.049631
step:    28160, time: 2.262, loss: 0.042885
step:    28180, time: 2.366, loss: 0.048540
step:    28200, time: 2.355, loss: 0.045949
step:    28220, time: 2.356, loss: 0.046610
step:    28240, time: 2.429, loss: 0.051349
step:    28260, time: 2.335, loss: 0.045578
step:    28280, time: 2.394, loss: 0.053880
step:    28300, time: 2.361, loss: 0.048300
step:    28320, time: 2.285, loss: 0.047722
step:    28340, time: 2.278, loss: 0.058476
step:    28360, time: 2.434, loss: 0.049776
step:    28380, time: 2.387, loss: 0.055094
step:    28400, time: 2.356, loss: 0.043344
step:    28420, time: 2.404, loss: 0.057580
step:    28440, time: 2.435, loss: 0.055016
step:    28460, time: 2.332, loss: 0.045667
step:    28480, time: 2.361, loss: 0.045240
step:    28500, time: 2.404, loss: 0.049295
step:    28520, time: 2.339, loss: 0.045679
step:    28540, time: 2.363, loss: 0.048368
step:    28560, time: 2.365, loss: 0.043322
step:    28580, time: 2.420, loss: 0.053647
step:    28600, time: 2.369, loss: 0.041507
step:    28620, time: 2.314, loss: 0.044286
step:    28640, time: 2.171, loss: 0.047725
step:    28660, time: 2.404, loss: 0.049768
step:    28680, time: 2.318, loss: 0.041909
step:    28700, time: 2.304, loss: 0.042640
step:    28720, time: 2.391, loss: 0.055985
step:    28740, time: 2.413, loss: 0.050225
step:    28760, time: 2.357, loss: 0.047302
step:    28780, time: 2.439, loss: 0.055354
step:    28800, time: 2.395, loss: 0.045931
step:    28820, time: 2.303, loss: 0.045200
step:    28840, time: 2.382, loss: 0.050948
step:    28860, time: 2.304, loss: 0.043292
step:    28880, time: 2.367, loss: 0.042167
step:    28900, time: 2.401, loss: 0.047208
step:    28920, time: 2.420, loss: 0.046971
step:    28940, time: 2.224, loss: 0.043515
step:    28960, time: 2.226, loss: 0.048698
step:    28980, time: 2.469, loss: 0.051238
step:    29000, time: 2.494, loss: 0.063538
step:    29020, time: 2.433, loss: 0.050560
step:    29040, time: 2.358, loss: 0.045806
step:    29060, time: 2.368, loss: 0.049907
step:    29080, time: 2.344, loss: 0.039568
step:    29100, time: 2.406, loss: 0.056988
step:    29120, time: 2.387, loss: 0.054256
step:    29140, time: 2.449, loss: 0.049236
step:    29160, time: 2.418, loss: 0.055541
step:    29180, time: 2.275, loss: 0.043690
step:    29200, time: 2.302, loss: 0.044100
step:    29220, time: 2.381, loss: 0.045815
step:    29240, time: 2.334, loss: 0.050309
step:    29260, time: 2.191, loss: 0.042028
step:    29280, time: 3.230, loss: 0.050485
step:    29300, time: 2.347, loss: 0.047016
step:    29320, time: 2.334, loss: 0.039777
step:    29340, time: 2.398, loss: 0.049667
step:    29360, time: 2.364, loss: 0.049656
step:    29380, time: 2.496, loss: 0.063169
step:    29400, time: 2.325, loss: 0.047286
step:    29420, time: 2.402, loss: 0.046143
step:    29440, time: 2.417, loss: 0.054632
step:    29460, time: 2.323, loss: 0.041550
step:    29480, time: 2.425, loss: 0.052344
step:    29500, time: 2.435, loss: 0.051939
step:    29520, time: 2.424, loss: 0.058063
step:    29540, time: 2.341, loss: 0.053505
step:    29560, time: 2.211, loss: 0.042206
step:    29580, time: 2.264, loss: 0.055070
step:    29600, time: 2.429, loss: 0.045618
step:    29620, time: 2.358, loss: 0.046334
step:    29640, time: 2.248, loss: 0.040987
step:    29660, time: 2.352, loss: 0.047146
step:    29680, time: 2.404, loss: 0.048256
step:    29700, time: 2.372, loss: 0.040778
step:    29720, time: 2.456, loss: 0.050685
step:    29740, time: 2.530, loss: 0.060404
step:    29760, time: 2.456, loss: 0.057803
step:    29780, time: 2.357, loss: 0.048591
step:    29800, time: 2.355, loss: 0.040611
step:    29820, time: 2.348, loss: 0.045468
step:    29840, time: 2.330, loss: 0.046793
step:    29860, time: 2.398, loss: 0.053281
step:    29880, time: 2.211, loss: 0.035466
step:    29900, time: 2.212, loss: 0.042526
step:    29920, time: 2.352, loss: 0.047825
step:    29940, time: 2.333, loss: 0.041057
step:    29960, time: 2.359, loss: 0.046544
step:    29980, time: 2.375, loss: 0.051637
step:    30000, time: 2.319, loss: 0.040768
step:    30020, time: 2.389, loss: 0.052528
step:    30040, time: 2.331, loss: 0.045964
step:    30060, time: 2.391, loss: 0.044921
step:    30080, time: 2.323, loss: 0.049984
step:    30100, time: 2.330, loss: 0.045786
step:    30120, time: 2.393, loss: 0.044082
step:    30140, time: 2.298, loss: 0.043095
step:    30160, time: 2.339, loss: 0.044646
step:    30180, time: 2.282, loss: 0.038916
step:    30200, time: 2.219, loss: 0.047658
step:    30220, time: 2.408, loss: 0.048671
step:    30240, time: 2.392, loss: 0.049699
step:    30260, time: 2.319, loss: 0.037021
step:    30280, time: 2.336, loss: 0.047341
step:    30300, time: 2.428, loss: 0.057694
step:    30320, time: 2.390, loss: 0.050388
step:    30340, time: 2.394, loss: 0.047597
step:    30360, time: 2.366, loss: 0.045423
step:    30380, time: 2.429, loss: 0.048450
step:    30400, time: 2.362, loss: 0.048293
step:    30420, time: 2.315, loss: 0.040388
step:    30440, time: 2.451, loss: 0.050548
step:    30460, time: 2.379, loss: 0.050197
step:    30480, time: 2.400, loss: 0.048420
step:    30500, time: 2.211, loss: 0.048997
step:    30520, time: 2.257, loss: 0.046114
step:    30540, time: 2.403, loss: 0.047396
step:    30560, time: 2.460, loss: 0.052155
step:    30580, time: 2.355, loss: 0.046215
step:    30600, time: 2.396, loss: 0.050149
step:    30620, time: 2.399, loss: 0.050047
step:    30640, time: 2.420, loss: 0.051646
step:    30660, time: 2.394, loss: 0.049784
step:    30680, time: 2.390, loss: 0.047474
step:    30700, time: 2.369, loss: 0.048723
step:    30720, time: 2.333, loss: 0.041354
step:    30740, time: 2.420, loss: 0.047012
step:    30760, time: 2.416, loss: 0.054089
step:    30780, time: 2.240, loss: 0.036716
step:    30800, time: 2.393, loss: 0.051744
step:    30820, time: 2.322, loss: 0.051999
step:    30840, time: 2.444, loss: 0.050934
step:    30860, time: 2.405, loss: 0.050568
step:    30880, time: 2.486, loss: 0.054844
step:    30900, time: 2.363, loss: 0.047173
step:    30920, time: 2.293, loss: 0.038110
step:    30940, time: 2.407, loss: 0.060167
step:    30960, time: 2.296, loss: 0.039522
step:    30980, time: 2.358, loss: 0.042378
step:    31000, time: 2.369, loss: 0.048637
step:    31020, time: 2.368, loss: 0.047213
step:    31040, time: 2.397, loss: 0.047826
step:    31060, time: 2.410, loss: 0.047743
step:    31080, time: 2.410, loss: 0.051330
step:    31100, time: 2.421, loss: 0.047683
step:    31120, time: 2.233, loss: 0.042438
step:    31140, time: 2.276, loss: 0.050376
step:    31160, time: 2.402, loss: 0.050223
step:    31180, time: 2.348, loss: 0.048979
step:    31200, time: 2.414, loss: 0.045483
step:    31220, time: 2.377, loss: 0.053553
step:    31240, time: 3.125, loss: 0.059972
step:    31260, time: 2.299, loss: 0.039156
step:    31280, time: 2.486, loss: 0.052845
step:    31300, time: 2.423, loss: 0.057005
step:    31320, time: 2.483, loss: 0.053341
step:    31340, time: 2.477, loss: 0.061341
step:    31360, time: 2.400, loss: 0.049442
step:    31380, time: 2.273, loss: 0.043339
step:    31400, time: 2.403, loss: 0.046550
step:    31420, time: 2.356, loss: 0.042249
step:    31440, time: 2.268, loss: 0.055311
step:    31460, time: 2.181, loss: 0.047998
step:    31480, time: 2.350, loss: 0.045172
step:    31500, time: 2.381, loss: 0.049215
step:    31520, time: 2.286, loss: 0.042090
step:    31540, time: 2.374, loss: 0.048935
step:    31560, time: 2.357, loss: 0.049823
step:    31580, time: 2.309, loss: 0.044954
step:    31600, time: 2.323, loss: 0.037562
step:    31620, time: 2.364, loss: 0.047267
step:    31640, time: 2.345, loss: 0.044995
step:    31660, time: 2.353, loss: 0.048971
step:    31680, time: 2.393, loss: 0.044361
step:    31700, time: 2.370, loss: 0.045353
step:    31720, time: 2.404, loss: 0.048905
step:    31740, time: 2.226, loss: 0.041815
step:    31760, time: 2.166, loss: 0.041627
step:    31780, time: 2.350, loss: 0.049083
step:    31800, time: 2.347, loss: 0.042060
step:    31820, time: 2.392, loss: 0.047139
step:    31840, time: 2.403, loss: 0.057262
step:    31860, time: 2.407, loss: 0.044493
step:    31880, time: 2.353, loss: 0.044229
step:    31900, time: 2.414, loss: 0.047672
step:    31920, time: 2.414, loss: 0.047180
step:    31940, time: 2.501, loss: 0.056194
step:    31960, time: 2.363, loss: 0.051489
step:    31980, time: 2.352, loss: 0.048479
step:    32000, time: 2.339, loss: 0.044776
step:    32020, time: 2.412, loss: 0.051836
step:    32040, time: 2.421, loss: 0.051963
step:    32060, time: 2.278, loss: 0.042625
step:    32080, time: 2.242, loss: 0.044761
step:    32100, time: 2.422, loss: 0.049286
step:    32120, time: 2.432, loss: 0.057582
step:    32140, time: 2.336, loss: 0.044201
step:    32160, time: 2.436, loss: 0.050787
step:    32180, time: 2.346, loss: 0.039106
step:    32200, time: 2.406, loss: 0.049506
step:    32220, time: 2.330, loss: 0.044758
step:    32240, time: 2.288, loss: 0.039627
step:    32260, time: 2.311, loss: 0.042965
step:    32280, time: 2.437, loss: 0.051651
step:    32300, time: 2.445, loss: 0.054137
step:    32320, time: 2.356, loss: 0.050316
step:    32340, time: 2.400, loss: 0.050922
step:    32360, time: 2.313, loss: 0.044522
step:    32380, time: 2.263, loss: 0.049417
step:    32400, time: 2.395, loss: 0.043460
step:    32420, time: 2.356, loss: 0.054324
step:    32440, time: 2.361, loss: 0.049404
step:    32460, time: 2.356, loss: 0.044349
step:    32480, time: 2.347, loss: 0.052501
step:    32500, time: 2.366, loss: 0.051364
step:    32520, time: 2.381, loss: 0.045687
step:    32540, time: 2.357, loss: 0.042321
step:    32560, time: 2.314, loss: 0.048747
step:    32580, time: 2.365, loss: 0.049930
step:    32600, time: 2.318, loss: 0.047521
step:    32620, time: 2.379, loss: 0.045063
step:    32640, time: 2.402, loss: 0.046031
step:    32660, time: 2.401, loss: 0.054764
step:    32680, time: 2.381, loss: 0.052852
step:    32700, time: 2.260, loss: 0.049209
step:    32720, time: 2.381, loss: 0.043415
step:    32740, time: 2.514, loss: 0.058937
step:    32760, time: 2.389, loss: 0.045165
step:    32780, time: 2.370, loss: 0.043386
step:    32800, time: 2.391, loss: 0.046096
step:    32820, time: 2.320, loss: 0.047624
step:    32840, time: 2.353, loss: 0.053379
step:    32860, time: 2.363, loss: 0.048556
step:    32880, time: 2.397, loss: 0.049967
step:    32900, time: 2.345, loss: 0.040012
step:    32920, time: 2.335, loss: 0.043498
step:    32940, time: 2.370, loss: 0.045622
step:    32960, time: 2.439, loss: 0.054616
step:    32980, time: 2.295, loss: 0.040262
step:    33000, time: 2.323, loss: 0.049958
step:    33020, time: 2.229, loss: 0.050923
step:    33040, time: 2.415, loss: 0.045134
step:    33060, time: 2.371, loss: 0.054222
step:    33080, time: 2.414, loss: 0.056563
step:    33100, time: 2.457, loss: 0.058618
step:    33120, time: 2.428, loss: 0.059606
step:    33140, time: 2.444, loss: 0.051661
step:    33160, time: 2.382, loss: 0.051492
step:    33180, time: 2.361, loss: 0.053725
step:    33200, time: 2.439, loss: 0.051415
step:    33220, time: 2.312, loss: 0.043271
step:    33240, time: 2.401, loss: 0.056904
step:    33260, time: 2.317, loss: 0.045864
step:    33280, time: 2.254, loss: 0.039874
step:    33300, time: 2.269, loss: 0.054751
step:    33320, time: 2.206, loss: 0.047870
step:    33340, time: 2.414, loss: 0.044695
step:    33360, time: 2.259, loss: 0.042581
step:    33380, time: 2.349, loss: 0.053471
step:    33400, time: 2.351, loss: 0.040353
step:    33420, time: 2.324, loss: 0.044324
step:    33440, time: 2.271, loss: 0.043860
step:    33460, time: 2.350, loss: 0.053380
step:    33480, time: 2.368, loss: 0.052523
step:    33500, time: 2.326, loss: 0.045310
step:    33520, time: 2.409, loss: 0.046267
step:    33540, time: 2.432, loss: 0.052335
step:    33560, time: 2.388, loss: 0.043853
step:    33580, time: 2.288, loss: 0.048690
step:    33600, time: 2.400, loss: 0.055725
step:    33620, time: 2.324, loss: 0.055813
step:    33640, time: 2.168, loss: 0.045945
step:    33660, time: 2.469, loss: 0.049830
step:    33680, time: 2.424, loss: 0.048354
step:    33700, time: 2.434, loss: 0.051439
step:    33720, time: 2.314, loss: 0.042470
step:    33740, time: 2.336, loss: 0.052635
step:    33760, time: 2.428, loss: 0.047908
step:    33780, time: 2.317, loss: 0.039892
step:    33800, time: 2.312, loss: 0.046449
step:    33820, time: 2.214, loss: 0.039334
step:    33840, time: 2.385, loss: 0.045841
step:    33860, time: 2.437, loss: 0.050096
step:    33880, time: 2.315, loss: 0.040127
step:    33900, time: 2.538, loss: 0.057126
step:    33920, time: 2.361, loss: 0.046286
step:    33940, time: 2.273, loss: 0.048077
step:    33960, time: 2.459, loss: 0.045795
step:    33980, time: 2.451, loss: 0.052060
step:    34000, time: 2.379, loss: 0.048570
step:    34020, time: 2.343, loss: 0.044182
step:    34040, time: 2.319, loss: 0.039774
step:    34060, time: 2.419, loss: 0.048818
step:    34080, time: 2.300, loss: 0.046183
step:    34100, time: 2.386, loss: 0.057197
step:    34120, time: 2.316, loss: 0.039133
step:    34140, time: 2.364, loss: 0.045725
step:    34160, time: 2.416, loss: 0.044737
step:    34180, time: 2.293, loss: 0.042868
step:    34200, time: 2.423, loss: 0.053816
step:    34220, time: 2.315, loss: 0.042418
step:    34240, time: 2.246, loss: 0.042173
step:    34260, time: 2.285, loss: 0.054400
step:    34280, time: 2.349, loss: 0.048254
step:    34300, time: 2.386, loss: 0.043022
step:    34320, time: 2.281, loss: 0.040139
step:    34340, time: 2.411, loss: 0.048403
step:    34360, time: 2.395, loss: 0.048854
step:    34380, time: 2.309, loss: 0.048903
step:    34400, time: 2.486, loss: 0.048085
step:    34420, time: 2.355, loss: 0.042931
step:    34440, time: 2.400, loss: 0.052279
step:    34460, time: 2.447, loss: 0.052162
step:    34480, time: 2.394, loss: 0.047130
step:    34500, time: 2.371, loss: 0.039837
step:    34520, time: 2.377, loss: 0.045549
step:    34540, time: 2.311, loss: 0.046283
step:    34560, time: 2.383, loss: 0.056521
step:    34580, time: 2.291, loss: 0.046911
step:    34600, time: 2.407, loss: 0.042482
step:    34620, time: 2.433, loss: 0.052792
step:    34640, time: 2.350, loss: 0.048166
step:    34660, time: 2.330, loss: 0.050734
step:    34680, time: 2.419, loss: 0.048111
step:    34700, time: 2.300, loss: 0.045089
step:    34720, time: 2.366, loss: 0.053311
step:    34740, time: 2.407, loss: 0.051640
step:    34760, time: 2.392, loss: 0.044084
step:    34780, time: 2.369, loss: 0.050474
step:    34800, time: 2.342, loss: 0.040892
step:    34820, time: 2.352, loss: 0.045047
step:    34840, time: 2.404, loss: 0.053272
step:    34860, time: 2.209, loss: 0.047574
step:    34880, time: 2.243, loss: 0.054215
step:    34900, time: 2.410, loss: 0.046443
step:    34920, time: 2.360, loss: 0.047664
step:    34940, time: 2.373, loss: 0.038480
step:    34960, time: 2.385, loss: 0.050004
step:    34980, time: 2.424, loss: 0.049065
step:    35000, time: 2.472, loss: 0.055774
step:    35020, time: 2.636, loss: 0.054785
step:    35040, time: 2.572, loss: 0.051829
step:    35060, time: 2.526, loss: 0.048706
step:    35080, time: 2.561, loss: 0.053958
step:    35100, time: 2.471, loss: 0.042214
step:    35120, time: 2.480, loss: 0.044554
step:    35140, time: 2.587, loss: 0.048629
step:    35160, time: 2.543, loss: 0.044176
step:    35180, time: 2.402, loss: 0.048807
step:    35200, time: 2.429, loss: 0.046930
step:    35220, time: 2.666, loss: 0.054465
step:    35240, time: 2.560, loss: 0.046772
step:    35260, time: 2.554, loss: 0.045833
step:    35280, time: 2.460, loss: 0.044843
step:    35300, time: 2.595, loss: 0.045781
step:    35320, time: 2.520, loss: 0.048529
step:    35340, time: 2.544, loss: 0.047303
step:    35360, time: 2.560, loss: 0.043729
step:    35380, time: 2.547, loss: 0.043720
step:    35400, time: 2.502, loss: 0.045017
step:    35420, time: 2.635, loss: 0.060799
step:    35440, time: 2.570, loss: 0.042933
step:    35460, time: 2.607, loss: 0.055894
step:    35480, time: 2.487, loss: 0.048454
step:    35500, time: 2.333, loss: 0.046157
step:    35520, time: 4.167, loss: 0.040550
step:    35540, time: 2.465, loss: 0.044601
step:    35560, time: 2.479, loss: 0.040498
step:    35580, time: 2.486, loss: 0.043691
step:    35600, time: 2.547, loss: 0.045769
step:    35620, time: 2.549, loss: 0.051472
step:    35640, time: 2.565, loss: 0.052302
step:    35660, time: 2.630, loss: 0.057322
step:    35680, time: 2.589, loss: 0.051261
step:    35700, time: 2.544, loss: 0.041287
step:    35720, time: 2.495, loss: 0.047491
step:    35740, time: 2.588, loss: 0.042524
step:    35760, time: 2.540, loss: 0.043971
step:    35780, time: 2.567, loss: 0.052042
step:    35800, time: 2.458, loss: 0.041235
step:    35820, time: 2.405, loss: 0.050887
step:    35840, time: 2.604, loss: 0.051503
step:    35860, time: 2.517, loss: 0.040112
step:    35880, time: 2.592, loss: 0.050128
step:    35900, time: 2.607, loss: 0.044429
step:    35920, time: 2.574, loss: 0.046271
step:    35940, time: 2.560, loss: 0.050584
step:    35960, time: 2.538, loss: 0.044383
step:    35980, time: 2.532, loss: 0.046231
step:    36000, time: 2.538, loss: 0.043760
step:    36020, time: 2.573, loss: 0.046887
step:    36040, time: 2.539, loss: 0.047002
step:    36060, time: 2.582, loss: 0.051847
step:    36080, time: 2.593, loss: 0.044389
step:    36100, time: 2.555, loss: 0.042918
step:    36120, time: 2.449, loss: 0.045071
step:    36140, time: 2.315, loss: 0.046602
step:    36160, time: 2.658, loss: 0.050458
step:    36180, time: 2.506, loss: 0.045138
step:    36200, time: 2.556, loss: 0.045575
step:    36220, time: 2.543, loss: 0.044011
step:    36240, time: 2.512, loss: 0.037962
step:    36260, time: 2.620, loss: 0.052535
step:    36280, time: 2.541, loss: 0.059204
step:    36300, time: 2.613, loss: 0.049655
step:    36320, time: 2.452, loss: 0.037396
step:    36340, time: 2.640, loss: 0.061565
step:    36360, time: 2.493, loss: 0.040601
step:    36380, time: 2.626, loss: 0.051576
step:    36400, time: 2.498, loss: 0.040627
step:    36420, time: 2.397, loss: 0.040893
step:    36440, time: 2.307, loss: 0.040769
step:    36460, time: 2.523, loss: 0.043337
step:    36480, time: 2.699, loss: 0.047594
step:    36500, time: 2.601, loss: 0.048480
step:    36520, time: 2.577, loss: 0.051718
step:    36540, time: 2.574, loss: 0.048380
step:    36560, time: 2.603, loss: 0.048630
step:    36580, time: 2.598, loss: 0.048714
step:    36600, time: 2.620, loss: 0.049263
step:    36620, time: 2.683, loss: 0.059320
step:    36640, time: 2.557, loss: 0.047337
step:    36660, time: 2.664, loss: 0.045413
step:    36680, time: 2.537, loss: 0.045220
step:    36700, time: 2.589, loss: 0.049459
step:    36720, time: 2.563, loss: 0.046770
step:    36740, time: 2.521, loss: 0.049548
step:    36760, time: 2.313, loss: 0.044398
step:    36780, time: 2.592, loss: 0.045637
step:    36800, time: 2.481, loss: 0.041658
step:    36820, time: 2.526, loss: 0.053124
step:    36840, time: 2.555, loss: 0.044045
step:    36860, time: 2.558, loss: 0.052697
step:    36880, time: 2.565, loss: 0.051005
step:    36900, time: 2.583, loss: 0.050724
step:    36920, time: 2.571, loss: 0.052535
step:    36940, time: 2.512, loss: 0.048659
step:    36960, time: 2.579, loss: 0.058886
step:    36980, time: 2.606, loss: 0.051209
step:    37000, time: 2.512, loss: 0.045966
step:    37020, time: 2.691, loss: 0.058136
step:    37040, time: 2.473, loss: 0.047837
step:    37060, time: 2.298, loss: 0.043352
step:    37080, time: 2.722, loss: 0.057561
step:    37100, time: 2.579, loss: 0.044836
step:    37120, time: 2.542, loss: 0.051631
step:    37140, time: 2.663, loss: 0.046883
step:    37160, time: 2.489, loss: 0.046985
step:    37180, time: 2.554, loss: 0.048508
step:    37200, time: 2.551, loss: 0.043683
step:    37220, time: 2.607, loss: 0.054942
step:    37240, time: 2.494, loss: 0.046784
step:    37260, time: 2.489, loss: 0.041942
step:    37280, time: 2.524, loss: 0.047152
step:    37300, time: 2.673, loss: 0.058667
step:    37320, time: 2.574, loss: 0.050865
step:    37340, time: 2.647, loss: 0.063527
step:    37360, time: 2.367, loss: 0.039780
step:    37380, time: 2.361, loss: 0.045612
step:    37400, time: 2.608, loss: 0.049933
step:    37420, time: 2.600, loss: 0.048343
step:    37440, time: 2.587, loss: 0.044793
step:    37460, time: 2.558, loss: 0.038376
step:    37480, time: 2.628, loss: 0.050354
step:    37500, time: 2.472, loss: 0.040572
step:    37520, time: 2.522, loss: 0.046956
step:    37540, time: 2.503, loss: 0.046330
step:    37560, time: 2.629, loss: 0.047566
step:    37580, time: 2.591, loss: 0.047444
step:    37600, time: 2.548, loss: 0.043177
step:    37620, time: 2.610, loss: 0.050018
step:    37640, time: 2.529, loss: 0.046366
step:    37660, time: 2.541, loss: 0.058155
step:    37680, time: 2.378, loss: 0.040273
step:    37700, time: 2.440, loss: 0.048555
step:    37720, time: 2.588, loss: 0.049249
step:    37740, time: 2.536, loss: 0.047959
step:    37760, time: 2.615, loss: 0.049691
step:    37780, time: 2.579, loss: 0.045127
step:    37800, time: 2.548, loss: 0.040697
step:    37820, time: 2.552, loss: 0.045013
step:    37840, time: 2.536, loss: 0.044581
step:    37860, time: 2.528, loss: 0.044795
step:    37880, time: 2.547, loss: 0.048280
step:    37900, time: 2.631, loss: 0.050392
step:    37920, time: 2.537, loss: 0.046292
step:    37940, time: 2.464, loss: 0.041903
step:    37960, time: 2.559, loss: 0.042175
step:    37980, time: 2.469, loss: 0.049070
step:    38000, time: 2.345, loss: 0.049575
step:    38020, time: 2.628, loss: 0.053235
step:    38040, time: 2.546, loss: 0.044630
step:    38060, time: 2.532, loss: 0.047388
step:    38080, time: 2.532, loss: 0.051742
step:    38100, time: 2.627, loss: 0.060030
step:    38120, time: 2.601, loss: 0.060836
step:    38140, time: 2.509, loss: 0.051676
step:    38160, time: 2.614, loss: 0.056872
step:    38180, time: 2.573, loss: 0.045303
step:    38200, time: 2.532, loss: 0.046231
step:    38220, time: 2.588, loss: 0.052235
step:    38240, time: 2.564, loss: 0.044304
step:    38260, time: 2.382, loss: 0.043963
step:    38280, time: 2.681, loss: 0.062137
step:    38300, time: 2.342, loss: 0.048160
step:    38320, time: 2.324, loss: 0.052071
step:    38340, time: 2.661, loss: 0.054523
step:    38360, time: 2.581, loss: 0.046538
step:    38380, time: 2.546, loss: 0.043177
step:    38400, time: 2.547, loss: 0.043010
step:    38420, time: 2.522, loss: 0.038953
step:    38440, time: 2.573, loss: 0.050823
step:    38460, time: 2.660, loss: 0.053454
step:    38480, time: 2.634, loss: 0.055918
step:    38500, time: 2.535, loss: 0.040447
step:    38520, time: 2.572, loss: 0.046553
step:    38540, time: 2.490, loss: 0.037883
step:    38560, time: 2.532, loss: 0.049619
step:    38580, time: 2.560, loss: 0.041793
step:    38600, time: 2.422, loss: 0.046634
step:    38620, time: 2.380, loss: 0.054826
step:    38640, time: 2.514, loss: 0.045893
step:    38660, time: 2.584, loss: 0.056836
step:    38680, time: 2.555, loss: 0.042683
step:    38700, time: 2.592, loss: 0.051100
step:    38720, time: 2.573, loss: 0.051155
step:    38740, time: 2.532, loss: 0.043453
step:    38760, time: 2.550, loss: 0.047716
step:    38780, time: 2.573, loss: 0.049582
step:    38800, time: 2.495, loss: 0.041839
step:    38820, time: 2.571, loss: 0.045761
step:    38840, time: 2.561, loss: 0.046781
step:    38860, time: 2.642, loss: 0.048751
step:    38880, time: 2.583, loss: 0.052413
step:    38900, time: 2.575, loss: 0.049459
step:    38920, time: 2.404, loss: 0.045345
step:    38940, time: 2.339, loss: 0.043593
step:    38960, time: 2.598, loss: 0.052008
step:    38980, time: 2.563, loss: 0.046587
step:    39000, time: 2.566, loss: 0.042929
step:    39020, time: 2.559, loss: 0.045607
step:    39040, time: 2.536, loss: 0.043912
step:    39060, time: 2.444, loss: 0.037801
step:    39080, time: 2.601, loss: 0.051607
step:    39100, time: 2.634, loss: 0.050935
step:    39120, time: 2.593, loss: 0.048778
step:    39140, time: 2.592, loss: 0.048784
step:    39160, time: 2.522, loss: 0.041820
step:    39180, time: 2.582, loss: 0.046429
step:    39200, time: 2.532, loss: 0.043778
step:    39220, time: 2.593, loss: 0.050319
step:    39240, time: 2.408, loss: 0.048945
step:    39260, time: 2.353, loss: 0.042113
step:    39280, time: 2.604, loss: 0.045831
step:    39300, time: 2.477, loss: 0.041692
step:    39320, time: 2.552, loss: 0.046528
step:    39340, time: 2.623, loss: 0.047237
step:    39360, time: 2.574, loss: 0.048854
step:    39380, time: 2.607, loss: 0.053863
step:    39400, time: 2.599, loss: 0.048316
step:    39420, time: 2.579, loss: 0.046836
step:    39440, time: 2.628, loss: 0.054913
step:    39460, time: 2.463, loss: 0.042913
step:    39480, time: 2.561, loss: 0.049126
step:    39500, time: 2.640, loss: 0.053190
step:    39520, time: 2.519, loss: 0.045484
step:    39540, time: 2.449, loss: 0.045448
step:    39560, time: 2.359, loss: 0.047248
step:    39580, time: 2.619, loss: 0.048489
step:    39600, time: 2.614, loss: 0.044381
step:    39620, time: 2.627, loss: 0.054137
step:    39640, time: 2.529, loss: 0.042235
step:    39660, time: 2.570, loss: 0.050059
step:    39680, time: 2.570, loss: 0.042955
step:    39700, time: 2.602, loss: 0.047614
step:    39720, time: 2.581, loss: 0.050092
step:    39740, time: 2.513, loss: 0.040035
step:    39760, time: 2.494, loss: 0.049046
step:    39780, time: 2.632, loss: 0.051500
step:    39800, time: 2.470, loss: 0.039205
step:    39820, time: 2.619, loss: 0.052472
step:    39840, time: 2.548, loss: 0.041336
step:    39860, time: 2.389, loss: 0.040676
step:    39880, time: 2.414, loss: 0.056655
step:    39900, time: 2.555, loss: 0.049750
step:    39920, time: 2.521, loss: 0.049062
step:    39940, time: 2.598, loss: 0.047819
step:    39960, time: 2.435, loss: 0.036059
step:    39980, time: 2.557, loss: 0.040655
step:    40000, time: 2.462, loss: 0.043713
step:    40020, time: 2.449, loss: 0.048651
step:    40040, time: 2.367, loss: 0.057201
step:    40060, time: 2.364, loss: 0.042501
step:    40080, time: 2.318, loss: 0.040803
step:    40100, time: 2.439, loss: 0.043380
step:    40120, time: 2.304, loss: 0.043892
step:    40140, time: 2.401, loss: 0.042681
step:    40160, time: 2.358, loss: 0.042778
step:    40180, time: 2.260, loss: 0.058366
step:    40200, time: 3.780, loss: 0.049587
step:    40220, time: 2.346, loss: 0.048160
step:    40240, time: 2.338, loss: 0.048643
step:    40260, time: 2.831, loss: 0.040812
step:    40280, time: 2.437, loss: 0.046728
step:    40300, time: 2.381, loss: 0.045230
step:    40320, time: 2.282, loss: 0.037161
step:    40340, time: 2.390, loss: 0.046283
step:    40360, time: 2.390, loss: 0.050623
step:    40380, time: 2.294, loss: 0.041995
step:    40400, time: 2.348, loss: 0.043910
step:    40420, time: 2.463, loss: 0.053603
step:    40440, time: 2.233, loss: 0.041001
step:    40460, time: 2.464, loss: 0.053965
step:    40480, time: 2.304, loss: 0.048291
step:    40500, time: 2.187, loss: 0.039291
step:    40520, time: 2.438, loss: 0.042809
step:    40540, time: 2.400, loss: 0.045054
step:    40560, time: 2.343, loss: 0.043523
step:    40580, time: 2.307, loss: 0.042498
step:    40600, time: 2.251, loss: 0.038399
step:    40620, time: 2.402, loss: 0.044420
step:    40640, time: 2.496, loss: 0.050745
step:    40660, time: 2.336, loss: 0.051664
step:    40680, time: 2.403, loss: 0.048536
step:    40700, time: 2.366, loss: 0.045397
step:    40720, time: 2.415, loss: 0.048595
step:    40740, time: 2.414, loss: 0.050947
step:    40760, time: 2.354, loss: 0.042710
step:    40780, time: 2.213, loss: 0.038005
step:    40800, time: 2.300, loss: 0.047677
step:    40820, time: 2.208, loss: 0.044233
step:    40840, time: 2.487, loss: 0.052211
step:    40860, time: 2.349, loss: 0.047058
step:    40880, time: 2.452, loss: 0.056194
step:    40900, time: 2.375, loss: 0.050126
step:    40920, time: 2.480, loss: 0.057772
step:    40940, time: 2.404, loss: 0.048261
step:    40960, time: 2.388, loss: 0.047783
step:    40980, time: 2.348, loss: 0.047202
step:    41000, time: 2.382, loss: 0.049090
step:    41020, time: 2.315, loss: 0.035267
step:    41040, time: 2.348, loss: 0.044892
step:    41060, time: 2.329, loss: 0.040015
step:    41080, time: 2.361, loss: 0.051294
step:    41100, time: 2.308, loss: 0.047644
step:    41120, time: 2.254, loss: 0.047187
step:    41140, time: 2.377, loss: 0.043039
step:    41160, time: 2.438, loss: 0.052678
step:    41180, time: 2.361, loss: 0.040578
step:    41200, time: 2.431, loss: 0.053391
step:    41220, time: 2.363, loss: 0.043912
step:    41240, time: 2.310, loss: 0.044130
step:    41260, time: 2.457, loss: 0.053818
step:    41280, time: 2.447, loss: 0.055475
step:    41300, time: 2.370, loss: 0.046131
step:    41320, time: 2.381, loss: 0.051506
step:    41340, time: 2.327, loss: 0.041245
step:    41360, time: 2.312, loss: 0.045589
step:    41380, time: 2.373, loss: 0.045581
step:    41400, time: 2.368, loss: 0.044285
step:    41420, time: 2.314, loss: 0.044427
step:    41440, time: 2.227, loss: 0.050604
step:    41460, time: 2.363, loss: 0.039890
step:    41480, time: 2.369, loss: 0.036454
step:    41500, time: 2.351, loss: 0.044779
step:    41520, time: 2.324, loss: 0.046860
step:    41540, time: 2.442, loss: 0.051531
step:    41560, time: 2.366, loss: 0.044708
step:    41580, time: 2.332, loss: 0.042155
step:    41600, time: 2.398, loss: 0.046610
step:    41620, time: 2.386, loss: 0.049028
step:    41640, time: 2.427, loss: 0.050744
step:    41660, time: 2.361, loss: 0.044645
step:    41680, time: 2.410, loss: 0.045793
step:    41700, time: 2.370, loss: 0.053616
step:    41720, time: 2.302, loss: 0.057480
step:    41740, time: 2.224, loss: 0.046732
step:    41760, time: 2.368, loss: 0.046643
step:    41780, time: 2.335, loss: 0.044659
step:    41800, time: 2.342, loss: 0.038470
step:    41820, time: 2.383, loss: 0.046898
step:    41840, time: 2.364, loss: 0.045750
step:    41860, time: 2.380, loss: 0.044399
step:    41880, time: 2.298, loss: 0.047287
step:    41900, time: 2.351, loss: 0.053388
step:    41920, time: 2.432, loss: 0.056353
step:    41940, time: 2.391, loss: 0.046387
step:    41960, time: 2.391, loss: 0.047963
step:    41980, time: 2.342, loss: 0.056765
step:    42000, time: 2.466, loss: 0.054405
step:    42020, time: 2.488, loss: 0.054729
step:    42040, time: 2.154, loss: 0.037813
step:    42060, time: 2.290, loss: 0.051722
step:    42080, time: 2.401, loss: 0.046159
step:    42100, time: 2.400, loss: 0.039418
step:    42120, time: 2.441, loss: 0.048886
step:    42140, time: 2.416, loss: 0.048088
step:    42160, time: 2.379, loss: 0.044527
step:    42180, time: 2.364, loss: 0.047805
step:    42200, time: 2.385, loss: 0.048629
step:    42220, time: 2.272, loss: 0.039709
step:    42240, time: 2.332, loss: 0.045537
step:    42260, time: 2.434, loss: 0.059458
step:    42280, time: 2.340, loss: 0.051231
step:    42300, time: 2.275, loss: 0.035102
step:    42320, time: 2.354, loss: 0.047003
step:    42340, time: 2.467, loss: 0.054455
step:    42360, time: 2.237, loss: 0.049348
step:    42380, time: 2.224, loss: 0.046500
step:    42400, time: 2.361, loss: 0.045636
step:    42420, time: 2.513, loss: 0.048499
step:    42440, time: 2.400, loss: 0.050935
step:    42460, time: 2.301, loss: 0.040348
step:    42480, time: 2.400, loss: 0.050168
step:    42500, time: 2.382, loss: 0.050599
step:    42520, time: 2.459, loss: 0.045573
step:    42540, time: 2.468, loss: 0.053613
step:    42560, time: 2.406, loss: 0.049108
step:    42580, time: 2.318, loss: 0.041207
step:    42600, time: 2.383, loss: 0.043921
step:    42620, time: 2.440, loss: 0.054923
step:    42640, time: 2.413, loss: 0.046564
step:    42660, time: 2.348, loss: 0.043103
step:    42680, time: 2.351, loss: 0.047725
step:    42700, time: 2.494, loss: 0.054417
step:    42720, time: 2.347, loss: 0.047896
step:    42740, time: 2.348, loss: 0.055216
step:    42760, time: 2.456, loss: 0.053758
step:    42780, time: 2.376, loss: 0.040598
step:    42800, time: 2.443, loss: 0.055742
step:    42820, time: 2.383, loss: 0.044553
step:    42840, time: 2.396, loss: 0.042031
step:    42860, time: 2.425, loss: 0.049216
step:    42880, time: 2.378, loss: 0.041527
step:    42900, time: 2.441, loss: 0.055588
step:    42920, time: 2.474, loss: 0.058612
step:    42940, time: 2.464, loss: 0.051578
step:    42960, time: 2.437, loss: 0.046912
step:    42980, time: 2.289, loss: 0.048382
step:    43000, time: 2.276, loss: 0.052720
step:    43020, time: 2.434, loss: 0.054518
step:    43040, time: 2.342, loss: 0.038029
step:    43060, time: 2.391, loss: 0.045814
step:    43080, time: 2.383, loss: 0.047906
step:    43100, time: 2.415, loss: 0.046653
step:    43120, time: 2.386, loss: 0.050718
step:    43140, time: 2.380, loss: 0.049398
step:    43160, time: 2.382, loss: 0.045311
step:    43180, time: 2.328, loss: 0.040382
step:    43200, time: 2.411, loss: 0.051720
step:    43220, time: 2.456, loss: 0.055658
step:    43240, time: 2.326, loss: 0.043131
step:    43260, time: 2.359, loss: 0.050745
step:    43280, time: 2.409, loss: 0.059490
step:    43300, time: 2.257, loss: 0.048412
step:    43320, time: 4.204, loss: 0.045334
step:    43340, time: 2.549, loss: 0.059648
step:    43360, time: 2.448, loss: 0.053201
step:    43380, time: 2.375, loss: 0.048032
step:    43400, time: 2.334, loss: 0.038561
step:    43420, time: 2.376, loss: 0.047137
step:    43440, time: 2.518, loss: 0.061846
step:    43460, time: 2.382, loss: 0.040132
step:    43480, time: 2.481, loss: 0.050856
step:    43500, time: 2.344, loss: 0.041853
step:    43520, time: 2.407, loss: 0.049087
step:    43540, time: 2.425, loss: 0.056124
step:    43560, time: 2.340, loss: 0.040664
step:    43580, time: 2.318, loss: 0.044576
step:    43600, time: 2.350, loss: 0.045295
step:    43620, time: 2.320, loss: 0.047689
step:    43640, time: 2.425, loss: 0.051733
step:    43660, time: 2.378, loss: 0.048772
step:    43680, time: 2.402, loss: 0.045279
step:    43700, time: 2.414, loss: 0.044105
step:    43720, time: 2.503, loss: 0.057662
step:    43740, time: 2.434, loss: 0.047787
step:    43760, time: 2.341, loss: 0.047272
step:    43780, time: 2.326, loss: 0.041710
step:    43800, time: 2.336, loss: 0.040744
step:    43820, time: 2.422, loss: 0.047769
step:    43840, time: 2.298, loss: 0.038345
step:    43860, time: 2.355, loss: 0.045835
step:    43880, time: 2.313, loss: 0.042059
step:    43900, time: 2.386, loss: 0.047582
step:    43920, time: 2.248, loss: 0.054374
step:    43940, time: 2.327, loss: 0.043827
step:    43960, time: 2.460, loss: 0.050036
step:    43980, time: 2.372, loss: 0.043431
step:    44000, time: 2.372, loss: 0.046856
step:    44020, time: 2.335, loss: 0.038899
step:    44040, time: 2.369, loss: 0.041905
step:    44060, time: 2.398, loss: 0.047075
step:    44080, time: 2.429, loss: 0.052945
step:    44100, time: 2.320, loss: 0.040898
step:    44120, time: 2.328, loss: 0.043711
step:    44140, time: 2.301, loss: 0.042690
step:    44160, time: 2.394, loss: 0.045640
step:    44180, time: 2.395, loss: 0.045450
step:    44200, time: 2.329, loss: 0.038393
step:    44220, time: 2.310, loss: 0.047744
step:    44240, time: 2.303, loss: 0.057023
step:    44260, time: 2.461, loss: 0.056408
step:    44280, time: 2.316, loss: 0.041359
step:    44300, time: 2.363, loss: 0.042119
step:    44320, time: 2.383, loss: 0.047707
step:    44340, time: 2.337, loss: 0.043954
step:    44360, time: 2.326, loss: 0.043991
step:    44380, time: 2.415, loss: 0.043831
step:    44400, time: 2.379, loss: 0.043599
step:    44420, time: 2.496, loss: 0.058895
step:    44440, time: 2.444, loss: 0.048432
step:    44460, time: 2.512, loss: 0.062704
step:    44480, time: 2.346, loss: 0.037593
step:    44500, time: 2.381, loss: 0.044612
step:    44520, time: 2.403, loss: 0.048928
step:    44540, time: 2.390, loss: 0.053192
step:    44560, time: 2.346, loss: 0.047448
step:    44580, time: 2.372, loss: 0.047684
step:    44600, time: 2.375, loss: 0.046871
step:    44620, time: 2.387, loss: 0.051277
step:    44640, time: 2.373, loss: 0.044188
step:    44660, time: 2.308, loss: 0.043032
step:    44680, time: 2.429, loss: 0.061676
step:    44700, time: 2.316, loss: 0.044793
step:    44720, time: 2.388, loss: 0.040931
step:    44740, time: 2.373, loss: 0.045247
step:    44760, time: 2.400, loss: 0.045419
step:    44780, time: 2.374, loss: 0.045059
step:    44800, time: 2.405, loss: 0.043714
step:    44820, time: 2.328, loss: 0.041366
step:    44840, time: 2.371, loss: 0.056188
step:    44860, time: 2.191, loss: 0.038403
step:    44880, time: 2.458, loss: 0.049831
step:    44900, time: 2.316, loss: 0.037527
step:    44920, time: 2.345, loss: 0.042478
step:    44940, time: 2.359, loss: 0.050587
step:    44960, time: 2.393, loss: 0.046815
step:    44980, time: 2.502, loss: 0.054887
step:    45000, time: 2.383, loss: 0.043315
step:    45020, time: 2.335, loss: 0.042587
step:    45040, time: 2.364, loss: 0.046469
step:    45060, time: 2.369, loss: 0.050920
step:    45080, time: 2.420, loss: 0.049461
step:    45100, time: 2.351, loss: 0.049897
step:    45120, time: 2.415, loss: 0.051983
step:    45140, time: 2.412, loss: 0.047796
step:    45160, time: 2.391, loss: 0.050519
step:    45180, time: 2.292, loss: 0.039902
step:    45200, time: 2.340, loss: 0.039386
step:    45220, time: 2.317, loss: 0.044811
step:    45240, time: 2.565, loss: 0.045693
step:    45260, time: 2.354, loss: 0.043569
step:    45280, time: 2.348, loss: 0.047781
step:    45300, time: 2.375, loss: 0.049416
step:    45320, time: 2.419, loss: 0.052692
step:    45340, time: 2.388, loss: 0.045756
step:    45360, time: 2.363, loss: 0.046214
step:    45380, time: 2.387, loss: 0.044743
step:    45400, time: 2.308, loss: 0.044370
step:    45420, time: 2.387, loss: 0.047313
step:    45440, time: 2.460, loss: 0.056620
step:    45460, time: 2.353, loss: 0.043325
step:    45480, time: 2.249, loss: 0.050273
step:    45500, time: 2.246, loss: 0.048655
step:    45520, time: 2.416, loss: 0.052651
step:    45540, time: 2.310, loss: 0.043628
step:    45560, time: 2.414, loss: 0.051494
step:    45580, time: 2.430, loss: 0.047607
step:    45600, time: 2.432, loss: 0.044640
step:    45620, time: 2.246, loss: 0.037867
step:    45640, time: 2.375, loss: 0.046163
step:    45660, time: 2.345, loss: 0.037771
step:    45680, time: 2.409, loss: 0.051075
step:    45700, time: 2.374, loss: 0.047129
step:    45720, time: 2.287, loss: 0.037772
step:    45740, time: 2.362, loss: 0.038101
step:    45760, time: 2.354, loss: 0.051515
step:    45780, time: 2.319, loss: 0.053477
step:    45800, time: 2.223, loss: 0.040715
step:    45820, time: 2.403, loss: 0.044329
step:    45840, time: 3.870, loss: 0.043351
step:    45860, time: 2.472, loss: 0.052520
step:    45880, time: 2.352, loss: 0.041801
step:    45900, time: 2.384, loss: 0.045511
step:    45920, time: 2.465, loss: 0.046965
step:    45940, time: 2.452, loss: 0.047349
step:    45960, time: 2.381, loss: 0.047018
step:    45980, time: 2.356, loss: 0.044561
step:    46000, time: 2.371, loss: 0.044727
step:    46020, time: 2.400, loss: 0.048150
step:    46040, time: 2.408, loss: 0.051727
step:    46060, time: 2.357, loss: 0.042340
step:    46080, time: 2.382, loss: 0.048796
step:    46100, time: 2.343, loss: 0.052360
step:    46120, time: 2.235, loss: 0.045648
step:    46140, time: 2.389, loss: 0.049603
step:    46160, time: 2.311, loss: 0.041869
step:    46180, time: 2.404, loss: 0.051089
step:    46200, time: 2.466, loss: 0.049574
step:    46220, time: 2.350, loss: 0.041947
step:    46240, time: 2.419, loss: 0.046226
step:    46260, time: 2.326, loss: 0.042452
step:    46280, time: 2.359, loss: 0.046863
step:    46300, time: 2.418, loss: 0.044899
step:    46320, time: 2.312, loss: 0.037756
step:    46340, time: 2.329, loss: 0.042699
step:    46360, time: 2.430, loss: 0.048036
step:    46380, time: 2.431, loss: 0.061171
step:    46400, time: 2.291, loss: 0.042723
step:    46420, time: 2.281, loss: 0.049273
step:    46440, time: 2.710, loss: 0.053289
step:    46460, time: 2.345, loss: 0.039397
step:    46480, time: 2.420, loss: 0.045740
step:    46500, time: 2.356, loss: 0.041156
step:    46520, time: 2.378, loss: 0.050488
step:    46540, time: 2.332, loss: 0.042808
step:    46560, time: 2.206, loss: 0.038061
step:    46580, time: 2.379, loss: 0.046818
step:    46600, time: 2.306, loss: 0.050043
step:    46620, time: 2.396, loss: 0.052463
step:    46640, time: 2.405, loss: 0.045753
step:    46660, time: 2.397, loss: 0.053698
step:    46680, time: 2.378, loss: 0.051561
step:    46700, time: 2.312, loss: 0.051691
step:    46720, time: 2.268, loss: 0.043936
step:    46740, time: 2.261, loss: 0.054442
step:    46760, time: 2.453, loss: 0.048418
step:    46780, time: 2.379, loss: 0.047631
step:    46800, time: 2.475, loss: 0.055588
step:    46820, time: 2.427, loss: 0.040410
step:    46840, time: 2.378, loss: 0.047115
step:    46860, time: 2.427, loss: 0.045684
step:    46880, time: 2.356, loss: 0.049399
step:    46900, time: 2.403, loss: 0.045758
step:    46920, time: 2.335, loss: 0.042326
step:    46940, time: 2.448, loss: 0.056168
step:    46960, time: 2.344, loss: 0.039945
step:    46980, time: 2.346, loss: 0.050530
step:    47000, time: 2.382, loss: 0.055665
step:    47020, time: 2.367, loss: 0.044552
step:    47040, time: 2.316, loss: 0.042666
step:    47060, time: 2.194, loss: 0.042930
step:    47080, time: 2.448, loss: 0.047885
step:    47100, time: 2.405, loss: 0.043674
step:    47120, time: 2.410, loss: 0.053297
step:    47140, time: 2.340, loss: 0.043862
step:    47160, time: 2.326, loss: 0.044756
step:    47180, time: 2.419, loss: 0.042177
step:    47200, time: 2.430, loss: 0.045049
step:    47220, time: 2.378, loss: 0.056509
step:    47240, time: 2.343, loss: 0.046132
step:    47260, time: 2.363, loss: 0.047613
step:    47280, time: 2.365, loss: 0.040642
step:    47300, time: 2.519, loss: 0.060249
step:    47320, time: 2.458, loss: 0.044924
step:    47340, time: 2.227, loss: 0.040169
step:    47360, time: 2.239, loss: 0.041785
step:    47380, time: 2.396, loss: 0.040029
step:    47400, time: 2.414, loss: 0.048869
step:    47420, time: 2.347, loss: 0.041807
step:    47440, time: 2.469, loss: 0.053035
step:    47460, time: 2.360, loss: 0.039426
step:    47480, time: 2.410, loss: 0.047497
step:    47500, time: 2.387, loss: 0.043357
step:    47520, time: 2.329, loss: 0.044082
step:    47540, time: 2.383, loss: 0.046630
step:    47560, time: 2.416, loss: 0.048847
step:    47580, time: 2.366, loss: 0.043097
step:    47600, time: 2.389, loss: 0.042182
step:    47620, time: 2.417, loss: 0.052306
step:    47640, time: 2.440, loss: 0.054785
step:    47660, time: 2.207, loss: 0.044882
step:    47680, time: 2.167, loss: 0.039472
step:    47700, time: 2.383, loss: 0.041839
step:    47720, time: 2.459, loss: 0.051793
step:    47740, time: 2.414, loss: 0.060369
step:    47760, time: 2.374, loss: 0.043985
step:    47780, time: 2.508, loss: 0.051300
step:    47800, time: 2.407, loss: 0.044151
step:    47820, time: 2.373, loss: 0.047936
step:    47840, time: 2.334, loss: 0.043972
step:    47860, time: 2.401, loss: 0.048784
step:    47880, time: 2.331, loss: 0.050110
step:    47900, time: 2.336, loss: 0.038153
step:    47920, time: 2.418, loss: 0.050369
step:    47940, time: 2.339, loss: 0.042080
step:    47960, time: 2.330, loss: 0.054885
step:    47980, time: 2.269, loss: 0.052454
step:    48000, time: 2.554, loss: 0.041684
step:    48020, time: 2.398, loss: 0.045658
step:    48040, time: 2.356, loss: 0.045158
step:    48060, time: 2.279, loss: 0.041132
step:    48080, time: 2.320, loss: 0.038668
step:    48100, time: 2.370, loss: 0.052026
step:    48120, time: 2.365, loss: 0.043919
step:    48140, time: 2.456, loss: 0.042456
step:    48160, time: 2.380, loss: 0.048495
step:    48180, time: 2.398, loss: 0.046875
step:    48200, time: 2.305, loss: 0.039414
step:    48220, time: 2.307, loss: 0.042051
step:    48240, time: 2.390, loss: 0.046336
step:    48260, time: 2.473, loss: 0.048271
step:    48280, time: 2.344, loss: 0.062023
step:    48300, time: 2.241, loss: 0.043208
step:    48320, time: 2.456, loss: 0.051629
step:    48340, time: 2.390, loss: 0.045798
step:    48360, time: 2.427, loss: 0.050296
step:    48380, time: 2.314, loss: 0.045683
step:    48400, time: 2.299, loss: 0.040877
step:    48420, time: 2.377, loss: 0.043111
step:    48440, time: 2.306, loss: 0.043375
step:    48460, time: 2.291, loss: 0.038181
step:    48480, time: 2.292, loss: 0.034312
step:    48500, time: 2.473, loss: 0.054239
step:    48520, time: 2.357, loss: 0.039801
step:    48540, time: 2.386, loss: 0.039282
step:    48560, time: 2.362, loss: 0.045962
step:    48580, time: 2.382, loss: 0.055938
step:    48600, time: 2.321, loss: 0.050937
step:    48620, time: 2.309, loss: 0.049495
step:    48640, time: 2.482, loss: 0.048392
step:    48660, time: 2.568, loss: 0.057251
step:    48680, time: 2.296, loss: 0.044371
step:    48700, time: 2.321, loss: 0.043647
step:    48720, time: 2.345, loss: 0.042941
step:    48740, time: 2.311, loss: 0.047284
step:    48760, time: 2.354, loss: 0.045825
step:    48780, time: 2.353, loss: 0.045859
step:    48800, time: 2.325, loss: 0.036020
step:    48820, time: 2.443, loss: 0.051015
step:    48840, time: 2.369, loss: 0.039221
step:    48860, time: 2.463, loss: 0.051915
step:    48880, time: 2.358, loss: 0.041546
step:    48900, time: 2.340, loss: 0.049892
step:    48920, time: 2.187, loss: 0.044810
step:    48940, time: 2.419, loss: 0.050238
step:    48960, time: 2.288, loss: 0.040132
step:    48980, time: 2.332, loss: 0.047842
step:    49000, time: 2.311, loss: 0.044498
step:    49020, time: 2.481, loss: 0.054255
step:    49040, time: 2.465, loss: 0.046491
step:    49060, time: 2.463, loss: 0.044330
step:    49080, time: 2.399, loss: 0.047330
step:    49100, time: 2.432, loss: 0.050851
step:    49120, time: 2.327, loss: 0.045945
step:    49140, time: 2.389, loss: 0.043442
step:    49160, time: 2.459, loss: 0.055257
step:    49180, time: 2.384, loss: 0.053868
step:    49200, time: 2.321, loss: 0.047323
step:    49220, time: 2.282, loss: 0.045111
step:    49240, time: 2.154, loss: 0.037573
step:    49260, time: 2.368, loss: 0.041787
step:    49280, time: 2.419, loss: 0.056335
step:    49300, time: 2.410, loss: 0.057362
step:    49320, time: 2.347, loss: 0.043572
step:    49340, time: 2.419, loss: 0.046625
step:    49360, time: 2.432, loss: 0.055089
step:    49380, time: 2.289, loss: 0.039816
step:    49400, time: 2.328, loss: 0.041453
step:    49420, time: 2.375, loss: 0.047393
step:    49440, time: 2.451, loss: 0.050225
step:    49460, time: 2.337, loss: 0.046032
step:    49480, time: 2.260, loss: 0.043523
step:    49500, time: 2.440, loss: 0.044397
step:    49520, time: 2.337, loss: 0.050311
step:    49540, time: 2.263, loss: 0.048143
step:    49560, time: 2.783, loss: 0.050796
step:    49580, time: 2.442, loss: 0.043545
step:    49600, time: 2.420, loss: 0.046789
step:    49620, time: 2.310, loss: 0.045261
step:    49640, time: 2.312, loss: 0.043958
step:    49660, time: 2.470, loss: 0.047631
step:    49680, time: 2.342, loss: 0.041883
step:    49700, time: 2.371, loss: 0.044838
step:    49720, time: 2.415, loss: 0.046617
step:    49740, time: 2.341, loss: 0.041416
step:    49760, time: 2.315, loss: 0.040189
step:    49780, time: 2.382, loss: 0.046031
step:    49800, time: 2.389, loss: 0.052357
step:    49820, time: 2.382, loss: 0.043894
step:    49840, time: 2.202, loss: 0.041056
step:    49860, time: 2.224, loss: 0.043565
step:    49880, time: 2.435, loss: 0.052028
step:    49900, time: 2.400, loss: 0.049363
step:    49920, time: 2.263, loss: 0.047515
step:    49940, time: 2.313, loss: 0.043693
step:    49960, time: 2.408, loss: 0.048902
step:    49980, time: 2.403, loss: 0.051871
step:    50000, time: 2.400, loss: 0.048604
step:    50020, time: 2.307, loss: 0.046672
step:    50040, time: 2.318, loss: 0.040657
step:    50060, time: 2.339, loss: 0.040785
step:    50080, time: 2.394, loss: 0.051759
step:    50100, time: 2.363, loss: 0.047386
step:    50120, time: 2.314, loss: 0.043301
step:    50140, time: 2.266, loss: 0.036829
step:    50160, time: 2.265, loss: 0.048122
step:    50180, time: 2.266, loss: 0.040910
step:    50200, time: 2.483, loss: 0.048350
step:    50220, time: 2.346, loss: 0.041243
step:    50240, time: 2.404, loss: 0.042072
step:    50260, time: 2.432, loss: 0.046146
step:    50280, time: 2.472, loss: 0.049109
step:    50300, time: 2.371, loss: 0.044330
step:    50320, time: 2.425, loss: 0.047426
step:    50340, time: 2.371, loss: 0.041217
step:    50360, time: 2.406, loss: 0.046986
step:    50380, time: 2.323, loss: 0.036739
step:    50400, time: 2.364, loss: 0.048775
step:    50420, time: 2.440, loss: 0.049708
step:    50440, time: 2.375, loss: 0.045280
step:    50460, time: 2.351, loss: 0.049140
step:    50480, time: 2.172, loss: 0.038656
step:    50500, time: 2.343, loss: 0.044283
step:    50520, time: 2.296, loss: 0.044161
step:    50540, time: 2.409, loss: 0.044584
step:    50560, time: 2.429, loss: 0.040997
step:    50580, time: 2.415, loss: 0.044855
step:    50600, time: 2.390, loss: 0.043951
step:    50620, time: 2.306, loss: 0.045171
step:    50640, time: 2.407, loss: 0.048890
step:    50660, time: 2.332, loss: 0.035860
step:    50680, time: 2.424, loss: 0.045749
step:    50700, time: 2.371, loss: 0.044050
step:    50720, time: 2.302, loss: 0.051570
step:    50740, time: 2.350, loss: 0.045072
step:    50760, time: 2.349, loss: 0.047387
step:    50780, time: 2.218, loss: 0.040727
step:    50800, time: 2.347, loss: 0.049485
step:    50820, time: 2.390, loss: 0.038971
step:    50840, time: 2.394, loss: 0.042243
step:    50860, time: 2.376, loss: 0.045619
step:    50880, time: 2.354, loss: 0.042290
step:    50900, time: 2.385, loss: 0.053038
step:    50920, time: 2.409, loss: 0.052257
step:    50940, time: 2.426, loss: 0.046781
step:    50960, time: 2.396, loss: 0.045403
step:    50980, time: 2.374, loss: 0.053330
step:    51000, time: 2.338, loss: 0.041085
step:    51020, time: 2.394, loss: 0.046798
step:    51040, time: 2.340, loss: 0.049011
step:    51060, time: 2.365, loss: 0.049423
step:    51080, time: 2.294, loss: 0.051078
step:    51100, time: 2.268, loss: 0.049941
step:    51120, time: 3.883, loss: 0.048191
step:    51140, time: 2.443, loss: 0.043532
step:    51160, time: 2.447, loss: 0.044793
step:    51180, time: 2.332, loss: 0.049717
step:    51200, time: 2.450, loss: 0.057900
step:    51220, time: 2.395, loss: 0.044102
step:    51240, time: 2.423, loss: 0.046318
step:    51260, time: 2.317, loss: 0.041378
step:    51280, time: 2.324, loss: 0.045308
step:    51300, time: 2.356, loss: 0.038003
step:    51320, time: 2.348, loss: 0.048331
step:    51340, time: 2.352, loss: 0.042924
step:    51360, time: 2.343, loss: 0.050176
step:    51380, time: 2.306, loss: 0.045874
step:    51400, time: 2.282, loss: 0.042823
step:    51420, time: 2.210, loss: 0.045547
step:    51440, time: 2.282, loss: 0.035640
step:    51460, time: 2.357, loss: 0.037144
step:    51480, time: 2.403, loss: 0.043196
step:    51500, time: 2.367, loss: 0.044582
step:    51520, time: 2.404, loss: 0.043663
step:    51540, time: 2.361, loss: 0.045860
step:    51560, time: 2.426, loss: 0.057308
step:    51580, time: 2.400, loss: 0.051846
step:    51600, time: 2.315, loss: 0.041362
step:    51620, time: 2.352, loss: 0.040712
step:    51640, time: 2.451, loss: 0.066169
step:    51660, time: 2.325, loss: 0.042051
step:    51680, time: 2.329, loss: 0.039313
step:    51700, time: 2.398, loss: 0.049712
step:    51720, time: 2.295, loss: 0.051204
step:    51740, time: 2.220, loss: 0.042623
step:    51760, time: 2.432, loss: 0.044108
step:    51780, time: 2.316, loss: 0.039965
step:    51800, time: 2.397, loss: 0.044759
step:    51820, time: 2.391, loss: 0.048753
step:    51840, time: 2.291, loss: 0.040772
step:    51860, time: 2.318, loss: 0.039058
step:    51880, time: 2.387, loss: 0.050513
step:    51900, time: 2.346, loss: 0.040011
step:    51920, time: 2.487, loss: 0.048380
step:    51940, time: 2.463, loss: 0.049420
step:    51960, time: 2.355, loss: 0.044000
step:    51980, time: 2.372, loss: 0.044676
step:    52000, time: 2.385, loss: 0.052138
step:    52020, time: 2.272, loss: 0.045869
step:    52040, time: 2.291, loss: 0.055328
step:    52060, time: 2.837, loss: 0.037988
step:    52080, time: 2.434, loss: 0.054463
step:    52100, time: 2.376, loss: 0.043803
step:    52120, time: 2.325, loss: 0.047563
step:    52140, time: 2.368, loss: 0.046609
step:    52160, time: 2.318, loss: 0.037332
step:    52180, time: 2.415, loss: 0.051291
step:    52200, time: 2.339, loss: 0.045464
step:    52220, time: 2.400, loss: 0.047926
step:    52240, time: 2.394, loss: 0.045461
step:    52260, time: 2.415, loss: 0.044359
step:    52280, time: 2.509, loss: 0.060398
step:    52300, time: 2.394, loss: 0.052799
step:    52320, time: 2.437, loss: 0.057153
step:    52340, time: 2.382, loss: 0.047687
step:    52360, time: 2.249, loss: 0.041532
step:    52380, time: 2.433, loss: 0.047551
step:    52400, time: 2.416, loss: 0.043859
step:    52420, time: 2.439, loss: 0.057344
step:    52440, time: 2.388, loss: 0.036142
step:    52460, time: 2.319, loss: 0.045756
step:    52480, time: 2.425, loss: 0.061877
step:    52500, time: 2.380, loss: 0.041039
step:    52520, time: 2.328, loss: 0.040953
step:    52540, time: 2.265, loss: 0.039655
step:    52560, time: 2.454, loss: 0.049869
step:    52580, time: 2.343, loss: 0.040221
step:    52600, time: 2.414, loss: 0.052577
step:    52620, time: 2.381, loss: 0.047986
step:    52640, time: 2.319, loss: 0.041848
step:    52660, time: 2.281, loss: 0.047301
step:    52680, time: 3.253, loss: 0.046183
step:    52700, time: 2.419, loss: 0.058264
step:    52720, time: 2.341, loss: 0.047314
step:    52740, time: 2.290, loss: 0.042611
step:    52760, time: 2.501, loss: 0.055816
step:    52780, time: 2.376, loss: 0.046703
step:    52800, time: 2.342, loss: 0.043584
step:    52820, time: 2.379, loss: 0.046908
step:    52840, time: 2.352, loss: 0.044981
step:    52860, time: 2.374, loss: 0.043649
step:    52880, time: 2.355, loss: 0.038891
step:    52900, time: 2.415, loss: 0.054121
step:    52920, time: 2.359, loss: 0.042502
step:    52940, time: 2.304, loss: 0.037559
step:    52960, time: 2.412, loss: 0.054154
step:    52980, time: 2.190, loss: 0.041450
step:    53000, time: 2.365, loss: 0.045033
step:    53020, time: 2.373, loss: 0.045057
step:    53040, time: 2.423, loss: 0.049587
step:    53060, time: 2.396, loss: 0.051497
step:    53080, time: 2.417, loss: 0.046196
step:    53100, time: 2.403, loss: 0.052053
step:    53120, time: 2.435, loss: 0.047955
step:    53140, time: 2.330, loss: 0.039879
step:    53160, time: 2.305, loss: 0.040689
step:    53180, time: 2.412, loss: 0.056151
step:    53200, time: 2.403, loss: 0.050104
step:    53220, time: 2.356, loss: 0.043938
step:    53240, time: 2.340, loss: 0.040067
step:    53260, time: 2.359, loss: 0.048914
step:    53280, time: 2.196, loss: 0.040224
step:    53300, time: 2.231, loss: 0.056344
step:    53320, time: 2.433, loss: 0.050652
step:    53340, time: 2.358, loss: 0.048941
step:    53360, time: 2.421, loss: 0.047003
step:    53380, time: 2.445, loss: 0.046836
step:    53400, time: 2.421, loss: 0.048680
step:    53420, time: 2.440, loss: 0.053924
step:    53440, time: 2.390, loss: 0.045370
step:    53460, time: 2.421, loss: 0.045627
step:    53480, time: 2.357, loss: 0.052924
step:    53500, time: 2.406, loss: 0.044979
step:    53520, time: 2.426, loss: 0.048253
step:    53540, time: 2.296, loss: 0.043744
step:    53560, time: 2.428, loss: 0.047580
step:    53580, time: 2.253, loss: 0.057654
step:    53600, time: 2.230, loss: 0.045219
step:    53620, time: 2.394, loss: 0.041454
step:    53640, time: 2.403, loss: 0.052754
step:    53660, time: 2.435, loss: 0.048316
step:    53680, time: 2.326, loss: 0.045775
step:    53700, time: 2.396, loss: 0.049897
step:    53720, time: 2.335, loss: 0.051193
step:    53740, time: 2.331, loss: 0.042665
step:    53760, time: 2.338, loss: 0.044380
step:    53780, time: 2.393, loss: 0.043177
step:    53800, time: 2.396, loss: 0.043533
step:    53820, time: 2.353, loss: 0.038261
step:    53840, time: 2.286, loss: 0.040306
step:    53860, time: 2.316, loss: 0.038237
step:    53880, time: 2.417, loss: 0.049119
step:    53900, time: 2.250, loss: 0.047236
step:    53920, time: 2.301, loss: 0.054498
step:    53940, time: 2.500, loss: 0.057592
step:    53960, time: 2.339, loss: 0.047266
step:    53980, time: 2.329, loss: 0.047262
step:    54000, time: 2.392, loss: 0.051973
step:    54020, time: 2.353, loss: 0.048223
step:    54040, time: 2.420, loss: 0.052613
step:    54060, time: 2.295, loss: 0.035742
step:    54080, time: 2.336, loss: 0.041264
step:    54100, time: 2.346, loss: 0.048629
step:    54120, time: 2.330, loss: 0.052083
step:    54140, time: 2.357, loss: 0.044664
step:    54160, time: 2.334, loss: 0.051822
step:    54180, time: 2.310, loss: 0.034573
step:    54200, time: 2.424, loss: 0.047082
step:    54220, time: 2.270, loss: 0.046552
step:    54240, time: 2.437, loss: 0.052106
step:    54260, time: 2.341, loss: 0.045992
step:    54280, time: 2.384, loss: 0.044591
step:    54300, time: 2.367, loss: 0.046927
step:    54320, time: 2.367, loss: 0.052171
step:    54340, time: 2.395, loss: 0.050228
step:    54360, time: 2.429, loss: 0.054001
step:    54380, time: 2.326, loss: 0.042763
step:    54400, time: 2.348, loss: 0.040669
step:    54420, time: 2.502, loss: 0.054815
step:    54440, time: 2.348, loss: 0.047767
step:    54460, time: 2.493, loss: 0.056370
step:    54480, time: 2.423, loss: 0.048363
step:    54500, time: 2.347, loss: 0.045758
step:    54520, time: 2.353, loss: 0.053895
step:    54540, time: 2.161, loss: 0.038846
step:    54560, time: 2.397, loss: 0.052609
step:    54580, time: 2.303, loss: 0.043010
step:    54600, time: 2.433, loss: 0.050529
step:    54620, time: 2.344, loss: 0.052878
step:    54640, time: 2.408, loss: 0.048272
step:    54660, time: 2.516, loss: 0.063180
step:    54680, time: 2.393, loss: 0.046834
step:    54700, time: 2.342, loss: 0.050042
step:    54720, time: 2.344, loss: 0.048591
step:    54740, time: 2.363, loss: 0.045193
step:    54760, time: 2.329, loss: 0.041661
step:    54780, time: 2.334, loss: 0.042231
step:    54800, time: 2.380, loss: 0.043120
step:    54820, time: 2.376, loss: 0.042881
step:    54840, time: 2.331, loss: 0.045269
step:    54860, time: 2.163, loss: 0.039609
step:    54880, time: 2.517, loss: 0.052292
step:    54900, time: 2.318, loss: 0.036578
step:    54920, time: 2.312, loss: 0.039251
step:    54940, time: 2.286, loss: 0.042511
step:    54960, time: 2.452, loss: 0.058187
step:    54980, time: 2.395, loss: 0.042257
step:    55000, time: 2.417, loss: 0.051898
step:    55020, time: 2.363, loss: 0.040087
step:    55040, time: 2.452, loss: 0.049227
step:    55060, time: 2.294, loss: 0.034940
step:    55080, time: 2.348, loss: 0.039821
step:    55100, time: 2.426, loss: 0.047151
step:    55120, time: 2.406, loss: 0.043207
step:    55140, time: 2.268, loss: 0.039008
step:    55160, time: 2.275, loss: 0.048514
step:    55180, time: 2.358, loss: 0.045351
step:    55200, time: 2.377, loss: 0.046534
step:    55220, time: 2.335, loss: 0.045843
step:    55240, time: 2.357, loss: 0.040965
step:    55260, time: 2.350, loss: 0.048437
step:    55280, time: 2.319, loss: 0.039892
step:    55300, time: 2.457, loss: 0.053651
step:    55320, time: 2.362, loss: 0.045428
step:    55340, time: 2.358, loss: 0.045554
step:    55360, time: 2.413, loss: 0.049867
step:    55380, time: 2.327, loss: 0.038958
step:    55400, time: 2.339, loss: 0.039193
step:    55420, time: 2.300, loss: 0.040957
step:    55440, time: 2.299, loss: 0.040557
step:    55460, time: 2.265, loss: 0.051593
step:    55480, time: 2.232, loss: 0.044544
step:    55500, time: 2.424, loss: 0.044234
step:    55520, time: 2.388, loss: 0.051307
step:    55540, time: 2.407, loss: 0.054753
step:    55560, time: 2.486, loss: 0.049190
step:    55580, time: 2.426, loss: 0.047323
step:    55600, time: 2.344, loss: 0.043059
step:    55620, time: 2.285, loss: 0.039175
step:    55640, time: 2.356, loss: 0.046548
step:    55660, time: 2.428, loss: 0.049150
step:    55680, time: 2.386, loss: 0.047821
step:    55700, time: 2.379, loss: 0.050921
step:    55720, time: 2.395, loss: 0.047763
step:    55740, time: 2.344, loss: 0.041563
step:    55760, time: 2.289, loss: 0.044712
step:    55780, time: 2.331, loss: 0.056225
step:    55800, time: 2.404, loss: 0.044003
step:    55820, time: 2.453, loss: 0.056788
step:    55840, time: 2.410, loss: 0.047830
step:    55860, time: 2.477, loss: 0.051229
step:    55880, time: 2.324, loss: 0.046107
step:    55900, time: 2.383, loss: 0.056808
step:    55920, time: 2.431, loss: 0.049546
step:    55940, time: 2.405, loss: 0.046385
step:    55960, time: 2.324, loss: 0.046283
step:    55980, time: 2.316, loss: 0.039278
step:    56000, time: 2.417, loss: 0.045111
step:    56020, time: 2.391, loss: 0.047429
step:    56040, time: 2.455, loss: 0.042364
step:    56060, time: 2.457, loss: 0.057468
step:    56080, time: 2.365, loss: 0.039207
step:    56100, time: 2.318, loss: 0.050384
step:    56120, time: 2.414, loss: 0.043284
step:    56140, time: 2.366, loss: 0.045622
step:    56160, time: 2.357, loss: 0.046501
step:    56180, time: 2.423, loss: 0.051529
step:    56200, time: 2.395, loss: 0.043766
step:    56220, time: 2.373, loss: 0.050236
step:    56240, time: 2.276, loss: 0.041181
step:    56260, time: 2.379, loss: 0.047035
step:    56280, time: 2.327, loss: 0.034452
step:    56300, time: 2.373, loss: 0.043416
step:    56320, time: 2.356, loss: 0.048528
step:    56340, time: 2.529, loss: 0.059215
step:    56360, time: 2.349, loss: 0.047684
step:    56380, time: 2.362, loss: 0.039214
step:    56400, time: 2.365, loss: 0.056196
step:    56420, time: 2.246, loss: 0.041822
step:    56440, time: 2.436, loss: 0.048777
step:    56460, time: 2.305, loss: 0.038432
step:    56480, time: 2.397, loss: 0.044679
step:    56500, time: 2.355, loss: 0.050794
step:    56520, time: 2.385, loss: 0.044481
step:    56540, time: 2.332, loss: 0.039790
step:    56560, time: 2.348, loss: 0.043776
step:    56580, time: 2.413, loss: 0.044063
step:    56600, time: 2.383, loss: 0.044018
step:    56620, time: 2.493, loss: 0.051774
step:    56640, time: 2.358, loss: 0.046936
step:    56660, time: 2.398, loss: 0.048729
step:    56680, time: 2.382, loss: 0.048467
step:    56700, time: 2.155, loss: 0.043116
step:    56720, time: 2.197, loss: 0.051608
step:    56740, time: 2.487, loss: 0.048609
step:    56760, time: 2.424, loss: 0.045148
step:    56780, time: 2.335, loss: 0.039827
step:    56800, time: 2.429, loss: 0.052333
step:    56820, time: 2.374, loss: 0.051298
step:    56840, time: 2.260, loss: 0.036633
step:    56860, time: 2.366, loss: 0.042141
step:    56880, time: 2.408, loss: 0.046121
step:    56900, time: 2.341, loss: 0.042942
step:    56920, time: 2.372, loss: 0.051602
step:    56940, time: 2.335, loss: 0.044292
step:    56960, time: 2.405, loss: 0.053311
step:    56980, time: 2.389, loss: 0.045790
step:    57000, time: 2.409, loss: 0.053730
step:    57020, time: 2.234, loss: 0.046333
step:    57040, time: 2.274, loss: 0.053813
step:    57060, time: 2.445, loss: 0.047954
step:    57080, time: 2.289, loss: 0.035273
step:    57100, time: 2.369, loss: 0.048146
step:    57120, time: 2.384, loss: 0.049929
step:    57140, time: 2.400, loss: 0.045035
step:    57160, time: 2.366, loss: 0.050001
step:    57180, time: 2.342, loss: 0.044537
step:    57200, time: 2.372, loss: 0.051080
step:    57220, time: 2.298, loss: 0.040703
step:    57240, time: 2.374, loss: 0.047340
step:    57260, time: 2.353, loss: 0.041775
step:    57280, time: 2.350, loss: 0.037107
step:    57300, time: 2.411, loss: 0.049500
step:    57320, time: 2.302, loss: 0.040449
step:    57340, time: 2.268, loss: 0.043035
step:    57360, time: 2.565, loss: 0.040050
step:    57380, time: 2.519, loss: 0.055468
step:    57400, time: 2.379, loss: 0.049180
step:    57420, time: 2.319, loss: 0.041569
step:    57440, time: 2.470, loss: 0.053415
step:    57460, time: 2.360, loss: 0.051392
step:    57480, time: 2.353, loss: 0.043018
step:    57500, time: 2.338, loss: 0.040090
step:    57520, time: 2.481, loss: 0.053994
step:    57540, time: 2.322, loss: 0.044973
step:    57560, time: 2.366, loss: 0.043116
step:    57580, time: 2.284, loss: 0.039940
step:    57600, time: 2.359, loss: 0.049245
step:    57620, time: 2.338, loss: 0.042629
step:    57640, time: 2.237, loss: 0.043025
step:    57660, time: 2.220, loss: 0.052365
step:    57680, time: 2.392, loss: 0.048197
step:    57700, time: 2.422, loss: 0.042883
step:    57720, time: 2.378, loss: 0.042622
step:    57740, time: 2.265, loss: 0.041737
step:    57760, time: 2.366, loss: 0.043421
step:    57780, time: 2.390, loss: 0.041228
step:    57800, time: 2.421, loss: 0.061602
step:    57820, time: 2.369, loss: 0.046664
step:    57840, time: 2.387, loss: 0.044887
step:    57860, time: 2.321, loss: 0.044265
step:    57880, time: 2.298, loss: 0.038451
step:    57900, time: 2.429, loss: 0.047209
step:    57920, time: 2.372, loss: 0.051420
step:    57940, time: 2.443, loss: 0.054332
step:    57960, time: 2.256, loss: 0.043116
step:    57980, time: 2.223, loss: 0.048210
step:    58000, time: 2.418, loss: 0.047325
step:    58020, time: 2.337, loss: 0.039230
step:    58040, time: 2.356, loss: 0.049941
step:    58060, time: 2.340, loss: 0.043851
step:    58080, time: 2.306, loss: 0.054168
step:    58100, time: 2.379, loss: 0.039693
step:    58120, time: 2.393, loss: 0.050944
step:    58140, time: 2.362, loss: 0.044269
step:    58160, time: 2.349, loss: 0.043645
step:    58180, time: 2.392, loss: 0.044749
step:    58200, time: 2.445, loss: 0.060118
step:    58220, time: 2.343, loss: 0.039728
step:    58240, time: 2.378, loss: 0.051612
step:    58260, time: 2.361, loss: 0.047174
step:    58280, time: 2.294, loss: 0.049932
step:    58300, time: 2.369, loss: 0.040600
step:    58320, time: 2.404, loss: 0.040393
step:    58340, time: 2.476, loss: 0.052427
step:    58360, time: 2.384, loss: 0.044912
step:    58380, time: 2.413, loss: 0.049157
step:    58400, time: 2.445, loss: 0.047546
step:    58420, time: 2.281, loss: 0.039055
step:    58440, time: 2.412, loss: 0.046209
step:    58460, time: 2.372, loss: 0.040939
step:    58480, time: 2.414, loss: 0.040214
step:    58500, time: 2.344, loss: 0.039909
step:    58520, time: 2.415, loss: 0.055536
step:    58540, time: 2.378, loss: 0.043219
step:    58560, time: 2.352, loss: 0.039482
step:    58580, time: 2.227, loss: 0.045592
step:    58600, time: 2.238, loss: 0.038981
step:    58620, time: 2.422, loss: 0.046140
step:    58640, time: 2.374, loss: 0.042633
step:    58660, time: 2.407, loss: 0.043708
step:    58680, time: 2.355, loss: 0.042469
step:    58700, time: 2.398, loss: 0.050765
step:    58720, time: 2.411, loss: 0.043718
step:    58740, time: 2.372, loss: 0.043699
step:    58760, time: 2.418, loss: 0.045123
step:    58780, time: 2.410, loss: 0.044249
step:    58800, time: 2.434, loss: 0.040029
step:    58820, time: 2.475, loss: 0.047369
step:    58840, time: 2.352, loss: 0.047775
step:    58860, time: 2.353, loss: 0.038540
step:    58880, time: 2.327, loss: 0.049564
step:    58900, time: 2.264, loss: 0.047296
step:    58920, time: 2.388, loss: 0.041739
step:    58940, time: 2.403, loss: 0.045338
step:    58960, time: 2.398, loss: 0.046915
step:    58980, time: 2.303, loss: 0.038847
step:    59000, time: 2.422, loss: 0.044539
step:    59020, time: 2.441, loss: 0.049366
step:    59040, time: 2.362, loss: 0.041446
step:    59060, time: 2.361, loss: 0.041983
step:    59080, time: 2.464, loss: 0.053014
step:    59100, time: 2.411, loss: 0.047726
step:    59120, time: 2.323, loss: 0.039733
step:    59140, time: 2.469, loss: 0.052114
step:    59160, time: 2.467, loss: 0.065264
step:    59180, time: 2.364, loss: 0.040663
step:    59200, time: 2.309, loss: 0.043832
step:    59220, time: 2.226, loss: 0.042501
step:    59240, time: 2.449, loss: 0.054429
step:    59260, time: 2.394, loss: 0.046287
step:    59280, time: 2.327, loss: 0.039892
step:    59300, time: 2.437, loss: 0.050899
step:    59320, time: 2.427, loss: 0.061105
step:    59340, time: 2.427, loss: 0.052076
step:    59360, time: 2.344, loss: 0.044715
step:    59380, time: 2.424, loss: 0.052363
step:    59400, time: 2.315, loss: 0.039795
step:    59420, time: 2.412, loss: 0.050583
step:    59440, time: 2.389, loss: 0.053468
step:    59460, time: 2.361, loss: 0.047180
step:    59480, time: 2.410, loss: 0.058526
step:    59500, time: 2.396, loss: 0.056931
step:    59520, time: 2.181, loss: 0.046000
step:    59540, time: 2.304, loss: 0.059091
step:    59560, time: 2.419, loss: 0.050323
step:    59580, time: 2.390, loss: 0.043541
step:    59600, time: 2.338, loss: 0.041757
step:    59620, time: 2.304, loss: 0.045873
step:    59640, time: 2.318, loss: 0.042908
step:    59660, time: 2.359, loss: 0.042342
step:    59680, time: 2.363, loss: 0.045464
step:    59700, time: 2.446, loss: 0.051017
step:    59720, time: 2.382, loss: 0.040526
step:    59740, time: 2.341, loss: 0.039631
step:    59760, time: 2.298, loss: 0.036270
step:    59780, time: 2.391, loss: 0.043160
step:    59800, time: 2.367, loss: 0.046846
step:    59820, time: 2.250, loss: 0.042156
step:    59840, time: 2.226, loss: 0.046141
step:    59860, time: 2.408, loss: 0.041677
step:    59880, time: 2.418, loss: 0.050773
step:    59900, time: 2.373, loss: 0.047225
step:    59920, time: 2.321, loss: 0.048471
step:    59940, time: 2.421, loss: 0.050085
step:    59960, time: 2.439, loss: 0.044902
step:    59980, time: 2.367, loss: 0.044239
step:    60000, time: 2.415, loss: 0.054027
step:    60020, time: 2.409, loss: 0.044059
step:    60040, time: 2.364, loss: 0.045718
step:    60060, time: 2.444, loss: 0.053674
step:    60080, time: 2.299, loss: 0.035413
step:    60100, time: 2.419, loss: 0.049982
step:    60120, time: 2.398, loss: 0.045542
step:    60140, time: 2.233, loss: 0.037146
step:    60160, time: 2.216, loss: 0.052581
step:    60180, time: 2.483, loss: 0.046636
step:    60200, time: 2.367, loss: 0.045436
step:    60220, time: 2.459, loss: 0.055188
step:    60240, time: 2.391, loss: 0.049539
step:    60260, time: 2.359, loss: 0.043481
step:    60280, time: 2.335, loss: 0.042972
step:    60300, time: 2.489, loss: 0.060273
step:    60320, time: 2.405, loss: 0.050964
step:    60340, time: 2.514, loss: 0.056678
step:    60360, time: 2.388, loss: 0.045967
step:    60380, time: 2.420, loss: 0.052877
step:    60400, time: 2.387, loss: 0.041526
step:    60420, time: 2.416, loss: 0.048880
step:    60440, time: 2.299, loss: 0.043742
step:    60460, time: 2.236, loss: 0.047302
step:    60480, time: 4.064, loss: 0.046038
step:    60500, time: 2.352, loss: 0.042076
step:    60520, time: 2.387, loss: 0.050249
step:    60540, time: 2.428, loss: 0.047820
step:    60560, time: 2.325, loss: 0.050111
step:    60580, time: 2.316, loss: 0.041386
step:    60600, time: 2.421, loss: 0.048079
step:    60620, time: 2.388, loss: 0.045448
step:    60640, time: 2.457, loss: 0.048023
step:    60660, time: 2.441, loss: 0.050328
step:    60680, time: 2.331, loss: 0.043937
step:    60700, time: 2.331, loss: 0.043413
step:    60720, time: 2.345, loss: 0.045598
step:    60740, time: 2.456, loss: 0.048495
step:    60760, time: 2.273, loss: 0.053978
step:    60780, time: 2.297, loss: 0.050121
step:    60800, time: 2.544, loss: 0.054598
step:    60820, time: 2.379, loss: 0.045115
step:    60840, time: 2.462, loss: 0.050892
step:    60860, time: 2.301, loss: 0.054678
step:    60880, time: 2.372, loss: 0.043929
step:    60900, time: 2.432, loss: 0.046598
step:    60920, time: 2.425, loss: 0.052729
step:    60940, time: 2.265, loss: 0.036816
step:    60960, time: 2.372, loss: 0.044042
step:    60980, time: 2.364, loss: 0.047035
step:    61000, time: 2.408, loss: 0.044824
step:    61020, time: 2.366, loss: 0.040625
step:    61040, time: 2.404, loss: 0.043342
step:    61060, time: 2.365, loss: 0.045423
step:    61080, time: 2.250, loss: 0.040409
step:    61100, time: 2.210, loss: 0.041803
step:    61120, time: 2.392, loss: 0.043418
step:    61140, time: 2.404, loss: 0.044369
step:    61160, time: 2.430, loss: 0.054213
step:    61180, time: 2.457, loss: 0.046428
step:    61200, time: 2.339, loss: 0.042638
step:    61220, time: 2.379, loss: 0.046185
step:    61240, time: 2.415, loss: 0.047818
step:    61260, time: 2.333, loss: 0.048509
step:    61280, time: 2.441, loss: 0.047589
step:    61300, time: 2.318, loss: 0.040690
step:    61320, time: 2.353, loss: 0.041758
step:    61340, time: 2.447, loss: 0.052453
step:    61360, time: 2.390, loss: 0.041925
step:    61380, time: 2.262, loss: 0.038720
step:    61400, time: 2.261, loss: 0.053636
step:    61420, time: 2.488, loss: 0.047631
step:    61440, time: 2.421, loss: 0.047610
step:    61460, time: 2.425, loss: 0.042091
step:    61480, time: 2.393, loss: 0.047376
step:    61500, time: 2.365, loss: 0.039301
step:    61520, time: 2.393, loss: 0.045790
step:    61540, time: 2.409, loss: 0.043132
step:    61560, time: 2.341, loss: 0.041740
step:    61580, time: 2.344, loss: 0.042414
step:    61600, time: 2.349, loss: 0.044913
step:    61620, time: 2.279, loss: 0.048002
step:    61640, time: 2.384, loss: 0.042538
step:    61660, time: 2.282, loss: 0.042268
step:    61680, time: 2.317, loss: 0.043034
step:    61700, time: 2.200, loss: 0.039483
step:    61720, time: 2.208, loss: 0.036556
step:    61740, time: 2.395, loss: 0.045948
step:    61760, time: 2.414, loss: 0.049902
step:    61780, time: 2.308, loss: 0.043549
step:    61800, time: 2.380, loss: 0.047101
step:    61820, time: 2.443, loss: 0.050509
step:    61840, time: 2.341, loss: 0.042590
step:    61860, time: 2.313, loss: 0.036758
step:    61880, time: 2.413, loss: 0.049493
step:    61900, time: 2.418, loss: 0.055552
step:    61920, time: 2.308, loss: 0.044709
step:    61940, time: 2.398, loss: 0.053889
step:    61960, time: 2.348, loss: 0.039976
step:    61980, time: 2.329, loss: 0.043190
step:    62000, time: 2.328, loss: 0.049896
step:    62020, time: 2.306, loss: 0.048865
step:    62040, time: 2.332, loss: 0.043642
step:    62060, time: 2.391, loss: 0.046753
step:    62080, time: 2.464, loss: 0.051914
step:    62100, time: 2.359, loss: 0.040728
step:    62120, time: 2.447, loss: 0.055736
step:    62140, time: 2.372, loss: 0.047146
step:    62160, time: 2.364, loss: 0.043204
step:    62180, time: 2.392, loss: 0.048735
step:    62200, time: 2.308, loss: 0.041920
step:    62220, time: 2.412, loss: 0.051603
step:    62240, time: 2.437, loss: 0.056123
step:    62260, time: 2.370, loss: 0.050513
step:    62280, time: 2.392, loss: 0.044000
step:    62300, time: 2.304, loss: 0.040311
step:    62320, time: 2.265, loss: 0.043050
step:    62340, time: 2.195, loss: 0.040910
step:    62360, time: 2.386, loss: 0.044423
step:    62380, time: 2.440, loss: 0.061439
step:    62400, time: 2.356, loss: 0.043815
step:    62420, time: 2.347, loss: 0.043671
step:    62440, time: 2.406, loss: 0.051729
step:    62460, time: 2.446, loss: 0.048249
step:    62480, time: 2.356, loss: 0.045000
step:    62500, time: 2.350, loss: 0.043433
step:    62520, time: 2.388, loss: 0.045538
step:    62540, time: 2.346, loss: 0.039267
step:    62560, time: 2.374, loss: 0.050742
step:    62580, time: 2.431, loss: 0.045808
step:    62600, time: 2.391, loss: 0.042495
step:    62620, time: 2.434, loss: 0.049597
step:    62640, time: 2.315, loss: 0.046718
step:    62660, time: 2.243, loss: 0.046551
step:    62680, time: 2.458, loss: 0.048698
step:    62700, time: 2.399, loss: 0.046438
step:    62720, time: 2.344, loss: 0.038624
step:    62740, time: 2.285, loss: 0.039723
step:    62760, time: 2.382, loss: 0.050894
step:    62780, time: 2.415, loss: 0.055473
step:    62800, time: 2.440, loss: 0.054178
step:    62820, time: 2.377, loss: 0.055391
step:    62840, time: 2.292, loss: 0.040067
step:    62860, time: 2.313, loss: 0.038831
step:    62880, time: 2.330, loss: 0.045155
step:    62900, time: 2.344, loss: 0.046840
step:    62920, time: 2.347, loss: 0.046055
step:    62940, time: 2.286, loss: 0.044818
step:    62960, time: 2.295, loss: 0.045962
step:    62980, time: 2.471, loss: 0.057374
step:    63000, time: 2.313, loss: 0.045476
step:    63020, time: 2.384, loss: 0.048153
step:    63040, time: 2.361, loss: 0.046693
step:    63060, time: 2.393, loss: 0.040243
step:    63080, time: 2.454, loss: 0.049036
step:    63100, time: 2.534, loss: 0.053435
step:    63120, time: 2.397, loss: 0.046574
step:    63140, time: 2.366, loss: 0.044978
step:    63160, time: 2.434, loss: 0.051644
step:    63180, time: 2.454, loss: 0.050568
step:    63200, time: 2.316, loss: 0.037564
step:    63220, time: 2.355, loss: 0.038778
step:    63240, time: 2.409, loss: 0.045632
step:    63260, time: 2.303, loss: 0.041977
step:    63280, time: 2.298, loss: 0.064375
step:    63300, time: 2.501, loss: 0.057686
step:    63320, time: 2.325, loss: 0.039863
step:    63340, time: 2.290, loss: 0.042533
step:    63360, time: 2.357, loss: 0.040184
step:    63380, time: 2.357, loss: 0.041181
step:    63400, time: 2.324, loss: 0.046808
step:    63420, time: 2.333, loss: 0.040637
step:    63440, time: 2.318, loss: 0.037388
step:    63460, time: 2.313, loss: 0.041592
step:    63480, time: 2.473, loss: 0.050936
step:    63500, time: 2.367, loss: 0.043155
step:    63520, time: 2.361, loss: 0.045526
step:    63540, time: 2.350, loss: 0.042947
step:    63560, time: 2.277, loss: 0.047670
step:    63580, time: 2.250, loss: 0.046276
step:    63600, time: 3.677, loss: 0.052209
step:    63620, time: 2.390, loss: 0.044967
step:    63640, time: 2.419, loss: 0.054350
step:    63660, time: 2.425, loss: 0.050502
step:    63680, time: 2.416, loss: 0.052800
step:    63700, time: 2.389, loss: 0.044200
step:    63720, time: 2.434, loss: 0.055703
step:    63740, time: 2.429, loss: 0.049449
step:    63760, time: 2.368, loss: 0.044085
step:    63780, time: 2.383, loss: 0.050765
step:    63800, time: 2.304, loss: 0.041864
step:    63820, time: 2.385, loss: 0.050668
step:    63840, time: 2.345, loss: 0.047664
step:    63860, time: 2.375, loss: 0.043075
step:    63880, time: 2.255, loss: 0.046078
step:    63900, time: 2.350, loss: 0.047775
step:    63920, time: 2.500, loss: 0.050691
step:    63940, time: 2.291, loss: 0.039947
step:    63960, time: 2.433, loss: 0.055801
step:    63980, time: 2.377, loss: 0.049696
step:    64000, time: 2.384, loss: 0.053209
step:    64020, time: 2.333, loss: 0.038412
step:    64040, time: 2.418, loss: 0.052331
step:    64060, time: 2.325, loss: 0.037591
step:    64080, time: 2.274, loss: 0.035148
step:    64100, time: 2.358, loss: 0.053903
step:    64120, time: 2.274, loss: 0.039837
step:    64140, time: 2.369, loss: 0.045303
step:    64160, time: 2.336, loss: 0.042790
step:    64180, time: 2.437, loss: 0.056002
step:    64200, time: 2.234, loss: 0.039259
step:    64220, time: 2.256, loss: 0.049635
step:    64240, time: 2.505, loss: 0.056820
step:    64260, time: 2.440, loss: 0.041106
step:    64280, time: 2.349, loss: 0.047395
step:    64300, time: 2.389, loss: 0.041374
step:    64320, time: 2.477, loss: 0.052753
step:    64340, time: 2.399, loss: 0.049463
step:    64360, time: 2.359, loss: 0.047694
step:    64380, time: 2.499, loss: 0.062719
step:    64400, time: 2.378, loss: 0.051754
step:    64420, time: 2.327, loss: 0.042923
step:    64440, time: 2.407, loss: 0.052510
step:    64460, time: 2.343, loss: 0.042353
step:    64480, time: 2.428, loss: 0.049935
step:    64500, time: 2.358, loss: 0.051626
step:    64520, time: 2.205, loss: 0.044262
step:    64540, time: 2.394, loss: 0.047455
step:    64560, time: 2.346, loss: 0.038314
step:    64580, time: 2.356, loss: 0.042996
step:    64600, time: 2.380, loss: 0.044553
step:    64620, time: 2.379, loss: 0.042808
step:    64640, time: 2.448, loss: 0.049394
step:    64660, time: 2.341, loss: 0.044185
step:    64680, time: 2.372, loss: 0.053787
step:    64700, time: 2.333, loss: 0.044754
step:    64720, time: 2.337, loss: 0.047624
step:    64740, time: 2.408, loss: 0.047717
step:    64760, time: 2.496, loss: 0.052060
step:    64780, time: 2.359, loss: 0.051226
step:    64800, time: 2.325, loss: 0.040959
step:    64820, time: 2.281, loss: 0.045805
step:    64840, time: 2.284, loss: 0.045415
step:    64860, time: 2.495, loss: 0.050662
step:    64880, time: 2.374, loss: 0.045741
step:    64900, time: 2.355, loss: 0.043650
step:    64920, time: 2.356, loss: 0.046676
step:    64940, time: 2.383, loss: 0.050896
step:    64960, time: 2.401, loss: 0.047062
step:    64980, time: 2.430, loss: 0.052246
step:    65000, time: 2.295, loss: 0.043405
step:    65020, time: 2.358, loss: 0.041860
step:    65040, time: 2.323, loss: 0.038099
step:    65060, time: 2.419, loss: 0.045470
step:    65080, time: 2.313, loss: 0.040236
step:    65100, time: 2.498, loss: 0.057861
step:    65120, time: 2.350, loss: 0.040444
step:    65140, time: 2.262, loss: 0.046460
step:    65160, time: 2.450, loss: 0.048460
step:    65180, time: 2.404, loss: 0.046800
step:    65200, time: 2.372, loss: 0.044690
step:    65220, time: 2.443, loss: 0.053576
step:    65240, time: 2.305, loss: 0.036182
step:    65260, time: 2.417, loss: 0.043532
step:    65280, time: 2.396, loss: 0.039766
step:    65300, time: 2.406, loss: 0.044206
step:    65320, time: 2.397, loss: 0.046068
step:    65340, time: 2.414, loss: 0.043808
step:    65360, time: 2.396, loss: 0.044154
step:    65380, time: 2.384, loss: 0.041943
step:    65400, time: 2.351, loss: 0.041346
step:    65420, time: 2.375, loss: 0.046084
step:    65440, time: 2.181, loss: 0.034702
step:    65460, time: 2.260, loss: 0.041396
step:    65480, time: 2.460, loss: 0.054380
step:    65500, time: 2.381, loss: 0.042795
step:    65520, time: 2.333, loss: 0.037461
step:    65540, time: 2.362, loss: 0.040251
step:    65560, time: 2.396, loss: 0.044937
step:    65580, time: 2.332, loss: 0.044753
step:    65600, time: 2.421, loss: 0.054172
step:    65620, time: 2.353, loss: 0.044328
step:    65640, time: 2.390, loss: 0.048188
step:    65660, time: 2.354, loss: 0.048301
step:    65680, time: 2.327, loss: 0.040819
step:    65700, time: 2.449, loss: 0.045773
step:    65720, time: 2.249, loss: 0.037195
step:    65740, time: 2.373, loss: 0.047465
step:    65760, time: 2.390, loss: 0.047789
step:    65780, time: 2.197, loss: 0.040988
step:    65800, time: 2.474, loss: 0.053438
step:    65820, time: 2.382, loss: 0.042888
step:    65840, time: 2.409, loss: 0.045753
step:    65860, time: 2.389, loss: 0.050144
step:    65880, time: 2.298, loss: 0.038098
step:    65900, time: 2.266, loss: 0.034033
step:    65920, time: 2.358, loss: 0.043371
step:    65940, time: 2.358, loss: 0.041673
step:    65960, time: 2.349, loss: 0.043944
step:    65980, time: 2.342, loss: 0.053004
step:    66000, time: 2.335, loss: 0.050226
step:    66020, time: 2.400, loss: 0.048717
step:    66040, time: 2.376, loss: 0.040030
step:    66060, time: 2.277, loss: 0.042303
step:    66080, time: 2.202, loss: 0.042338
step:    66100, time: 2.467, loss: 0.049417
step:    66120, time: 2.388, loss: 0.049954
step:    66140, time: 2.324, loss: 0.042070
step:    66160, time: 2.295, loss: 0.035981
step:    66180, time: 2.406, loss: 0.048552
step:    66200, time: 2.322, loss: 0.050193
step:    66220, time: 2.362, loss: 0.051724
step:    66240, time: 2.401, loss: 0.045653
step:    66260, time: 2.464, loss: 0.056253
step:    66280, time: 2.349, loss: 0.039951
step:    66300, time: 2.362, loss: 0.042224
step:    66320, time: 2.408, loss: 0.043864
step:    66340, time: 2.390, loss: 0.045260
step:    66360, time: 2.371, loss: 0.042065
step:    66380, time: 2.212, loss: 0.036666
step:    66400, time: 2.336, loss: 0.055710
step:    66420, time: 2.320, loss: 0.047342
step:    66440, time: 2.286, loss: 0.039573
step:    66460, time: 2.318, loss: 0.051994
step:    66480, time: 2.324, loss: 0.048165
step:    66500, time: 2.375, loss: 0.043831
step:    66520, time: 2.386, loss: 0.041248
step:    66540, time: 2.356, loss: 0.043903
step:    66560, time: 2.421, loss: 0.049653
step:    66580, time: 2.346, loss: 0.044722
step:    66600, time: 2.396, loss: 0.045562
step:    66620, time: 2.328, loss: 0.046430
step:    66640, time: 2.341, loss: 0.041913
step:    66660, time: 2.359, loss: 0.044275
step:    66680, time: 2.369, loss: 0.044264
step:    66700, time: 2.296, loss: 0.057917
step:    66720, time: 2.873, loss: 0.041246
step:    66740, time: 2.391, loss: 0.041204
step:    66760, time: 2.476, loss: 0.048721
step:    66780, time: 2.406, loss: 0.040180
step:    66800, time: 2.368, loss: 0.041197
step:    66820, time: 2.340, loss: 0.041178
step:    66840, time: 2.328, loss: 0.043973
step:    66860, time: 2.303, loss: 0.039951
step:    66880, time: 2.389, loss: 0.046608
step:    66900, time: 2.404, loss: 0.039997
step:    66920, time: 2.377, loss: 0.043252
step:    66940, time: 2.324, loss: 0.045551
step:    66960, time: 2.375, loss: 0.045736
step:    66980, time: 2.424, loss: 0.051805
step:    67000, time: 2.354, loss: 0.055173
step:    67020, time: 2.193, loss: 0.045104
step:    67040, time: 2.372, loss: 0.043487
step:    67060, time: 2.334, loss: 0.038335
step:    67080, time: 2.405, loss: 0.042236
step:    67100, time: 2.348, loss: 0.041718
step:    67120, time: 2.452, loss: 0.054731
step:    67140, time: 2.322, loss: 0.045056
step:    67160, time: 2.350, loss: 0.040120
step:    67180, time: 2.476, loss: 0.051361
step:    67200, time: 2.451, loss: 0.055109
step:    67220, time: 2.396, loss: 0.042557
step:    67240, time: 2.345, loss: 0.041447
step:    67260, time: 2.447, loss: 0.048743
step:    67280, time: 2.421, loss: 0.043491
step:    67300, time: 2.323, loss: 0.042529
step:    67320, time: 2.230, loss: 0.050455
step:    67340, time: 2.245, loss: 0.040355
step:    67360, time: 2.441, loss: 0.048971
step:    67380, time: 2.413, loss: 0.044626
step:    67400, time: 2.404, loss: 0.046919
step:    67420, time: 2.302, loss: 0.047257
step:    67440, time: 2.366, loss: 0.041725
step:    67460, time: 2.310, loss: 0.040271
step:    67480, time: 2.445, loss: 0.053415
step:    67500, time: 2.472, loss: 0.057564
step:    67520, time: 2.337, loss: 0.045547
step:    67540, time: 2.315, loss: 0.040655
step:    67560, time: 2.370, loss: 0.043875
step:    67580, time: 2.342, loss: 0.038330
step:    67600, time: 2.345, loss: 0.042161
step:    67620, time: 2.303, loss: 0.043390
step:    67640, time: 2.267, loss: 0.052028
step:    67660, time: 2.428, loss: 0.043830
step:    67680, time: 2.325, loss: 0.042275
step:    67700, time: 2.401, loss: 0.043585
step:    67720, time: 2.460, loss: 0.053854
step:    67740, time: 2.393, loss: 0.049430
step:    67760, time: 2.475, loss: 0.046782
step:    67780, time: 2.415, loss: 0.042591
step:    67800, time: 2.435, loss: 0.051398
step:    67820, time: 2.430, loss: 0.048660
step:    67840, time: 2.457, loss: 0.051519
step:    67860, time: 2.328, loss: 0.039766
step:    67880, time: 2.365, loss: 0.050234
step:    67900, time: 2.474, loss: 0.051066
step:    67920, time: 2.418, loss: 0.047774
step:    67940, time: 2.286, loss: 0.043950
step:    67960, time: 2.214, loss: 0.043191
step:    67980, time: 2.458, loss: 0.047304
step:    68000, time: 2.351, loss: 0.044066
step:    68020, time: 2.488, loss: 0.059000
step:    68040, time: 2.378, loss: 0.041525
step:    68060, time: 2.403, loss: 0.046094
step:    68080, time: 2.403, loss: 0.044805
step:    68100, time: 2.332, loss: 0.040847
step:    68120, time: 2.412, loss: 0.051051
step:    68140, time: 2.303, loss: 0.033810
step:    68160, time: 2.476, loss: 0.048209
step:    68180, time: 2.342, loss: 0.049214
step:    68200, time: 2.378, loss: 0.039631
step:    68220, time: 2.374, loss: 0.039552
step:    68240, time: 2.308, loss: 0.041334
step:    68260, time: 2.327, loss: 0.048267
step:    68280, time: 2.832, loss: 0.049994
step:    68300, time: 2.377, loss: 0.047114
step:    68320, time: 2.383, loss: 0.043358
step:    68340, time: 2.422, loss: 0.050700
step:    68360, time: 2.386, loss: 0.048897
step:    68380, time: 2.380, loss: 0.047942
step:    68400, time: 2.473, loss: 0.046537
step:    68420, time: 2.328, loss: 0.044844
step:    68440, time: 2.483, loss: 0.052895
step:    68460, time: 2.382, loss: 0.048033
step:    68480, time: 2.483, loss: 0.052106
step:    68500, time: 2.389, loss: 0.047071
step:    68520, time: 2.352, loss: 0.040241
step:    68540, time: 2.426, loss: 0.042672
step:    68560, time: 2.318, loss: 0.046401
step:    68580, time: 2.261, loss: 0.046557
step:    68600, time: 2.440, loss: 0.054977
step:    68620, time: 2.362, loss: 0.044566
step:    68640, time: 2.561, loss: 0.052988
step:    68660, time: 2.375, loss: 0.049405
step:    68680, time: 2.367, loss: 0.046934
step:    68700, time: 2.369, loss: 0.037885
step:    68720, time: 2.404, loss: 0.044306
step:    68740, time: 2.389, loss: 0.040535
step:    68760, time: 2.482, loss: 0.050843
step:    68780, time: 2.360, loss: 0.053255
step:    68800, time: 2.361, loss: 0.047840
step:    68820, time: 2.394, loss: 0.042027
step:    68840, time: 2.389, loss: 0.043280
step:    68860, time: 2.378, loss: 0.044983
step:    68880, time: 2.136, loss: 0.030906
step:    68900, time: 2.303, loss: 0.045222
step:    68920, time: 2.495, loss: 0.056942
step:    68940, time: 2.462, loss: 0.052523
step:    68960, time: 2.431, loss: 0.050646
step:    68980, time: 2.344, loss: 0.045978
step:    69000, time: 2.399, loss: 0.047455
step:    69020, time: 2.438, loss: 0.051952
step:    69040, time: 2.343, loss: 0.043468
step:    69060, time: 2.345, loss: 0.042430
step:    69080, time: 2.415, loss: 0.046813
step:    69100, time: 2.369, loss: 0.039335
step:    69120, time: 2.377, loss: 0.041101
step:    69140, time: 2.294, loss: 0.038818
step:    69160, time: 2.485, loss: 0.050838
step:    69180, time: 2.251, loss: 0.057809
step:    69200, time: 2.258, loss: 0.047639
step:    69220, time: 2.419, loss: 0.053402
step:    69240, time: 2.284, loss: 0.040390
step:    69260, time: 2.480, loss: 0.052253
step:    69280, time: 2.421, loss: 0.050677
step:    69300, time: 2.461, loss: 0.050726
step:    69320, time: 2.385, loss: 0.040642
step:    69340, time: 2.344, loss: 0.047080
step:    69360, time: 2.441, loss: 0.049533
step:    69380, time: 2.318, loss: 0.043373
step:    69400, time: 2.467, loss: 0.051483
step:    69420, time: 2.472, loss: 0.046178
step:    69440, time: 2.370, loss: 0.039539
step:    69460, time: 2.278, loss: 0.039198
step:    69480, time: 2.427, loss: 0.046599
step:    69500, time: 2.208, loss: 0.037328
step:    69520, time: 2.165, loss: 0.039844
step:    69540, time: 2.356, loss: 0.036882
step:    69560, time: 2.462, loss: 0.045983
step:    69580, time: 2.335, loss: 0.040472
step:    69600, time: 2.324, loss: 0.041539
step:    69620, time: 2.389, loss: 0.050281
step:    69640, time: 2.288, loss: 0.041101
step:    69660, time: 2.405, loss: 0.043025
step:    69680, time: 2.371, loss: 0.053583
step:    69700, time: 2.312, loss: 0.040920
step:    69720, time: 2.452, loss: 0.047331
step:    69740, time: 2.376, loss: 0.056714
step:    69760, time: 2.390, loss: 0.047791
step:    69780, time: 2.380, loss: 0.039030
step:    69800, time: 2.320, loss: 0.044678
step:    69820, time: 2.230, loss: 0.046547
step:    69840, time: 2.378, loss: 0.043771
step:    69860, time: 2.399, loss: 0.048554
step:    69880, time: 2.319, loss: 0.043526
step:    69900, time: 2.350, loss: 0.048488
step:    69920, time: 2.428, loss: 0.049572
step:    69940, time: 2.488, loss: 0.051249
step:    69960, time: 2.368, loss: 0.045763
step:    69980, time: 2.464, loss: 0.055215
step:    70000, time: 2.356, loss: 0.041691
step:    70020, time: 2.377, loss: 0.044720
step:    70040, time: 2.440, loss: 0.056790
step:    70060, time: 2.405, loss: 0.048216
step:    70080, time: 2.378, loss: 0.047584
step:    70100, time: 2.359, loss: 0.042077
step:    70120, time: 2.327, loss: 0.043770
step:    70140, time: 2.369, loss: 0.048149
step:    70160, time: 2.358, loss: 0.040718
step:    70180, time: 2.424, loss: 0.051470
step:    70200, time: 2.398, loss: 0.042297
step:    70220, time: 2.440, loss: 0.045526
step:    70240, time: 2.424, loss: 0.049528
step:    70260, time: 2.326, loss: 0.040472
step:    70280, time: 2.366, loss: 0.044256
step:    70300, time: 2.422, loss: 0.048523
step:    70320, time: 2.458, loss: 0.040313
step:    70340, time: 2.338, loss: 0.045916
step:    70360, time: 2.352, loss: 0.039447
step:    70380, time: 2.444, loss: 0.048451
step:    70400, time: 2.334, loss: 0.041878
step:    70420, time: 2.369, loss: 0.040082
step:    70440, time: 2.356, loss: 0.051070
step:    70460, time: 2.251, loss: 0.048040
step:    70480, time: 2.423, loss: 0.042474
step:    70500, time: 2.453, loss: 0.050838
step:    70520, time: 2.457, loss: 0.051157
step:    70540, time: 2.400, loss: 0.041934
step:    70560, time: 2.349, loss: 0.042207
step:    70580, time: 2.413, loss: 0.052257
step:    70600, time: 2.432, loss: 0.060116
step:    70620, time: 2.342, loss: 0.043547
step:    70640, time: 2.463, loss: 0.056854
step:    70660, time: 2.334, loss: 0.038260
step:    70680, time: 2.368, loss: 0.048406
step:    70700, time: 2.295, loss: 0.041137
step:    70720, time: 2.397, loss: 0.053018
step:    70740, time: 2.204, loss: 0.038072
step:    70760, time: 2.245, loss: 0.045195
step:    70780, time: 2.517, loss: 0.053340
step:    70800, time: 2.339, loss: 0.039732
step:    70820, time: 2.403, loss: 0.042748
step:    70840, time: 2.355, loss: 0.042104
step:    70860, time: 2.481, loss: 0.051791
step:    70880, time: 2.474, loss: 0.054829
step:    70900, time: 2.371, loss: 0.046399
step:    70920, time: 2.365, loss: 0.042609
step:    70940, time: 2.304, loss: 0.042927
step:    70960, time: 2.318, loss: 0.042300
step:    70980, time: 2.290, loss: 0.038778
step:    71000, time: 2.413, loss: 0.045236
step:    71020, time: 2.368, loss: 0.052797
step:    71040, time: 2.363, loss: 0.038451
step:    71060, time: 2.257, loss: 0.038421
step:    71080, time: 2.254, loss: 0.044746
step:    71100, time: 2.432, loss: 0.052296
step:    71120, time: 2.374, loss: 0.044575
step:    71140, time: 2.340, loss: 0.041698
step:    71160, time: 2.327, loss: 0.039616
step:    71180, time: 2.431, loss: 0.046596
step:    71200, time: 2.278, loss: 0.039876
step:    71220, time: 2.416, loss: 0.042795
step:    71240, time: 2.409, loss: 0.046189
step:    71260, time: 2.403, loss: 0.046683
step:    71280, time: 2.381, loss: 0.048568
step:    71300, time: 2.361, loss: 0.044572
step:    71320, time: 2.408, loss: 0.043700
step:    71340, time: 2.373, loss: 0.045179
step:    71360, time: 2.448, loss: 0.054015
step:    71380, time: 2.232, loss: 0.044156
step:    71400, time: 2.371, loss: 0.041241
step:    71420, time: 2.436, loss: 0.053707
step:    71440, time: 2.353, loss: 0.042625
step:    71460, time: 2.405, loss: 0.040688
step:    71480, time: 2.459, loss: 0.048788
step:    71500, time: 2.414, loss: 0.045508
step:    71520, time: 2.446, loss: 0.043961
step:    71540, time: 2.399, loss: 0.047261
step:    71560, time: 2.399, loss: 0.046678
step:    71580, time: 2.413, loss: 0.055564
step:    71600, time: 2.394, loss: 0.044406
step:    71620, time: 2.517, loss: 0.053211
step:    71640, time: 2.450, loss: 0.048473
step:    71660, time: 2.380, loss: 0.041970
step:    71680, time: 2.387, loss: 0.050590
step:    71700, time: 2.329, loss: 0.050650
step:    71720, time: 2.429, loss: 0.046181
step:    71740, time: 2.338, loss: 0.039427
step:    71760, time: 2.371, loss: 0.042424
step:    71780, time: 2.359, loss: 0.042011
step:    71800, time: 2.336, loss: 0.045326
step:    71820, time: 2.397, loss: 0.052186
step:    71840, time: 2.375, loss: 0.045640
step:    71860, time: 2.428, loss: 0.049158
step:    71880, time: 2.435, loss: 0.049929
step:    71900, time: 2.362, loss: 0.046425
step:    71920, time: 2.353, loss: 0.036372
step:    71940, time: 2.414, loss: 0.046109
step:    71960, time: 2.415, loss: 0.052397
step:    71980, time: 2.342, loss: 0.052760
step:    72000, time: 2.301, loss: 0.046346
step:    72020, time: 2.214, loss: 0.038187
step:    72040, time: 2.457, loss: 0.053104
step:    72060, time: 2.488, loss: 0.052441
step:    72080, time: 2.346, loss: 0.045527
step:    72100, time: 2.332, loss: 0.042393
step:    72120, time: 2.401, loss: 0.047864
step:    72140, time: 2.357, loss: 0.037770
step:    72160, time: 2.325, loss: 0.036581
step:    72180, time: 2.396, loss: 0.041770
step:    72200, time: 2.252, loss: 0.040628
step:    72220, time: 2.287, loss: 0.040279
step:    72240, time: 2.368, loss: 0.051197
step:    72260, time: 2.408, loss: 0.053972
step:    72280, time: 2.418, loss: 0.045225
step:    72300, time: 2.220, loss: 0.036602
step:    72320, time: 2.250, loss: 0.047901
step:    72340, time: 2.538, loss: 0.057072
step:    72360, time: 2.426, loss: 0.053105
step:    72380, time: 2.304, loss: 0.035380
step:    72400, time: 2.299, loss: 0.045040
step:    72420, time: 2.389, loss: 0.051878
step:    72440, time: 2.362, loss: 0.039266
step:    72460, time: 2.311, loss: 0.045955
step:    72480, time: 2.503, loss: 0.061045
step:    72500, time: 2.382, loss: 0.047387
step:    72520, time: 2.381, loss: 0.046522
step:    72540, time: 2.417, loss: 0.047918
step:    72560, time: 2.364, loss: 0.051859
step:    72580, time: 2.413, loss: 0.049264
step:    72600, time: 2.412, loss: 0.047583
step:    72620, time: 2.330, loss: 0.048389
step:    72640, time: 2.179, loss: 0.041791
step:    72660, time: 2.403, loss: 0.051354
step:    72680, time: 2.371, loss: 0.043527
step:    72700, time: 2.420, loss: 0.050385
step:    72720, time: 2.345, loss: 0.041156
step:    72740, time: 2.364, loss: 0.041570
step:    72760, time: 2.388, loss: 0.052972
step:    72780, time: 2.401, loss: 0.048192
step:    72800, time: 2.386, loss: 0.045496
step:    72820, time: 2.353, loss: 0.046335
step:    72840, time: 2.373, loss: 0.047884
step:    72860, time: 2.416, loss: 0.049550
step:    72880, time: 2.329, loss: 0.047104
step:    72900, time: 2.415, loss: 0.054006
step:    72920, time: 2.369, loss: 0.044165
step:    72940, time: 2.293, loss: 0.048975
step:    72960, time: 2.635, loss: 0.048723
step:    72980, time: 2.365, loss: 0.044288
step:    73000, time: 2.417, loss: 0.048141
step:    73020, time: 2.356, loss: 0.048502
step:    73040, time: 2.400, loss: 0.045284
step:    73060, time: 2.394, loss: 0.045005
step:    73080, time: 2.383, loss: 0.042643
step:    73100, time: 2.416, loss: 0.049854
step:    73120, time: 2.401, loss: 0.048065
step:    73140, time: 2.358, loss: 0.042085
step:    73160, time: 2.289, loss: 0.041845
step:    73180, time: 2.415, loss: 0.049090
step:    73200, time: 2.308, loss: 0.045658
step:    73220, time: 2.307, loss: 0.040315
step:    73240, time: 2.232, loss: 0.037129
step:    73260, time: 2.205, loss: 0.042636
step:    73280, time: 2.436, loss: 0.050402
step:    73300, time: 2.459, loss: 0.060821
step:    73320, time: 2.379, loss: 0.044700
step:    73340, time: 2.296, loss: 0.040820
step:    73360, time: 2.405, loss: 0.042046
step:    73380, time: 2.372, loss: 0.041992
step:    73400, time: 2.339, loss: 0.043613
step:    73420, time: 2.457, loss: 0.046797
step:    73440, time: 2.476, loss: 0.049628
step:    73460, time: 2.370, loss: 0.046309
step:    73480, time: 2.439, loss: 0.059150
step:    73500, time: 2.375, loss: 0.047783
step:    73520, time: 2.394, loss: 0.041566
step:    73540, time: 2.367, loss: 0.053106
step:    73560, time: 2.272, loss: 0.039061
step:    73580, time: 2.317, loss: 0.053629
step:    73600, time: 2.437, loss: 0.050840
step:    73620, time: 2.402, loss: 0.043879
step:    73640, time: 2.355, loss: 0.035923
step:    73660, time: 2.372, loss: 0.044031
step:    73680, time: 2.438, loss: 0.042223
step:    73700, time: 2.313, loss: 0.046161
step:    73720, time: 2.302, loss: 0.045657
step:    73740, time: 2.338, loss: 0.044473
step:    73760, time: 2.363, loss: 0.048152
step:    73780, time: 2.355, loss: 0.043405
step:    73800, time: 2.402, loss: 0.053480
step:    73820, time: 2.353, loss: 0.042207
step:    73840, time: 2.277, loss: 0.041101
step:    73860, time: 2.258, loss: 0.042034
step:    73880, time: 2.285, loss: 0.038868
step:    73900, time: 2.399, loss: 0.043447
step:    73920, time: 2.326, loss: 0.045538
step:    73940, time: 2.313, loss: 0.038651
step:    73960, time: 2.351, loss: 0.043205
step:    73980, time: 2.352, loss: 0.047258
step:    74000, time: 2.369, loss: 0.045306
step:    74020, time: 2.438, loss: 0.055763
step:    74040, time: 2.424, loss: 0.046819
step:    74060, time: 2.419, loss: 0.047073
step:    74080, time: 2.348, loss: 0.043199
step:    74100, time: 2.392, loss: 0.051452
step:    74120, time: 2.410, loss: 0.045351
step:    74140, time: 2.349, loss: 0.049396
step:    74160, time: 2.411, loss: 0.047118
step:    74180, time: 2.203, loss: 0.040343
step:    74200, time: 2.194, loss: 0.038566
step:    74220, time: 2.462, loss: 0.051328
step:    74240, time: 2.362, loss: 0.042991
step:    74260, time: 2.351, loss: 0.047931
step:    74280, time: 2.370, loss: 0.044461
step:    74300, time: 2.431, loss: 0.045194
step:    74320, time: 2.425, loss: 0.046107
step:    74340, time: 2.396, loss: 0.040749
step:    74360, time: 2.412, loss: 0.054216
step:    74380, time: 2.409, loss: 0.045941
step:    74400, time: 2.382, loss: 0.050099
step:    74420, time: 2.433, loss: 0.051071
step:    74440, time: 2.320, loss: 0.040536
step:    74460, time: 2.408, loss: 0.041450
step:    74480, time: 2.344, loss: 0.040115
step:    74500, time: 2.203, loss: 0.041354
step:    74520, time: 2.372, loss: 0.046152
step:    74540, time: 2.406, loss: 0.042754
step:    74560, time: 2.333, loss: 0.046767
step:    74580, time: 2.264, loss: 0.039443
step:    74600, time: 2.408, loss: 0.046281
step:    74620, time: 2.302, loss: 0.042983
step:    74640, time: 2.478, loss: 0.063187
step:    74660, time: 2.322, loss: 0.041926
step:    74680, time: 2.361, loss: 0.047156
step:    74700, time: 2.427, loss: 0.046476
step:    74720, time: 2.477, loss: 0.048073
step:    74740, time: 2.334, loss: 0.042125
step:    74760, time: 2.322, loss: 0.039239
step:    74780, time: 2.489, loss: 0.060011
step:    74800, time: 2.233, loss: 0.043730
step:    74820, time: 2.187, loss: 0.040742
step:    74840, time: 2.451, loss: 0.047395
step:    74860, time: 2.318, loss: 0.041051
step:    74880, time: 2.419, loss: 0.042933
step:    74900, time: 2.417, loss: 0.047133
step:    74920, time: 2.298, loss: 0.048134
step:    74940, time: 2.356, loss: 0.047075
step:    74960, time: 2.214, loss: 0.035251
step:    74980, time: 2.315, loss: 0.051020
step:    75000, time: 2.391, loss: 0.043351
step:    75020, time: 2.492, loss: 0.059345
step:    75040, time: 2.400, loss: 0.047444
step:    75060, time: 2.373, loss: 0.047032
step:    75080, time: 2.377, loss: 0.049209
step:    75100, time: 2.323, loss: 0.051126
step:    75120, time: 2.319, loss: 0.046723
step:    75140, time: 2.237, loss: 0.044043
step:    75160, time: 2.361, loss: 0.044726
step:    75180, time: 2.320, loss: 0.037574
step:    75200, time: 2.285, loss: 0.041401
step:    75220, time: 2.425, loss: 0.053118
step:    75240, time: 2.321, loss: 0.039435
step:    75260, time: 2.372, loss: 0.047940
step:    75280, time: 2.423, loss: 0.042490
step:    75300, time: 2.288, loss: 0.042926
step:    75320, time: 2.375, loss: 0.040004
step:    75340, time: 2.345, loss: 0.035829
step:    75360, time: 2.384, loss: 0.045450
step:    75380, time: 2.381, loss: 0.051381
step:    75400, time: 2.379, loss: 0.043405
step:    75420, time: 2.332, loss: 0.039562
step:    75440, time: 2.244, loss: 0.040702
step:    75460, time: 2.514, loss: 0.055514
step:    75480, time: 2.342, loss: 0.041036
step:    75500, time: 2.325, loss: 0.043203
step:    75520, time: 2.322, loss: 0.042338
step:    75540, time: 2.329, loss: 0.037443
step:    75560, time: 2.431, loss: 0.050359
step:    75580, time: 2.378, loss: 0.046024
step:    75600, time: 2.440, loss: 0.048014
step:    75620, time: 2.392, loss: 0.047253
step:    75640, time: 2.409, loss: 0.052425
step:    75660, time: 2.421, loss: 0.044214
step:    75680, time: 2.425, loss: 0.049823
step:    75700, time: 2.415, loss: 0.049495
step:    75720, time: 2.431, loss: 0.046434
step:    75740, time: 2.244, loss: 0.048982
step:    75760, time: 2.144, loss: 0.035089
step:    75780, time: 2.464, loss: 0.043192
step:    75800, time: 2.335, loss: 0.043102
step:    75820, time: 2.380, loss: 0.043231
step:    75840, time: 2.298, loss: 0.043027
step:    75860, time: 2.462, loss: 0.049395
step:    75880, time: 2.330, loss: 0.052139
step:    75900, time: 2.459, loss: 0.054052
step:    75920, time: 2.313, loss: 0.039473
step:    75940, time: 2.408, loss: 0.050323
step:    75960, time: 2.374, loss: 0.040925
step:    75980, time: 2.369, loss: 0.047331
step:    76000, time: 2.393, loss: 0.053310
step:    76020, time: 2.304, loss: 0.043412
step:    76040, time: 2.375, loss: 0.047684
step:    76060, time: 2.261, loss: 0.057147
step:    76080, time: 2.462, loss: 0.057378
step:    76100, time: 2.381, loss: 0.046679
step:    76120, time: 2.437, loss: 0.054682
step:    76140, time: 2.460, loss: 0.048866
step:    76160, time: 2.492, loss: 0.052554
step:    76180, time: 2.354, loss: 0.039237
step:    76200, time: 2.454, loss: 0.043999
step:    76220, time: 2.380, loss: 0.041333
step:    76240, time: 2.256, loss: 0.040348
step:    76260, time: 2.384, loss: 0.042489
step:    76280, time: 2.297, loss: 0.043999
step:    76300, time: 2.341, loss: 0.042321
step:    76320, time: 2.356, loss: 0.051666
step:    76340, time: 2.389, loss: 0.048296
step:    76360, time: 2.251, loss: 0.041187
step:    76380, time: 2.248, loss: 0.042859
step:    76400, time: 2.537, loss: 0.048038
step:    76420, time: 2.380, loss: 0.042736
step:    76440, time: 2.363, loss: 0.037850
step:    76460, time: 2.327, loss: 0.042556
step:    76480, time: 2.380, loss: 0.041895
step:    76500, time: 2.330, loss: 0.039183
step:    76520, time: 2.351, loss: 0.038280
step:    76540, time: 2.374, loss: 0.046394
step:    76560, time: 2.436, loss: 0.043475
step:    76580, time: 2.459, loss: 0.050342
step:    76600, time: 2.459, loss: 0.053448
step:    76620, time: 2.373, loss: 0.042777
step:    76640, time: 2.324, loss: 0.042483
step:    76660, time: 2.372, loss: 0.043889
step:    76680, time: 2.307, loss: 0.039968
step:    76700, time: 2.209, loss: 0.048036
step:    76720, time: 2.307, loss: 0.049426
step:    76740, time: 2.365, loss: 0.041828
step:    76760, time: 2.403, loss: 0.044660
step:    76780, time: 2.443, loss: 0.052720
step:    76800, time: 2.318, loss: 0.044145
step:    76820, time: 2.373, loss: 0.046814
step:    76840, time: 2.329, loss: 0.035345
step:    76860, time: 2.365, loss: 0.044910
step:    76880, time: 2.328, loss: 0.044489
step:    76900, time: 2.371, loss: 0.046936
step:    76920, time: 2.428, loss: 0.046388
step:    76940, time: 2.400, loss: 0.046932
step:    76960, time: 2.355, loss: 0.051951
step:    76980, time: 2.377, loss: 0.046098
step:    77000, time: 2.283, loss: 0.047442
step:    77020, time: 2.399, loss: 0.040129
step:    77040, time: 2.409, loss: 0.048587
step:    77060, time: 2.305, loss: 0.037361
step:    77080, time: 2.347, loss: 0.049212
step:    77100, time: 2.385, loss: 0.048404
step:    77120, time: 2.262, loss: 0.038187
step:    77140, time: 2.329, loss: 0.040990
step:    77160, time: 2.427, loss: 0.044374
step:    77180, time: 2.360, loss: 0.039166
step:    77200, time: 2.326, loss: 0.039663
step:    77220, time: 2.351, loss: 0.042882
step:    77240, time: 2.438, loss: 0.048984
step:    77260, time: 2.383, loss: 0.050415
step:    77280, time: 2.399, loss: 0.056292
step:    77300, time: 2.281, loss: 0.047596
step:    77320, time: 2.267, loss: 0.045330
step:    77340, time: 2.456, loss: 0.044448
step:    77360, time: 2.287, loss: 0.047319
step:    77380, time: 2.448, loss: 0.056121
step:    77400, time: 2.439, loss: 0.053896
step:    77420, time: 2.277, loss: 0.041691
step:    77440, time: 2.336, loss: 0.042135
step:    77460, time: 2.345, loss: 0.044523
step:    77480, time: 2.458, loss: 0.052416
step:    77500, time: 2.413, loss: 0.055342
step:    77520, time: 2.346, loss: 0.048988
step:    77540, time: 2.360, loss: 0.042185
step:    77560, time: 2.408, loss: 0.046808
step:    77580, time: 2.332, loss: 0.046011
step:    77600, time: 2.381, loss: 0.046992
step:    77620, time: 2.261, loss: 0.044167
step:    77640, time: 3.908, loss: 0.042173
step:    77660, time: 2.496, loss: 0.055849
step:    77680, time: 2.358, loss: 0.047673
step:    77700, time: 2.436, loss: 0.048504
step:    77720, time: 2.399, loss: 0.044842
step:    77740, time: 2.361, loss: 0.040493
step:    77760, time: 2.391, loss: 0.043703
step:    77780, time: 2.364, loss: 0.043458
step:    77800, time: 2.363, loss: 0.047151
step:    77820, time: 2.350, loss: 0.038141
step:    77840, time: 2.393, loss: 0.040688
step:    77860, time: 2.388, loss: 0.042202
step:    77880, time: 2.389, loss: 0.044758
step:    77900, time: 2.378, loss: 0.046215
step:    77920, time: 2.380, loss: 0.046611
step:    77940, time: 2.286, loss: 0.043492
step:    77960, time: 2.407, loss: 0.040997
step:    77980, time: 2.411, loss: 0.043951
step:    78000, time: 2.369, loss: 0.050652
step:    78020, time: 2.481, loss: 0.050799
step:    78040, time: 2.348, loss: 0.042701
step:    78060, time: 2.368, loss: 0.045637
step:    78080, time: 2.399, loss: 0.049021
step:    78100, time: 2.359, loss: 0.044476
step:    78120, time: 2.297, loss: 0.042561
step:    78140, time: 2.488, loss: 0.049835
step:    78160, time: 2.438, loss: 0.051959
step:    78180, time: 2.379, loss: 0.043206
step:    78200, time: 2.389, loss: 0.048218
step:    78220, time: 2.418, loss: 0.049598
step:    78240, time: 2.241, loss: 0.042228
step:    78260, time: 2.285, loss: 0.058554
step:    78280, time: 2.318, loss: 0.041083
step:    78300, time: 2.348, loss: 0.047366
step:    78320, time: 2.456, loss: 0.047953
step:    78340, time: 2.399, loss: 0.042540
step:    78360, time: 2.439, loss: 0.048070
step:    78380, time: 2.370, loss: 0.041959
step:    78400, time: 2.337, loss: 0.044323
step:    78420, time: 2.314, loss: 0.033961
step:    78440, time: 2.466, loss: 0.045045
step:    78460, time: 2.382, loss: 0.047324
step:    78480, time: 2.369, loss: 0.039218
step:    78500, time: 2.397, loss: 0.038690
step:    78520, time: 2.315, loss: 0.047642
step:    78540, time: 2.390, loss: 0.046522
step:    78560, time: 2.219, loss: 0.047695
step:    78580, time: 2.373, loss: 0.039345
step:    78600, time: 2.399, loss: 0.043872
step:    78620, time: 2.415, loss: 0.042317
step:    78640, time: 2.421, loss: 0.050514
step:    78660, time: 2.309, loss: 0.035526
step:    78680, time: 2.349, loss: 0.042139
step:    78700, time: 2.349, loss: 0.041642
step:    78720, time: 2.317, loss: 0.041438
step:    78740, time: 2.348, loss: 0.039711
step:    78760, time: 2.373, loss: 0.048721
step:    78780, time: 2.420, loss: 0.050941
step:    78800, time: 2.411, loss: 0.043449
step:    78820, time: 2.381, loss: 0.043535
step:    78840, time: 2.297, loss: 0.042053
step:    78860, time: 2.414, loss: 0.048144
step:    78880, time: 2.350, loss: 0.054762
step:    78900, time: 2.435, loss: 0.046362
step:    78920, time: 2.443, loss: 0.043017
step:    78940, time: 2.362, loss: 0.043759
step:    78960, time: 2.467, loss: 0.052369
step:    78980, time: 2.239, loss: 0.036288
step:    79000, time: 2.419, loss: 0.049159
step:    79020, time: 2.465, loss: 0.045785
step:    79040, time: 2.390, loss: 0.045438
step:    79060, time: 2.374, loss: 0.040886
step:    79080, time: 2.447, loss: 0.049347
step:    79100, time: 2.413, loss: 0.038558
step:    79120, time: 2.318, loss: 0.039641
step:    79140, time: 2.372, loss: 0.039268
step:    79160, time: 2.347, loss: 0.055285
step:    79180, time: 2.248, loss: 0.047397
step:    79200, time: 3.418, loss: 0.045343
step:    79220, time: 2.370, loss: 0.043365
step:    79240, time: 2.333, loss: 0.039112
step:    79260, time: 2.340, loss: 0.045056
step:    79280, time: 2.348, loss: 0.042154
step:    79300, time: 2.451, loss: 0.040516
step:    79320, time: 2.381, loss: 0.041861
step:    79340, time: 2.392, loss: 0.041151
step:    79360, time: 2.356, loss: 0.042591
step:    79380, time: 2.465, loss: 0.050749
step:    79400, time: 2.349, loss: 0.045050
step:    79420, time: 2.430, loss: 0.048194
step:    79440, time: 2.397, loss: 0.047589
step:    79460, time: 2.405, loss: 0.041862
step:    79480, time: 2.244, loss: 0.034648
step:    79500, time: 2.312, loss: 0.045532
step:    79520, time: 2.476, loss: 0.050843
step:    79540, time: 2.463, loss: 0.046351
step:    79560, time: 2.406, loss: 0.044774
step:    79580, time: 2.347, loss: 0.036534
step:    79600, time: 2.385, loss: 0.047177
step:    79620, time: 2.411, loss: 0.048086
step:    79640, time: 2.458, loss: 0.049008
step:    79660, time: 2.343, loss: 0.045198
step:    79680, time: 2.327, loss: 0.041967
step:    79700, time: 2.382, loss: 0.042776
step:    79720, time: 2.353, loss: 0.044686
step:    79740, time: 2.410, loss: 0.043262
step:    79760, time: 2.370, loss: 0.053178
step:    79780, time: 2.373, loss: 0.040478
step:    79800, time: 2.207, loss: 0.051898
step:    79820, time: 2.265, loss: 0.044569
step:    79840, time: 2.360, loss: 0.037223
step:    79860, time: 2.444, loss: 0.052199
step:    79880, time: 2.441, loss: 0.047959
step:    79900, time: 2.454, loss: 0.049038
step:    79920, time: 2.289, loss: 0.035151
step:    79940, time: 2.369, loss: 0.044720
step:    79960, time: 2.354, loss: 0.039033
step:    79980, time: 2.308, loss: 0.037643
step:    80000, time: 2.361, loss: 0.041705
step:    80020, time: 2.299, loss: 0.038865
step:    80040, time: 2.310, loss: 0.044017
step:    80060, time: 2.423, loss: 0.042914
step:    80080, time: 2.475, loss: 0.054210
step:    80100, time: 2.316, loss: 0.044268
step:    80120, time: 2.407, loss: 0.051116
step:    80140, time: 2.379, loss: 0.041768
step:    80160, time: 2.336, loss: 0.047515
step:    80180, time: 2.382, loss: 0.041842
step:    80200, time: 2.396, loss: 0.054039
step:    80220, time: 2.412, loss: 0.045076
step:    80240, time: 2.444, loss: 0.056061
step:    80260, time: 2.459, loss: 0.041376
step:    80280, time: 2.420, loss: 0.044031
step:    80300, time: 2.381, loss: 0.041769
step:    80320, time: 2.411, loss: 0.047421
step:    80340, time: 2.371, loss: 0.042563
step:    80360, time: 2.432, loss: 0.043324
step:    80380, time: 2.461, loss: 0.048782
step:    80400, time: 2.457, loss: 0.056247
step:    80420, time: 2.254, loss: 0.042450
step:    80440, time: 2.305, loss: 0.051891
step:    80460, time: 2.363, loss: 0.041570
step:    80480, time: 2.420, loss: 0.050122
step:    80500, time: 2.403, loss: 0.041076
step:    80520, time: 2.261, loss: 0.039058
step:    80540, time: 2.407, loss: 0.040193
step:    80560, time: 2.404, loss: 0.037981
step:    80580, time: 2.439, loss: 0.048011
step:    80600, time: 2.440, loss: 0.048044
step:    80620, time: 2.326, loss: 0.043418
step:    80640, time: 2.322, loss: 0.041270
step:    80660, time: 2.340, loss: 0.040036
step:    80680, time: 2.295, loss: 0.045004
step:    80700, time: 2.443, loss: 0.049682
step:    80720, time: 2.338, loss: 0.043204
step:    80740, time: 2.304, loss: 0.049826
step:    80760, time: 3.076, loss: 0.048927
step:    80780, time: 2.363, loss: 0.037294
step:    80800, time: 2.339, loss: 0.042728
step:    80820, time: 2.387, loss: 0.047682
step:    80840, time: 2.301, loss: 0.036284
step:    80860, time: 2.387, loss: 0.043072
step:    80880, time: 2.495, loss: 0.048717
step:    80900, time: 2.377, loss: 0.045436
step:    80920, time: 2.347, loss: 0.046687
step:    80940, time: 2.487, loss: 0.049804
step:    80960, time: 2.306, loss: 0.039211
step:    80980, time: 2.340, loss: 0.037708
step:    81000, time: 2.340, loss: 0.039731
step:    81020, time: 2.427, loss: 0.044415
step:    81040, time: 2.278, loss: 0.045042
step:    81060, time: 2.299, loss: 0.044860
step:    81080, time: 2.452, loss: 0.043221
step:    81100, time: 2.417, loss: 0.046723
step:    81120, time: 2.361, loss: 0.041250
step:    81140, time: 2.352, loss: 0.045754
step:    81160, time: 2.411, loss: 0.037962
step:    81180, time: 2.412, loss: 0.042285
step:    81200, time: 2.397, loss: 0.049947
step:    81220, time: 2.418, loss: 0.049002
step:    81240, time: 2.333, loss: 0.036830
step:    81260, time: 2.378, loss: 0.050970
step:    81280, time: 2.413, loss: 0.040398
step:    81300, time: 2.388, loss: 0.043448
step:    81320, time: 2.385, loss: 0.043100
step:    81340, time: 2.401, loss: 0.044840
step:    81360, time: 2.358, loss: 0.044590
step:    81380, time: 2.268, loss: 0.044616
step:    81400, time: 2.375, loss: 0.042033
step:    81420, time: 2.440, loss: 0.046162
step:    81440, time: 2.486, loss: 0.039906
step:    81460, time: 2.344, loss: 0.055676
step:    81480, time: 2.339, loss: 0.042888
step:    81500, time: 2.435, loss: 0.055907
step:    81520, time: 2.398, loss: 0.043958
step:    81540, time: 2.411, loss: 0.046973
step:    81560, time: 2.403, loss: 0.049417
step:    81580, time: 2.502, loss: 0.050968
step:    81600, time: 2.321, loss: 0.036250
step:    81620, time: 2.387, loss: 0.046625
step:    81640, time: 2.342, loss: 0.038074
step:    81660, time: 2.292, loss: 0.046089
step:    81680, time: 2.302, loss: 0.048135
step:    81700, time: 2.463, loss: 0.048625
step:    81720, time: 2.356, loss: 0.039839
step:    81740, time: 2.312, loss: 0.043626
step:    81760, time: 2.383, loss: 0.043723
step:    81780, time: 2.314, loss: 0.045780
step:    81800, time: 2.440, loss: 0.055602
step:    81820, time: 2.484, loss: 0.054645
step:    81840, time: 2.431, loss: 0.048081
step:    81860, time: 2.352, loss: 0.036505
step:    81880, time: 2.428, loss: 0.043420
step:    81900, time: 2.416, loss: 0.049773
step:    81920, time: 2.400, loss: 0.052126
step:    81940, time: 2.321, loss: 0.042330
step:    81960, time: 2.325, loss: 0.040119
step:    81980, time: 2.261, loss: 0.041590
step:    82000, time: 2.211, loss: 0.039308
step:    82020, time: 2.463, loss: 0.051703
step:    82040, time: 2.316, loss: 0.042009
step:    82060, time: 2.319, loss: 0.051113
step:    82080, time: 2.412, loss: 0.047738
step:    82100, time: 2.275, loss: 0.035889
step:    82120, time: 2.325, loss: 0.041004
step:    82140, time: 2.289, loss: 0.040166
step:    82160, time: 2.299, loss: 0.037207
step:    82180, time: 2.295, loss: 0.040959
step:    82200, time: 2.395, loss: 0.046631
step:    82220, time: 2.409, loss: 0.040467
step:    82240, time: 2.384, loss: 0.045273
step:    82260, time: 2.428, loss: 0.044434
step:    82280, time: 2.392, loss: 0.048116
step:    82300, time: 2.310, loss: 0.051061
step:    82320, time: 2.401, loss: 0.044454
step:    82340, time: 2.349, loss: 0.042180
step:    82360, time: 2.332, loss: 0.044155
step:    82380, time: 2.474, loss: 0.050250
step:    82400, time: 2.447, loss: 0.041997
step:    82420, time: 2.363, loss: 0.042566
step:    82440, time: 2.385, loss: 0.043745
step:    82460, time: 2.384, loss: 0.040599
step:    82480, time: 2.340, loss: 0.042387
step:    82500, time: 2.386, loss: 0.049352
step:    82520, time: 2.389, loss: 0.051831
step:    82540, time: 2.324, loss: 0.042103
step:    82560, time: 2.370, loss: 0.050068
step:    82580, time: 2.326, loss: 0.039799
step:    82600, time: 2.239, loss: 0.041365
step:    82620, time: 2.286, loss: 0.047519
step:    82640, time: 2.376, loss: 0.043502
step:    82660, time: 2.490, loss: 0.056501
step:    82680, time: 2.399, loss: 0.042799
step:    82700, time: 2.359, loss: 0.049131
step:    82720, time: 2.374, loss: 0.045348
step:    82740, time: 2.439, loss: 0.051220
step:    82760, time: 2.458, loss: 0.053932
step:    82780, time: 2.452, loss: 0.055244
step:    82800, time: 2.407, loss: 0.054679
step:    82820, time: 2.248, loss: 0.040186
step:    82840, time: 2.354, loss: 0.044344
step:    82860, time: 2.343, loss: 0.043732
step:    82880, time: 2.424, loss: 0.046841
step:    82900, time: 2.321, loss: 0.036644
step:    82920, time: 2.286, loss: 0.044831
step:    82940, time: 2.291, loss: 0.054734
step:    82960, time: 2.452, loss: 0.044877
step:    82980, time: 2.434, loss: 0.052443
step:    83000, time: 2.368, loss: 0.044808
step:    83020, time: 2.409, loss: 0.048250
step:    83040, time: 2.350, loss: 0.042531
step:    83060, time: 2.293, loss: 0.036610
step:    83080, time: 2.378, loss: 0.041619
step:    83100, time: 2.436, loss: 0.056944
step:    83120, time: 2.401, loss: 0.048560
step:    83140, time: 2.342, loss: 0.046081
step:    83160, time: 2.381, loss: 0.043059
step:    83180, time: 2.336, loss: 0.041596
step:    83200, time: 2.414, loss: 0.043248
step:    83220, time: 2.227, loss: 0.050222
step:    83240, time: 2.277, loss: 0.042474
step:    83260, time: 2.330, loss: 0.044346
step:    83280, time: 2.504, loss: 0.050101
step:    83300, time: 2.458, loss: 0.049217
step:    83320, time: 2.354, loss: 0.045412
step:    83340, time: 2.391, loss: 0.044505
step:    83360, time: 2.416, loss: 0.044050
step:    83380, time: 2.434, loss: 0.054877
step:    83400, time: 2.507, loss: 0.047468
step:    83420, time: 2.437, loss: 0.059268
step:    83440, time: 2.332, loss: 0.039063
step:    83460, time: 2.368, loss: 0.040479
step:    83480, time: 2.371, loss: 0.050430
step:    83500, time: 2.379, loss: 0.040010
step:    83520, time: 2.400, loss: 0.046435
step:    83540, time: 2.302, loss: 0.042951
step:    83560, time: 2.257, loss: 0.054422
step:    83580, time: 2.505, loss: 0.050477
step:    83600, time: 2.458, loss: 0.051252
step:    83620, time: 2.423, loss: 0.040320
step:    83640, time: 2.355, loss: 0.041988
step:    83660, time: 2.334, loss: 0.043307
step:    83680, time: 2.349, loss: 0.039774
step:    83700, time: 2.369, loss: 0.042540
step:    83720, time: 2.481, loss: 0.052201
step:    83740, time: 2.424, loss: 0.045486
step:    83760, time: 2.386, loss: 0.046351
step:    83780, time: 2.386, loss: 0.043731
step:    83800, time: 2.325, loss: 0.044342
step:    83820, time: 2.430, loss: 0.051013
step:    83840, time: 2.305, loss: 0.044981
step:    83860, time: 2.197, loss: 0.043884
step:    83880, time: 3.940, loss: 0.043406
step:    83900, time: 2.357, loss: 0.041262
step:    83920, time: 2.417, loss: 0.041869
step:    83940, time: 2.355, loss: 0.037366
step:    83960, time: 2.313, loss: 0.045781
step:    83980, time: 2.453, loss: 0.045676
step:    84000, time: 2.336, loss: 0.044982
step:    84020, time: 2.425, loss: 0.054842
step:    84040, time: 2.342, loss: 0.041170
step:    84060, time: 2.366, loss: 0.044796
step:    84080, time: 2.414, loss: 0.051644
step:    84100, time: 2.325, loss: 0.038112
step:    84120, time: 2.408, loss: 0.039019
step:    84140, time: 2.352, loss: 0.041347
step:    84160, time: 2.287, loss: 0.040110
step:    84180, time: 2.290, loss: 0.048841
step:    84200, time: 2.472, loss: 0.042930
step:    84220, time: 2.328, loss: 0.036638
step:    84240, time: 2.400, loss: 0.042903
step:    84260, time: 2.383, loss: 0.051133
step:    84280, time: 2.266, loss: 0.035795
step:    84300, time: 2.457, loss: 0.056981
step:    84320, time: 2.417, loss: 0.041580
step:    84340, time: 2.377, loss: 0.041046
step:    84360, time: 2.424, loss: 0.050999
step:    84380, time: 2.397, loss: 0.048071
step:    84400, time: 2.311, loss: 0.041944
step:    84420, time: 2.399, loss: 0.053182
step:    84440, time: 2.340, loss: 0.041091
step:    84460, time: 2.425, loss: 0.046908
step:    84480, time: 2.321, loss: 0.048155
step:    84500, time: 2.307, loss: 0.048145
step:    84520, time: 2.423, loss: 0.042116
step:    84540, time: 2.305, loss: 0.036218
step:    84560, time: 2.466, loss: 0.043174
step:    84580, time: 2.390, loss: 0.042755
step:    84600, time: 2.375, loss: 0.046250
step:    84620, time: 2.393, loss: 0.041534
step:    84640, time: 2.332, loss: 0.042660
step:    84660, time: 2.359, loss: 0.044828
step:    84680, time: 2.453, loss: 0.047255
step:    84700, time: 2.436, loss: 0.048443
step:    84720, time: 2.311, loss: 0.043993
step:    84740, time: 2.318, loss: 0.037534
step:    84760, time: 2.379, loss: 0.044253
step:    84780, time: 2.374, loss: 0.055390
step:    84800, time: 2.281, loss: 0.046440
step:    84820, time: 2.437, loss: 0.048217
step:    84840, time: 2.401, loss: 0.043502
step:    84860, time: 2.303, loss: 0.038771
step:    84880, time: 2.395, loss: 0.042321
step:    84900, time: 2.373, loss: 0.043764
step:    84920, time: 2.369, loss: 0.044860
step:    84940, time: 2.277, loss: 0.035521
step:    84960, time: 2.417, loss: 0.053307
step:    84980, time: 2.417, loss: 0.049746
step:    85000, time: 2.385, loss: 0.052603
step:    85020, time: 2.412, loss: 0.039467
step:    85040, time: 2.420, loss: 0.053416
step:    85060, time: 2.326, loss: 0.043060
step:    85080, time: 2.345, loss: 0.044902
step:    85100, time: 2.313, loss: 0.051208
step:    85120, time: 2.284, loss: 0.045910
step:    85140, time: 2.386, loss: 0.046165
step:    85160, time: 2.328, loss: 0.042138
step:    85180, time: 2.473, loss: 0.056054
step:    85200, time: 2.449, loss: 0.046599
step:    85220, time: 2.490, loss: 0.049902
step:    85240, time: 2.423, loss: 0.050121
step:    85260, time: 2.412, loss: 0.047150
step:    85280, time: 2.361, loss: 0.042219
step:    85300, time: 2.419, loss: 0.048276
step:    85320, time: 2.331, loss: 0.038288
step:    85340, time: 2.359, loss: 0.041302
step:    85360, time: 2.352, loss: 0.054555
step:    85380, time: 2.335, loss: 0.042328
step:    85400, time: 2.291, loss: 0.043515
step:    85420, time: 2.258, loss: 0.040727
step:    85440, time: 2.529, loss: 0.055935
step:    85460, time: 2.325, loss: 0.038798
step:    85480, time: 2.406, loss: 0.045609
step:    85500, time: 2.319, loss: 0.040476
step:    85520, time: 2.414, loss: 0.052412
step:    85540, time: 2.375, loss: 0.044920
step:    85560, time: 2.404, loss: 0.047931
step:    85580, time: 2.366, loss: 0.047873
step:    85600, time: 2.382, loss: 0.044171
step:    85620, time: 2.403, loss: 0.049764
step:    85640, time: 2.316, loss: 0.039250
step:    85660, time: 2.361, loss: 0.041987
step:    85680, time: 2.390, loss: 0.048967
step:    85700, time: 2.318, loss: 0.038836
step:    85720, time: 2.283, loss: 0.040264
step:    85740, time: 2.254, loss: 0.038638
step:    85760, time: 2.397, loss: 0.041013
step:    85780, time: 2.246, loss: 0.038459
step:    85800, time: 2.383, loss: 0.045123
step:    85820, time: 2.396, loss: 0.050332
step:    85840, time: 2.374, loss: 0.044270
step:    85860, time: 2.377, loss: 0.045086
step:    85880, time: 2.335, loss: 0.041224
step:    85900, time: 2.411, loss: 0.043942
step:    85920, time: 2.381, loss: 0.039134
step:    85940, time: 2.390, loss: 0.046550
step:    85960, time: 2.370, loss: 0.042751
step:    85980, time: 2.287, loss: 0.043245
step:    86000, time: 2.461, loss: 0.057272
step:    86020, time: 2.432, loss: 0.057562
step:    86040, time: 2.254, loss: 0.046657
step:    86060, time: 2.212, loss: 0.046189
step:    86080, time: 2.429, loss: 0.040383
step:    86100, time: 2.420, loss: 0.048253
step:    86120, time: 2.262, loss: 0.034430
step:    86140, time: 2.408, loss: 0.046138
step:    86160, time: 2.407, loss: 0.044765
step:    86180, time: 2.371, loss: 0.046155
step:    86200, time: 2.248, loss: 0.037010
step:    86220, time: 2.459, loss: 0.058290
step:    86240, time: 2.362, loss: 0.045430
step:    86260, time: 2.405, loss: 0.055785
step:    86280, time: 2.420, loss: 0.041170
step:    86300, time: 2.316, loss: 0.037567
step:    86320, time: 2.248, loss: 0.038620
step:    86340, time: 2.327, loss: 0.053477
step:    86360, time: 2.245, loss: 0.042668
step:    86380, time: 2.459, loss: 0.057377
step:    86400, time: 2.360, loss: 0.044079
step:    86420, time: 2.423, loss: 0.046371
step:    86440, time: 2.475, loss: 0.054743
step:    86460, time: 2.376, loss: 0.041558
step:    86480, time: 2.366, loss: 0.044073
step:    86500, time: 2.409, loss: 0.048900
step:    86520, time: 2.388, loss: 0.038096
step:    86540, time: 2.396, loss: 0.044128
step:    86560, time: 2.416, loss: 0.044632
step:    86580, time: 2.418, loss: 0.045468
step:    86600, time: 2.401, loss: 0.049626
step:    86620, time: 2.345, loss: 0.047553
step:    86640, time: 2.306, loss: 0.035452
step:    86660, time: 2.387, loss: 0.050612
step:    86680, time: 2.215, loss: 0.041047
step:    86700, time: 2.409, loss: 0.044940
step:    86720, time: 2.432, loss: 0.041879
step:    86740, time: 2.325, loss: 0.039517
step:    86760, time: 2.305, loss: 0.047061
step:    86780, time: 2.406, loss: 0.052044
step:    86800, time: 2.338, loss: 0.050292
step:    86820, time: 2.392, loss: 0.052465
step:    86840, time: 2.394, loss: 0.049245
step:    86860, time: 2.383, loss: 0.040422
step:    86880, time: 2.448, loss: 0.050888
step:    86900, time: 2.334, loss: 0.037928
step:    86920, time: 2.410, loss: 0.050279
step:    86940, time: 2.332, loss: 0.044820
step:    86960, time: 2.316, loss: 0.040618
step:    86980, time: 2.300, loss: 0.044293
step:    87000, time: 2.434, loss: 0.053968
step:    87020, time: 2.276, loss: 0.041155
step:    87040, time: 2.314, loss: 0.039666
step:    87060, time: 2.459, loss: 0.048212
step:    87080, time: 2.361, loss: 0.039260
step:    87100, time: 2.450, loss: 0.054378
step:    87120, time: 2.340, loss: 0.042850
step:    87140, time: 2.250, loss: 0.039043
step:    87160, time: 2.411, loss: 0.043639
step:    87180, time: 2.328, loss: 0.040748
step:    87200, time: 2.380, loss: 0.043180
step:    87220, time: 2.342, loss: 0.048758
step:    87240, time: 2.434, loss: 0.056355
step:    87260, time: 2.344, loss: 0.041581
step:    87280, time: 2.378, loss: 0.057461
step:    87300, time: 2.210, loss: 0.037709
step:    87320, time: 2.482, loss: 0.046883
step:    87340, time: 2.350, loss: 0.039146
step:    87360, time: 2.379, loss: 0.044156
step:    87380, time: 2.369, loss: 0.040671
step:    87400, time: 2.416, loss: 0.047860
step:    87420, time: 2.398, loss: 0.050881
step:    87440, time: 2.391, loss: 0.041137
step:    87460, time: 2.385, loss: 0.046704
step:    87480, time: 2.389, loss: 0.053883
step:    87500, time: 2.337, loss: 0.040488
step:    87520, time: 2.405, loss: 0.048424
step:    87540, time: 2.362, loss: 0.047368
step:    87560, time: 2.352, loss: 0.042063
step:    87580, time: 2.345, loss: 0.050409
step:    87600, time: 2.304, loss: 0.046127
step:    87620, time: 2.235, loss: 0.054990
step:    87640, time: 2.464, loss: 0.046859
step:    87660, time: 2.363, loss: 0.050302
step:    87680, time: 2.367, loss: 0.040136
step:    87700, time: 2.327, loss: 0.040067
step:    87720, time: 2.402, loss: 0.047965
step:    87740, time: 2.352, loss: 0.045842
step:    87760, time: 2.375, loss: 0.046177
step:    87780, time: 2.333, loss: 0.039751
step:    87800, time: 2.280, loss: 0.042530
step:    87820, time: 2.365, loss: 0.039473
step:    87840, time: 2.426, loss: 0.051054
step:    87860, time: 2.373, loss: 0.042284
step:    87880, time: 2.430, loss: 0.052758
step:    87900, time: 2.360, loss: 0.047208
step:    87920, time: 2.312, loss: 0.060230
step:    87940, time: 2.402, loss: 0.045983
step:    87960, time: 2.350, loss: 0.035761
step:    87980, time: 2.316, loss: 0.034217
step:    88000, time: 2.442, loss: 0.057291
step:    88020, time: 2.327, loss: 0.042276
step:    88040, time: 2.352, loss: 0.038715
step:    88060, time: 2.409, loss: 0.044717
step:    88080, time: 2.405, loss: 0.043866
step:    88100, time: 2.430, loss: 0.049378
step:    88120, time: 2.304, loss: 0.039475
step:    88140, time: 2.418, loss: 0.044311
step:    88160, time: 2.354, loss: 0.038742
step:    88180, time: 2.401, loss: 0.042939
step:    88200, time: 2.374, loss: 0.045888
step:    88220, time: 2.382, loss: 0.049850
step:    88240, time: 2.196, loss: 0.043845
step:    88260, time: 2.330, loss: 0.039289
step:    88280, time: 2.432, loss: 0.041764
step:    88300, time: 2.360, loss: 0.046588
step:    88320, time: 2.368, loss: 0.043010
step:    88340, time: 2.253, loss: 0.036487
step:    88360, time: 2.301, loss: 0.038991
step:    88380, time: 2.377, loss: 0.052472
step:    88400, time: 2.435, loss: 0.051852
step:    88420, time: 2.362, loss: 0.049104
step:    88440, time: 2.324, loss: 0.042802
step:    88460, time: 2.348, loss: 0.046760
step:    88480, time: 2.362, loss: 0.044619
step:    88500, time: 2.301, loss: 0.044908
step:    88520, time: 2.432, loss: 0.055149
step:    88540, time: 2.224, loss: 0.046257
step:    88560, time: 3.808, loss: 0.053586
step:    88580, time: 2.429, loss: 0.047423
step:    88600, time: 2.395, loss: 0.042658
step:    88620, time: 2.401, loss: 0.047234
step:    88640, time: 2.368, loss: 0.046561
step:    88660, time: 2.376, loss: 0.041049
step:    88680, time: 2.438, loss: 0.048817
step:    88700, time: 2.417, loss: 0.056795
step:    88720, time: 2.328, loss: 0.040048
step:    88740, time: 2.372, loss: 0.040398
step:    88760, time: 2.401, loss: 0.049264
step:    88780, time: 2.390, loss: 0.047169
step:    88800, time: 2.424, loss: 0.044930
step:    88820, time: 2.407, loss: 0.040773
step:    88840, time: 2.389, loss: 0.054089
step:    88860, time: 2.261, loss: 0.046139
step:    88880, time: 2.401, loss: 0.043716
step:    88900, time: 2.402, loss: 0.046670
step:    88920, time: 2.346, loss: 0.044307
step:    88940, time: 2.318, loss: 0.037566
step:    88960, time: 2.312, loss: 0.037438
step:    88980, time: 2.326, loss: 0.040313
step:    89000, time: 2.363, loss: 0.045202
step:    89020, time: 2.298, loss: 0.046391
step:    89040, time: 2.373, loss: 0.048920
step:    89060, time: 2.398, loss: 0.044238
step:    89080, time: 2.330, loss: 0.042848
step:    89100, time: 2.478, loss: 0.059572
step:    89120, time: 2.371, loss: 0.049011
step:    89140, time: 2.452, loss: 0.048593
step:    89160, time: 2.262, loss: 0.047911
step:    89180, time: 2.209, loss: 0.041488
step:    89200, time: 2.461, loss: 0.044904
step:    89220, time: 2.371, loss: 0.047480
step:    89240, time: 2.369, loss: 0.040525
step:    89260, time: 2.434, loss: 0.046812
step:    89280, time: 2.385, loss: 0.048818
step:    89300, time: 2.362, loss: 0.043474
step:    89320, time: 2.388, loss: 0.049635
step:    89340, time: 2.463, loss: 0.043989
step:    89360, time: 2.353, loss: 0.044645
step:    89380, time: 2.244, loss: 0.037186
step:    89400, time: 2.321, loss: 0.037349
step:    89420, time: 2.384, loss: 0.044623
step:    89440, time: 2.378, loss: 0.043912
step:    89460, time: 2.288, loss: 0.040824
step:    89480, time: 2.200, loss: 0.045636
step:    89500, time: 2.374, loss: 0.051518
step:    89520, time: 2.369, loss: 0.045991
step:    89540, time: 2.413, loss: 0.043128
step:    89560, time: 2.423, loss: 0.052300
step:    89580, time: 2.351, loss: 0.038617
step:    89600, time: 2.309, loss: 0.049909
step:    89620, time: 2.407, loss: 0.042027
step:    89640, time: 2.456, loss: 0.049817
step:    89660, time: 2.416, loss: 0.051070
step:    89680, time: 2.392, loss: 0.048183
step:    89700, time: 2.265, loss: 0.035702
step:    89720, time: 2.348, loss: 0.037693
step:    89740, time: 2.340, loss: 0.035661
step:    89760, time: 2.324, loss: 0.043009
step:    89780, time: 2.302, loss: 0.050223
step:    89800, time: 2.319, loss: 0.050312
step:    89820, time: 2.350, loss: 0.047721
step:    89840, time: 2.414, loss: 0.043489
step:    89860, time: 2.680, loss: 0.056007
step:    89880, time: 2.423, loss: 0.041387
step:    89900, time: 2.344, loss: 0.047693
step:    89920, time: 2.374, loss: 0.047017
step:    89940, time: 2.319, loss: 0.037638
step:    89960, time: 2.365, loss: 0.050116
step:    89980, time: 2.454, loss: 0.046812
step:    90000, time: 2.305, loss: 0.040713
step:    90020, time: 2.424, loss: 0.045807
step:    90040, time: 2.316, loss: 0.044751
step:    90060, time: 2.325, loss: 0.038360
step:    90080, time: 2.328, loss: 0.042814
step:    90100, time: 2.355, loss: 0.040410
step:    90120, time: 2.343, loss: 0.042766
step:    90140, time: 2.374, loss: 0.040426
step:    90160, time: 2.299, loss: 0.039753
step:    90180, time: 2.455, loss: 0.049680
step:    90200, time: 2.330, loss: 0.041021
step:    90220, time: 2.365, loss: 0.053263
step:    90240, time: 2.380, loss: 0.052884
step:    90260, time: 2.312, loss: 0.037906
step:    90280, time: 2.400, loss: 0.042373
step:    90300, time: 2.345, loss: 0.041202
step:    90320, time: 2.418, loss: 0.046515
step:    90340, time: 2.433, loss: 0.042081
step:    90360, time: 2.416, loss: 0.043259
step:    90380, time: 2.314, loss: 0.040924
step:    90400, time: 2.180, loss: 0.045092
step:    90420, time: 2.231, loss: 0.041724
step:    90440, time: 2.340, loss: 0.038905
step:    90460, time: 2.367, loss: 0.050492
step:    90480, time: 2.384, loss: 0.049277
step:    90500, time: 2.300, loss: 0.042205
step:    90520, time: 2.362, loss: 0.041986
step:    90540, time: 2.428, loss: 0.049445
step:    90560, time: 2.376, loss: 0.042759
step:    90580, time: 2.450, loss: 0.056660
step:    90600, time: 2.356, loss: 0.045999
step:    90620, time: 2.379, loss: 0.040686
step:    90640, time: 2.405, loss: 0.044699
step:    90660, time: 2.421, loss: 0.048528
step:    90680, time: 2.369, loss: 0.045233
step:    90700, time: 2.424, loss: 0.047671
step:    90720, time: 2.231, loss: 0.040399
step:    90740, time: 2.165, loss: 0.035950
step:    90760, time: 2.500, loss: 0.060209
step:    90780, time: 2.417, loss: 0.044224
step:    90800, time: 2.449, loss: 0.053837
step:    90820, time: 2.278, loss: 0.038252
step:    90840, time: 2.394, loss: 0.045181
step:    90860, time: 2.316, loss: 0.041321
step:    90880, time: 2.402, loss: 0.053882
step:    90900, time: 2.409, loss: 0.044982
step:    90920, time: 2.420, loss: 0.039386
step:    90940, time: 2.397, loss: 0.045339
step:    90960, time: 2.389, loss: 0.046247
step:    90980, time: 2.334, loss: 0.040841
step:    91000, time: 2.438, loss: 0.049324
step:    91020, time: 2.310, loss: 0.043252
step:    91040, time: 2.218, loss: 0.042004
step:    91060, time: 2.437, loss: 0.042163
step:    91080, time: 2.412, loss: 0.045027
step:    91100, time: 2.371, loss: 0.041792
step:    91120, time: 2.373, loss: 0.044563
step:    91140, time: 2.406, loss: 0.045657
step:    91160, time: 2.349, loss: 0.044783
step:    91180, time: 2.425, loss: 0.044813
step:    91200, time: 2.390, loss: 0.043316
step:    91220, time: 2.404, loss: 0.044270
step:    91240, time: 2.398, loss: 0.042198
step:    91260, time: 2.355, loss: 0.040508
step:    91280, time: 2.444, loss: 0.054959
step:    91300, time: 2.380, loss: 0.051438
step:    91320, time: 2.429, loss: 0.044669
step:    91340, time: 2.320, loss: 0.060827
step:    91360, time: 2.315, loss: 0.047125
step:    91380, time: 2.401, loss: 0.045247
step:    91400, time: 2.471, loss: 0.057162
step:    91420, time: 2.369, loss: 0.041089
step:    91440, time: 2.298, loss: 0.037797
step:    91460, time: 2.361, loss: 0.046848
step:    91480, time: 2.403, loss: 0.051558
step:    91500, time: 2.367, loss: 0.046729
step:    91520, time: 2.340, loss: 0.042553
step:    91540, time: 2.312, loss: 0.046242
step:    91560, time: 2.404, loss: 0.046004
step:    91580, time: 2.314, loss: 0.036372
step:    91600, time: 2.415, loss: 0.047519
step:    91620, time: 2.485, loss: 0.051197
step:    91640, time: 2.328, loss: 0.041963
step:    91660, time: 2.303, loss: 0.055661
step:    91680, time: 2.735, loss: 0.042445
step:    91700, time: 2.372, loss: 0.036943
step:    91720, time: 2.275, loss: 0.036232
step:    91740, time: 2.454, loss: 0.055138
step:    91760, time: 2.336, loss: 0.039594
step:    91780, time: 2.388, loss: 0.040562
step:    91800, time: 2.337, loss: 0.047809
step:    91820, time: 2.360, loss: 0.047319
step:    91840, time: 2.346, loss: 0.038828
step:    91860, time: 2.477, loss: 0.053846
step:    91880, time: 2.355, loss: 0.037998
step:    91900, time: 2.381, loss: 0.043819
step:    91920, time: 2.451, loss: 0.049125
step:    91940, time: 2.352, loss: 0.036029
step:    91960, time: 2.341, loss: 0.041516
step:    91980, time: 2.215, loss: 0.040791
step:    92000, time: 2.355, loss: 0.039826
step:    92020, time: 2.359, loss: 0.046145
step:    92040, time: 2.364, loss: 0.043388
step:    92060, time: 2.422, loss: 0.046318
step:    92080, time: 2.409, loss: 0.044187
step:    92100, time: 2.430, loss: 0.045623
step:    92120, time: 2.387, loss: 0.042495
step:    92140, time: 2.426, loss: 0.043690
step:    92160, time: 2.430, loss: 0.052709
step:    92180, time: 2.263, loss: 0.039318
step:    92200, time: 2.397, loss: 0.046189
step:    92220, time: 2.365, loss: 0.051514
step:    92240, time: 2.412, loss: 0.046842
step:    92260, time: 2.360, loss: 0.042267
step:    92280, time: 2.199, loss: 0.043268
step:    92300, time: 2.294, loss: 0.044954
step:    92320, time: 2.425, loss: 0.039569
step:    92340, time: 2.395, loss: 0.048779
step:    92360, time: 2.317, loss: 0.039612
step:    92380, time: 2.471, loss: 0.053057
step:    92400, time: 2.359, loss: 0.036417
step:    92420, time: 2.401, loss: 0.048619
step:    92440, time: 2.353, loss: 0.039451
step:    92460, time: 2.429, loss: 0.048728
step:    92480, time: 2.394, loss: 0.046129
step:    92500, time: 2.353, loss: 0.037668
step:    92520, time: 2.377, loss: 0.047267
step:    92540, time: 2.361, loss: 0.044258
step:    92560, time: 2.402, loss: 0.047827
step:    92580, time: 2.346, loss: 0.052386
step:    92600, time: 2.179, loss: 0.043875
step:    92620, time: 2.424, loss: 0.047271
step:    92640, time: 2.375, loss: 0.048132
step:    92660, time: 2.371, loss: 0.039712
step:    92680, time: 2.316, loss: 0.047968
step:    92700, time: 2.426, loss: 0.047122
step:    92720, time: 2.358, loss: 0.052589
step:    92740, time: 2.456, loss: 0.042641
step:    92760, time: 2.347, loss: 0.040652
step:    92780, time: 2.441, loss: 0.051727
step:    92800, time: 2.334, loss: 0.040715
step:    92820, time: 2.384, loss: 0.040929
step:    92840, time: 2.462, loss: 0.048151
step:    92860, time: 2.390, loss: 0.044643
step:    92880, time: 2.323, loss: 0.040828
step:    92900, time: 2.279, loss: 0.047918
step:    92920, time: 2.262, loss: 0.047698
step:    92940, time: 2.447, loss: 0.053997
step:    92960, time: 2.411, loss: 0.049456
step:    92980, time: 2.375, loss: 0.044967
step:    93000, time: 2.314, loss: 0.052836
step:    93020, time: 2.351, loss: 0.040371
step:    93040, time: 2.274, loss: 0.040335
step:    93060, time: 2.419, loss: 0.060950
step:    93080, time: 2.277, loss: 0.042745
step:    93100, time: 2.330, loss: 0.037693
step:    93120, time: 2.330, loss: 0.042430
step:    93140, time: 2.357, loss: 0.042954
step:    93160, time: 2.334, loss: 0.042080
step:    93180, time: 2.454, loss: 0.045653
step:    93200, time: 2.392, loss: 0.046798
step:    93220, time: 2.330, loss: 0.050272
step:    93240, time: 3.119, loss: 0.044426
step:    93260, time: 2.390, loss: 0.043140
step:    93280, time: 2.401, loss: 0.048429
step:    93300, time: 2.375, loss: 0.051969
step:    93320, time: 2.496, loss: 0.052816
step:    93340, time: 2.296, loss: 0.045485
step:    93360, time: 2.345, loss: 0.044182
step:    93380, time: 2.433, loss: 0.045518
step:    93400, time: 2.376, loss: 0.044936
step:    93420, time: 2.302, loss: 0.038707
step:    93440, time: 2.448, loss: 0.043975
step:    93460, time: 2.327, loss: 0.038979
step:    93480, time: 2.393, loss: 0.044438
step:    93500, time: 2.430, loss: 0.044073
step:    93520, time: 2.291, loss: 0.042607
step:    93540, time: 2.280, loss: 0.051193
step:    93560, time: 2.313, loss: 0.032614
step:    93580, time: 2.391, loss: 0.044850
step:    93600, time: 2.380, loss: 0.040013
step:    93620, time: 2.357, loss: 0.040294
step:    93640, time: 2.404, loss: 0.054545
step:    93660, time: 2.341, loss: 0.047315
step:    93680, time: 2.336, loss: 0.039295
step:    93700, time: 2.319, loss: 0.035115
step:    93720, time: 2.371, loss: 0.050795
step:    93740, time: 2.339, loss: 0.038935
step:    93760, time: 2.436, loss: 0.046431
step:    93780, time: 2.394, loss: 0.049374
step:    93800, time: 2.404, loss: 0.047235
step:    93820, time: 2.424, loss: 0.046238
step:    93840, time: 2.295, loss: 0.041629
step:    93860, time: 2.219, loss: 0.041201
step:    93880, time: 2.494, loss: 0.056401
step:    93900, time: 2.409, loss: 0.046788
step:    93920, time: 2.266, loss: 0.038830
step:    93940, time: 2.477, loss: 0.055605
step:    93960, time: 2.364, loss: 0.043453
step:    93980, time: 2.306, loss: 0.040945
step:    94000, time: 2.244, loss: 0.035071
step:    94020, time: 2.350, loss: 0.039393
step:    94040, time: 2.376, loss: 0.042960
step:    94060, time: 2.465, loss: 0.048569
step:    94080, time: 2.362, loss: 0.044763
step:    94100, time: 2.322, loss: 0.040607
step:    94120, time: 2.349, loss: 0.051469
step:    94140, time: 2.341, loss: 0.043697
step:    94160, time: 2.281, loss: 0.044594
step:    94180, time: 2.415, loss: 0.041996
step:    94200, time: 2.381, loss: 0.056254
step:    94220, time: 2.469, loss: 0.049068
step:    94240, time: 2.381, loss: 0.045934
step:    94260, time: 2.423, loss: 0.048280
step:    94280, time: 2.417, loss: 0.050877
step:    94300, time: 2.398, loss: 0.040198
step:    94320, time: 2.418, loss: 0.046153
step:    94340, time: 2.429, loss: 0.044561
step:    94360, time: 2.377, loss: 0.040037
step:    94380, time: 2.329, loss: 0.039616
step:    94400, time: 2.465, loss: 0.048063
step:    94420, time: 2.280, loss: 0.041507
step:    94440, time: 2.373, loss: 0.046564
step:    94460, time: 2.376, loss: 0.043941
step:    94480, time: 2.316, loss: 0.048622
step:    94500, time: 2.468, loss: 0.055558
step:    94520, time: 2.368, loss: 0.043951
step:    94540, time: 2.347, loss: 0.039706
step:    94560, time: 2.328, loss: 0.037007
step:    94580, time: 2.314, loss: 0.041129
step:    94600, time: 2.383, loss: 0.048219
step:    94620, time: 2.323, loss: 0.039313
step:    94640, time: 2.342, loss: 0.043230
step:    94660, time: 2.410, loss: 0.043103
step:    94680, time: 2.423, loss: 0.050030
step:    94700, time: 2.361, loss: 0.044838
step:    94720, time: 2.394, loss: 0.045283
step:    94740, time: 2.429, loss: 0.049722
step:    94760, time: 2.435, loss: 0.051095
step:    94780, time: 2.228, loss: 0.042962
step:    94800, time: 2.437, loss: 0.041445
step:    94820, time: 2.449, loss: 0.052824
step:    94840, time: 2.376, loss: 0.044411
step:    94860, time: 2.313, loss: 0.043380
step:    94880, time: 2.368, loss: 0.039525
step:    94900, time: 2.481, loss: 0.054781
step:    94920, time: 2.368, loss: 0.047292
step:    94940, time: 2.408, loss: 0.055214
step:    94960, time: 2.419, loss: 0.046175
step:    94980, time: 2.419, loss: 0.039616
step:    95000, time: 2.292, loss: 0.034065
step:    95020, time: 2.352, loss: 0.037288
step:    95040, time: 2.354, loss: 0.037177
step:    95060, time: 2.411, loss: 0.046795
step:    95080, time: 2.322, loss: 0.049747
step:    95100, time: 2.243, loss: 0.051388
step:    95120, time: 2.415, loss: 0.039678
step:    95140, time: 2.372, loss: 0.045104
step:    95160, time: 2.500, loss: 0.057510
step:    95180, time: 2.401, loss: 0.048905
step:    95200, time: 2.392, loss: 0.041753
step:    95220, time: 2.428, loss: 0.043469
step:    95240, time: 2.372, loss: 0.040807
step:    95260, time: 2.357, loss: 0.043747
step:    95280, time: 2.330, loss: 0.044652
step:    95300, time: 2.340, loss: 0.039412
step:    95320, time: 2.360, loss: 0.042002
step:    95340, time: 2.323, loss: 0.036809
step:    95360, time: 2.418, loss: 0.047554
step:    95380, time: 2.333, loss: 0.047585
step:    95400, time: 2.269, loss: 0.045274
step:    95420, time: 2.204, loss: 0.037252
step:    95440, time: 2.409, loss: 0.040088
step:    95460, time: 2.322, loss: 0.042911
step:    95480, time: 2.407, loss: 0.039072
step:    95500, time: 2.371, loss: 0.041599
step:    95520, time: 2.453, loss: 0.057258
step:    95540, time: 2.356, loss: 0.042632
step:    95560, time: 2.398, loss: 0.045347
step:    95580, time: 2.405, loss: 0.044439
step:    95600, time: 2.418, loss: 0.048722
step:    95620, time: 2.415, loss: 0.045706
step:    95640, time: 2.354, loss: 0.042281
step:    95660, time: 2.492, loss: 0.052370
step:    95680, time: 2.357, loss: 0.038942
step:    95700, time: 2.334, loss: 0.038609
step:    95720, time: 2.273, loss: 0.045315
step:    95740, time: 2.368, loss: 0.037121
step:    95760, time: 2.423, loss: 0.048587
step:    95780, time: 2.366, loss: 0.046007
step:    95800, time: 2.404, loss: 0.043960
step:    95820, time: 2.424, loss: 0.048254
step:    95840, time: 2.394, loss: 0.039604
step:    95860, time: 2.336, loss: 0.041481
step:    95880, time: 2.357, loss: 0.042054
step:    95900, time: 2.370, loss: 0.054186
step:    95920, time: 2.330, loss: 0.046006
step:    95940, time: 2.313, loss: 0.038387
step:    95960, time: 2.334, loss: 0.041999
step:    95980, time: 2.379, loss: 0.043247
step:    96000, time: 2.439, loss: 0.044834
step:    96020, time: 2.304, loss: 0.044429
step:    96040, time: 2.292, loss: 0.046868
step:    96060, time: 2.377, loss: 0.040748
step:    96080, time: 2.393, loss: 0.050244
step:    96100, time: 2.407, loss: 0.050875
step:    96120, time: 2.314, loss: 0.038649
step:    96140, time: 2.302, loss: 0.035308
step:    96160, time: 2.328, loss: 0.047680
step:    96180, time: 2.480, loss: 0.052858
step:    96200, time: 2.354, loss: 0.038844
step:    96220, time: 2.377, loss: 0.047292
step:    96240, time: 2.307, loss: 0.038007
step:    96260, time: 2.367, loss: 0.035551
step:    96280, time: 2.336, loss: 0.040559
step:    96300, time: 2.427, loss: 0.046566
step:    96320, time: 2.385, loss: 0.053814
step:    96340, time: 2.289, loss: 0.049827
step:    96360, time: 2.375, loss: 0.041509
step:    96380, time: 2.362, loss: 0.046715
step:    96400, time: 2.375, loss: 0.043036
step:    96420, time: 2.388, loss: 0.045222
step:    96440, time: 2.476, loss: 0.052139
step:    96460, time: 2.327, loss: 0.040594
step:    96480, time: 2.349, loss: 0.038389
step:    96500, time: 2.374, loss: 0.052467
step:    96520, time: 2.428, loss: 0.057493
step:    96540, time: 2.367, loss: 0.047059
step:    96560, time: 2.388, loss: 0.055071
step:    96580, time: 2.301, loss: 0.036126
step:    96600, time: 2.261, loss: 0.032289
step:    96620, time: 2.404, loss: 0.049293
step:    96640, time: 2.212, loss: 0.044227
step:    96660, time: 2.303, loss: 0.044128
step:    96680, time: 2.457, loss: 0.049393
step:    96700, time: 2.433, loss: 0.049272
step:    96720, time: 2.360, loss: 0.038808
step:    96740, time: 2.351, loss: 0.040395
step:    96760, time: 2.351, loss: 0.042030
step:    96780, time: 2.388, loss: 0.046098
step:    96800, time: 2.397, loss: 0.041718
step:    96820, time: 2.429, loss: 0.050472
step:    96840, time: 2.391, loss: 0.042605
step:    96860, time: 2.435, loss: 0.051275
step:    96880, time: 2.250, loss: 0.037692
step:    96900, time: 2.431, loss: 0.055468
step:    96920, time: 2.329, loss: 0.039189
step:    96940, time: 2.403, loss: 0.043923
step:    96960, time: 2.223, loss: 0.049562
step:    96980, time: 2.284, loss: 0.045575
step:    97000, time: 2.435, loss: 0.048552
step:    97020, time: 2.399, loss: 0.043558
step:    97040, time: 2.415, loss: 0.053851
step:    97060, time: 2.419, loss: 0.041398
step:    97080, time: 2.390, loss: 0.054233
step:    97100, time: 2.349, loss: 0.037017
step:    97120, time: 2.301, loss: 0.043217
step:    97140, time: 2.505, loss: 0.050608
step:    97160, time: 2.365, loss: 0.037570
step:    97180, time: 2.417, loss: 0.043413
step:    97200, time: 2.447, loss: 0.051039
step:    97220, time: 2.325, loss: 0.037507
step:    97240, time: 2.439, loss: 0.049322
step:    97260, time: 2.339, loss: 0.047879
step:    97280, time: 2.143, loss: 0.040833
step:    97300, time: 2.355, loss: 0.041901
step:    97320, time: 2.431, loss: 0.041883
step:    97340, time: 2.386, loss: 0.039333
step:    97360, time: 2.369, loss: 0.042256
step:    97380, time: 2.453, loss: 0.058656
step:    97400, time: 2.332, loss: 0.055690
step:    97420, time: 2.321, loss: 0.040979
step:    97440, time: 2.305, loss: 0.038688
step:    97460, time: 2.477, loss: 0.052978
step:    97480, time: 2.352, loss: 0.041146
step:    97500, time: 2.461, loss: 0.041650
step:    97520, time: 2.352, loss: 0.045070
step:    97540, time: 2.373, loss: 0.043231
step:    97560, time: 2.293, loss: 0.040098
step:    97580, time: 2.221, loss: 0.057017
step:    97600, time: 2.275, loss: 0.042435
step:    97620, time: 2.378, loss: 0.045285
step:    97640, time: 2.493, loss: 0.055186
step:    97660, time: 2.360, loss: 0.038692
step:    97680, time: 2.398, loss: 0.036888
step:    97700, time: 2.308, loss: 0.038299
step:    97720, time: 2.413, loss: 0.043226
step:    97740, time: 2.398, loss: 0.041519
step:    97760, time: 2.463, loss: 0.046611
step:    97780, time: 2.369, loss: 0.048295
step:    97800, time: 2.330, loss: 0.043128
step:    97820, time: 2.335, loss: 0.046900
step:    97840, time: 2.436, loss: 0.043111
step:    97860, time: 2.419, loss: 0.048805
step:    97880, time: 2.258, loss: 0.036911
step:    97900, time: 2.230, loss: 0.044831
step:    97920, time: 2.529, loss: 0.048994
step:    97940, time: 2.392, loss: 0.045666
step:    97960, time: 2.437, loss: 0.046616
step:    97980, time: 2.298, loss: 0.041632
step:    98000, time: 2.393, loss: 0.043600
step:    98020, time: 2.360, loss: 0.043858
step:    98040, time: 2.338, loss: 0.049193
step:    98060, time: 2.294, loss: 0.034216
step:    98080, time: 2.351, loss: 0.047064
step:    98100, time: 2.430, loss: 0.053116
step:    98120, time: 2.411, loss: 0.049653
step:    98140, time: 2.477, loss: 0.054845
step:    98160, time: 2.350, loss: 0.040293
step:    98180, time: 2.434, loss: 0.044883
step:    98200, time: 2.280, loss: 0.043250
step:    98220, time: 2.296, loss: 0.049804
step:    98240, time: 2.390, loss: 0.040361
step:    98260, time: 2.355, loss: 0.041023
step:    98280, time: 2.342, loss: 0.047003
step:    98300, time: 2.405, loss: 0.047388
step:    98320, time: 2.407, loss: 0.047630
step:    98340, time: 2.331, loss: 0.042420
step:    98360, time: 2.402, loss: 0.042496
step:    98380, time: 2.338, loss: 0.039618
step:    98400, time: 2.428, loss: 0.045242
step:    98420, time: 2.426, loss: 0.044378
step:    98440, time: 2.343, loss: 0.040033
step:    98460, time: 2.378, loss: 0.041412
step:    98480, time: 2.418, loss: 0.049480
step:    98500, time: 2.458, loss: 0.053887
step:    98520, time: 2.219, loss: 0.045878
step:    98540, time: 2.161, loss: 0.038738
step:    98560, time: 2.356, loss: 0.038653
step:    98580, time: 2.373, loss: 0.045104
step:    98600, time: 2.475, loss: 0.046550
step:    98620, time: 2.376, loss: 0.046975
step:    98640, time: 2.315, loss: 0.042233
step:    98660, time: 2.365, loss: 0.049315
step:    98680, time: 2.340, loss: 0.041065
step:    98700, time: 2.360, loss: 0.041041
step:    98720, time: 2.406, loss: 0.046932
step:    98740, time: 2.429, loss: 0.045041
step:    98760, time: 2.406, loss: 0.039972
step:    98780, time: 2.367, loss: 0.045021
step:    98800, time: 2.255, loss: 0.039264
step:    98820, time: 2.257, loss: 0.037820
step:    98840, time: 2.240, loss: 0.042361
step:    98860, time: 2.408, loss: 0.044345
step:    98880, time: 2.400, loss: 0.055134
step:    98900, time: 2.376, loss: 0.044741
step:    98920, time: 2.409, loss: 0.047013
step:    98940, time: 2.457, loss: 0.049381
step:    98960, time: 2.507, loss: 0.051720
step:    98980, time: 2.424, loss: 0.055165
step:    99000, time: 2.374, loss: 0.044428
step:    99020, time: 2.384, loss: 0.041357
step:    99040, time: 2.338, loss: 0.038619
step:    99060, time: 2.345, loss: 0.043681
step:    99080, time: 2.382, loss: 0.041680
step:    99100, time: 2.349, loss: 0.045154
step:    99120, time: 2.339, loss: 0.041091
step:    99140, time: 2.273, loss: 0.041123
step:    99160, time: 2.243, loss: 0.045520
step:    99180, time: 2.376, loss: 0.041143
step:    99200, time: 2.346, loss: 0.044203
step:    99220, time: 2.334, loss: 0.039699
step:    99240, time: 2.360, loss: 0.043393
step:    99260, time: 2.349, loss: 0.047442
step:    99280, time: 2.282, loss: 0.034935
step:    99300, time: 2.399, loss: 0.046451
step:    99320, time: 2.386, loss: 0.037267
step:    99340, time: 2.441, loss: 0.046651
step:    99360, time: 2.399, loss: 0.043461
step:    99380, time: 2.404, loss: 0.049544
step:    99400, time: 2.398, loss: 0.058435
step:    99420, time: 2.393, loss: 0.046514
step:    99440, time: 2.386, loss: 0.050635
step:    99460, time: 2.269, loss: 0.050438
step:    99480, time: 3.315, loss: 0.049085
step:    99500, time: 2.411, loss: 0.037492
step:    99520, time: 2.386, loss: 0.048494
step:    99540, time: 2.338, loss: 0.046290
step:    99560, time: 2.325, loss: 0.043299
step:    99580, time: 2.372, loss: 0.044803
step:    99600, time: 2.372, loss: 0.037333
step:    99620, time: 2.400, loss: 0.050291
step:    99640, time: 2.385, loss: 0.040933
step:    99660, time: 2.409, loss: 0.041962
step:    99680, time: 2.389, loss: 0.044448
step:    99700, time: 2.348, loss: 0.037602
step:    99720, time: 2.395, loss: 0.041794
step:    99740, time: 2.384, loss: 0.042377
step:    99760, time: 2.349, loss: 0.044086
step:    99780, time: 2.228, loss: 0.038199
step:    99800, time: 2.384, loss: 0.043423
step:    99820, time: 2.353, loss: 0.035970
step:    99840, time: 2.308, loss: 0.035900
step:    99860, time: 2.348, loss: 0.048510
step:    99880, time: 2.325, loss: 0.041579
step:    99900, time: 2.385, loss: 0.044202
step:    99920, time: 2.312, loss: 0.039665
step:    99940, time: 2.378, loss: 0.037020
step:    99960, time: 2.370, loss: 0.045503
step:    99980, time: 2.456, loss: 0.051120
step:   100000, time: 2.425, loss: 0.050566
Finished training GMM, named: GMM_with_SSIM_loss_new_channels_higher_weight!
